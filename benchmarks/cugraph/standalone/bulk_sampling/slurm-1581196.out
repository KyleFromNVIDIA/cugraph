Node IP: 10.180.6.152
Num Nodes: 2
Num GPUs Per Node: 8
no change     /opt/conda/condabin/conda
no change     /opt/conda/bin/conda
no change     /opt/conda/bin/conda-env
no change     /opt/conda/bin/activate
no change     /opt/conda/bin/deactivate
no change     /opt/conda/etc/profile.d/conda.sh
no change     /opt/conda/etc/fish/conf.d/conda.fish
no change     /opt/conda/shell/condabin/Conda.psm1
no change     /opt/conda/shell/condabin/conda-hook.ps1
no change     /opt/conda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /opt/conda/etc/profile.d/conda.csh
no change     /root/.bashrc
No action taken.

EnvironmentNameNotFound: Could not find conda environment: rapids
You can list all discoverable environments with `conda info --envs`.


properly waiting for workers to connect
>>>> Using cluster configurtion for TCP
>>>> Logs written to: /logs
no change     /opt/conda/condabin/conda
no change     /opt/conda/bin/conda
no change     /opt/conda/bin/conda-env
no change     /opt/conda/bin/activate
no change     /opt/conda/bin/deactivate
no change     /opt/conda/etc/profile.d/conda.sh
no change     /opt/conda/etc/fish/conf.d/conda.fish
no change     /opt/conda/shell/condabin/Conda.psm1
no change     /opt/conda/shell/condabin/conda-hook.ps1
no change     /opt/conda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /opt/conda/etc/profile.d/conda.csh
no change     /root/.bashrc
No action taken.
run-dask-process.sh: /scripts/mg_utils/dask_scheduler.json not present - waiting to start workers...
wait_for_workers.py - initializing client...done.

EnvironmentNameNotFound: Could not find conda environment: rapids
You can list all discoverable environments with `conda info --envs`.


properly waiting for workers to connect
>>>> Using cluster configurtion for TCP
>>>> Logs written to: /logs
wait_for_workers.py - initializing client...done.
worker(s) started.
waiting for worker pid 693150 to finish before exiting script...
wait_for_workers.py expected 16 but got 0, waiting...
wait_for_workers.py expected 16 but got 0, waiting...
scheduler started.
worker(s) started.
waiting for worker pid 1727804 to finish before exiting script...
wait_for_workers.py expected 16 but got 0, waiting...
wait_for_workers.py expected 16 but got 0, waiting...
wait_for_workers.py expected 16 but got 8, waiting...
wait_for_workers.py expected 16 but got 8, waiting...
wait_for_workers.py got 16 workers, done.
wait_for_workers.py got 16 workers, done.
0
Launching Python Script
1
INFO:__main__:starting dask client
INFO:__main__:dask client started
INFO:__main__:dataset: ogbn_papers100M
INFO:__main__:batch size: 512
INFO:__main__:fanout: [10, 10, 10]
INFO:__main__:seeds_per_call: 524288
INFO:__main__:num epochs: 1
INFO:__main__:ogbn_papers100M
INFO:__main__:Number of input edges = 1,615,685,872
INFO:__main__:constructed graph
/opt/conda/lib/python3.10/site-packages/cudf/core/index.py:3284: FutureWarning: cudf.StringIndex is deprecated and will be removed from cudf in a future version. Use cudf.Index with the appropriate dtype instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/cudf/core/index.py:3284: FutureWarning: cudf.StringIndex is deprecated and will be removed from cudf in a future version. Use cudf.Index with the appropriate dtype instead.
  warnings.warn(
INFO:__main__:input memory: 38776460928
INFO:cugraph.gnn.data_loading.bulk_sampler:Number of input seeds (1081726) is >= seeds per call (524288). Calling flush() to compute and write minibatches.
INFO:cugraph.gnn.data_loading.bulk_sampler:Calculated batches to sample; min = dd.Scalar<series-..., dtype=int32> and max = dd.Scalar<sub-870..., dtype=int64>; took 0.0109 s
/opt/conda/lib/python3.10/site-packages/cugraph/dask/sampling/uniform_neighbor_sample.py:533: FutureWarning: The with_edge_properties flag is deprecated and will be removed in the next release.
  warnings.warn(warning_msg, FutureWarning)
/opt/conda/lib/python3.10/site-packages/cugraph/dask/sampling/uniform_neighbor_sample.py:559: FutureWarning: The include_hop_column flag is deprecated and will be removed in the next release in favor of always excluding the hop column when return_offsets is True
  warnings.warn(warning_msg, FutureWarning)
INFO:cugraph.gnn.data_loading.bulk_sampler:Called uniform neighbor sample, took 1.0482 s
INFO:cugraph.gnn.data_loading.bulk_sampler:Wrote samples to parquet, took 5.496245765127242 seconds
INFO:cugraph.gnn.data_loading.bulk_sampler:There are still 281727 samples remaining, calling flush() again...
INFO:cugraph.gnn.data_loading.bulk_sampler:Calculated batches to sample; min = dd.Scalar<series-..., dtype=int32> and max = dd.Scalar<sub-b7f..., dtype=int64>; took 0.0102 s
/opt/conda/lib/python3.10/site-packages/cugraph/dask/sampling/uniform_neighbor_sample.py:533: FutureWarning: The with_edge_properties flag is deprecated and will be removed in the next release.
  warnings.warn(warning_msg, FutureWarning)
/opt/conda/lib/python3.10/site-packages/cugraph/dask/sampling/uniform_neighbor_sample.py:559: FutureWarning: The include_hop_column flag is deprecated and will be removed in the next release in favor of always excluding the hop column when return_offsets is True
  warnings.warn(warning_msg, FutureWarning)
INFO:cugraph.gnn.data_loading.bulk_sampler:Called uniform neighbor sample, took 0.7594 s
INFO:cugraph.gnn.data_loading.bulk_sampler:Wrote samples to parquet, took 0.3181804046034813 seconds
INFO:cugraph.gnn.data_loading.bulk_sampler:Number of input seeds (773907) is >= seeds per call (524288). Calling flush() to compute and write minibatches.
INFO:cugraph.gnn.data_loading.bulk_sampler:Calculated batches to sample; min = dd.Scalar<series-..., dtype=int32> and max = dd.Scalar<sub-ae1..., dtype=int64>; took 0.0107 s
/opt/conda/lib/python3.10/site-packages/cugraph/dask/sampling/uniform_neighbor_sample.py:533: FutureWarning: The with_edge_properties flag is deprecated and will be removed in the next release.
  warnings.warn(warning_msg, FutureWarning)
/opt/conda/lib/python3.10/site-packages/cugraph/dask/sampling/uniform_neighbor_sample.py:559: FutureWarning: The include_hop_column flag is deprecated and will be removed in the next release in favor of always excluding the hop column when return_offsets is True
  warnings.warn(warning_msg, FutureWarning)
INFO:cugraph.gnn.data_loading.bulk_sampler:Called uniform neighbor sample, took 0.8618 s
INFO:cugraph.gnn.data_loading.bulk_sampler:Wrote samples to parquet, took 0.5542019009590149 seconds
INFO:cugraph.gnn.data_loading.bulk_sampler:Number of input seeds (772875) is >= seeds per call (524288). Calling flush() to compute and write minibatches.
INFO:cugraph.gnn.data_loading.bulk_sampler:Calculated batches to sample; min = dd.Scalar<series-..., dtype=int32> and max = dd.Scalar<sub-59a..., dtype=int64>; took 0.0102 s
/opt/conda/lib/python3.10/site-packages/cugraph/dask/sampling/uniform_neighbor_sample.py:533: FutureWarning: The with_edge_properties flag is deprecated and will be removed in the next release.
  warnings.warn(warning_msg, FutureWarning)
/opt/conda/lib/python3.10/site-packages/cugraph/dask/sampling/uniform_neighbor_sample.py:559: FutureWarning: The include_hop_column flag is deprecated and will be removed in the next release in favor of always excluding the hop column when return_offsets is True
  warnings.warn(warning_msg, FutureWarning)
INFO:cugraph.gnn.data_loading.bulk_sampler:Called uniform neighbor sample, took 0.8702 s
INFO:cugraph.gnn.data_loading.bulk_sampler:Wrote samples to parquet, took 4.945730976760387 seconds
INFO:__main__:allocation counts b:
INFO:__main__:dict_values([{'current_bytes': 1797218576, 'current_count': 44, 'peak_bytes': 3648738768, 'peak_count': 316, 'total_bytes': 78721724832, 'total_count': 3638}, {'current_bytes': 1324268432, 'current_count': 55, 'peak_bytes': 3125547408, 'peak_count': 286, 'total_bytes': 66945408776, 'total_count': 3464}, {'current_bytes': 950521984, 'current_count': 34, 'peak_bytes': 3411416496, 'peak_count': 283, 'total_bytes': 77176651552, 'total_count': 3590}, {'current_bytes': 1094964320, 'current_count': 40, 'peak_bytes': 3998979008, 'peak_count': 331, 'total_bytes': 77127985760, 'total_count': 3554}, {'current_bytes': 1128736880, 'current_count': 30, 'peak_bytes': 3098585696, 'peak_count': 277, 'total_bytes': 69220139080, 'total_count': 3051}, {'current_bytes': 641731216, 'current_count': 42, 'peak_bytes': 3354091888, 'peak_count': 317, 'total_bytes': 76584517088, 'total_count': 4034}, {'current_bytes': 1448900272, 'current_count': 36, 'peak_bytes': 3703882320, 'peak_count': 330, 'total_bytes': 79492285520, 'total_count': 3653}, {'current_bytes': 1781772720, 'current_count': 58, 'peak_bytes': 3735403472, 'peak_count': 278, 'total_bytes': 72256173968, 'total_count': 3467}, {'current_bytes': 1554635632, 'current_count': 37, 'peak_bytes': 4348037136, 'peak_count': 367, 'total_bytes': 78630312904, 'total_count': 3208}, {'current_bytes': 970590512, 'current_count': 39, 'peak_bytes': 3483604672, 'peak_count': 334, 'total_bytes': 77792715128, 'total_count': 3649}, {'current_bytes': 1810640480, 'current_count': 42, 'peak_bytes': 4206602592, 'peak_count': 338, 'total_bytes': 75900082296, 'total_count': 3186}, {'current_bytes': 1732609664, 'current_count': 50, 'peak_bytes': 3749166560, 'peak_count': 341, 'total_bytes': 75509918264, 'total_count': 3205}, {'current_bytes': 1854185856, 'current_count': 40, 'peak_bytes': 4309537168, 'peak_count': 348, 'total_bytes': 81190575096, 'total_count': 3264}, {'current_bytes': 307565104, 'current_count': 33, 'peak_bytes': 3630779520, 'peak_count': 353, 'total_bytes': 70144358680, 'total_count': 3548}, {'current_bytes': 1583199152, 'current_count': 41, 'peak_bytes': 3724237808, 'peak_count': 321, 'total_bytes': 75063243192, 'total_count': 3186}, {'current_bytes': 1613558192, 'current_count': 40, 'peak_bytes': 3802691456, 'peak_count': 316, 'total_bytes': 71658793880, 'total_count': 3122}])
INFO:__main__:Number of edges in final graph = 1,615,685,872
INFO:__main__:--------------------------------------------------------------------------------

Dask client created using /scripts/mg_utils/dask_scheduler.json
Loading edge index for edge type paper__cites__paper
Loading node labels for node type paper (offset=0)
created batches
flushed all batches
created batches
flushed all batches
created batches
flushed all batches
function:  sample_graph
function args: () kwargs: {'G': <cugraph.structure.graph_classes.MultiGraph object at 0x1458d986f340>, 'label_df': <dask_cudf.DataFrame | 32 tasks | 16 npartitions>, 'output_path': '/samples/ogbn_papers100M[1]_b512_f[10, 10, 10]', 'num_epochs': 1, 'seed': 42, 'batch_size': 512, 'seeds_per_call': 524288, 'batches_per_partition': 781, 'fanout': [10, 10, 10], 'sampling_kwargs': {'deduplicate_sources': True, 'prior_sources_behavior': 'exclude', 'renumber': True, 'compression': 'COO', 'compress_per_hop': False, 'use_legacy_names': False, 'include_hop_column': True}}
execution_time: 15.987128257751465
allocation_counts:
{   'tcp://10.180.6.152:35915': {   'current_bytes': '1.7GB',
                                    'peak_bytes': '3.4GB',
                                    'total_bytes': '73.3GB'},
    'tcp://10.180.6.152:40635': {   'current_bytes': '1.2GB',
                                    'peak_bytes': '2.9GB',
                                    'total_bytes': '62.3GB'},
    'tcp://10.180.6.152:41253': {   'current_bytes': '906.5MB',
                                    'peak_bytes': '3.2GB',
                                    'total_bytes': '71.9GB'},
    'tcp://10.180.6.152:43815': {   'current_bytes': '1.0GB',
                                    'peak_bytes': '3.7GB',
                                    'total_bytes': '71.8GB'},
    'tcp://10.180.6.152:44619': {   'current_bytes': '1.1GB',
                                    'peak_bytes': '2.9GB',
                                    'total_bytes': '64.5GB'},
    'tcp://10.180.6.152:44975': {   'current_bytes': '612.0MB',
                                    'peak_bytes': '3.1GB',
                                    'total_bytes': '71.3GB'},
    'tcp://10.180.6.152:45029': {   'current_bytes': '1.3GB',
                                    'peak_bytes': '3.4GB',
                                    'total_bytes': '74.0GB'},
    'tcp://10.180.6.152:46667': {   'current_bytes': '1.7GB',
                                    'peak_bytes': '3.5GB',
                                    'total_bytes': '67.3GB'},
    'tcp://10.180.6.153:34327': {   'current_bytes': '1.4GB',
                                    'peak_bytes': '4.0GB',
                                    'total_bytes': '73.2GB'},
    'tcp://10.180.6.153:35661': {   'current_bytes': '925.6MB',
                                    'peak_bytes': '3.2GB',
                                    'total_bytes': '72.5GB'},
    'tcp://10.180.6.153:36727': {   'current_bytes': '1.7GB',
                                    'peak_bytes': '3.9GB',
                                    'total_bytes': '70.7GB'},
    'tcp://10.180.6.153:37691': {   'current_bytes': '1.6GB',
                                    'peak_bytes': '3.5GB',
                                    'total_bytes': '70.3GB'},
    'tcp://10.180.6.153:40027': {   'current_bytes': '1.7GB',
                                    'peak_bytes': '4.0GB',
                                    'total_bytes': '75.6GB'},
    'tcp://10.180.6.153:41203': {   'current_bytes': '293.3MB',
                                    'peak_bytes': '3.4GB',
                                    'total_bytes': '65.3GB'},
    'tcp://10.180.6.153:42921': {   'current_bytes': '1.5GB',
                                    'peak_bytes': '3.5GB',
                                    'total_bytes': '69.9GB'},
    'tcp://10.180.6.153:43119': {   'current_bytes': '1.5GB',
                                    'peak_bytes': '3.5GB',
                                    'total_bytes': '66.7GB'}}
Edge List Memory = 2.3GB
Peak Memory across workers = 4.0GB
Max Peak to output graph ratio across workers = 11.80
Max Peak to avg input graph ratio across workers = 1.79
----------------------------------------dataset = ogbn_papers100M completed----------------------------------------

Dask client closed.
[1702579578.440170] [rno1-m02-d03-dgx1-066:1727970:0]          parser.c:2036 UCX  WARN  unused environment variable: UCX_MEMTYPE_CACHE (maybe: UCX_MEMTYPE_CACHE?)
[1702579578.440170] [rno1-m02-d03-dgx1-066:1727970:0]          parser.c:2036 UCX  WARN  (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
1727660 /bin/bash /scripts/mg_utils/run-dask-process.sh scheduler workers
1727714 /opt/conda/bin/python3.10 /opt/conda/bin/dask-scheduler --protocol=tcp --scheduler-file /scripts/mg_utils/dask_scheduler.json
1727804 /opt/conda/bin/python /opt/conda/bin/dask-cuda-worker --rmm-pool-size=28G --rmm-async --local-directory=/tmp/abarghi --scheduler-file=/scripts/mg_utils/dask_scheduler.json --memory-limit=auto --device-memory-limit=auto
1726587 /usr/bin/python2 /usr/local/dcgm-nvdataflow/DcgmNVDataflowPoster.py
1727808 /opt/conda/bin/python -c from multiprocessing.resource_tracker import main;main(45)
1727811 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=54) --multiprocessing-fork
1727815 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=59) --multiprocessing-fork
1727819 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=68) --multiprocessing-fork
1727824 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=77) --multiprocessing-fork
1727828 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=84) --multiprocessing-fork
1727832 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=91) --multiprocessing-fork
1727836 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=98) --multiprocessing-fork
1727839 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=101) --multiprocessing-fork
pkill: killing pid 1726587 failed: Operation not permitted
1726587 /usr/bin/python2 /usr/local/dcgm-nvdataflow/DcgmNVDataflowPoster.py
1727808 /opt/conda/bin/python -c from multiprocessing.resource_tracker import main;main(45)
1727811 python
1727815 python
1727819 python
1727824 python
1727828 python
1727832 python
1727836 python
1727839 python
1727714 /opt/conda/bin/python3.10 /opt/conda/bin/dask-scheduler --protocol=tcp --scheduler-file /scripts/mg_utils/dask_scheduler.json
1727804 /opt/conda/bin/python /opt/conda/bin/dask-cuda-worker --rmm-pool-size=28G --rmm-async --local-directory=/tmp/abarghi --scheduler-file=/scripts/mg_utils/dask_scheduler.json --memory-limit=auto --device-memory-limit=auto
693105 /bin/bash /scripts/mg_utils/run-dask-process.sh workers
693150 /opt/conda/bin/python /opt/conda/bin/dask-cuda-worker --rmm-pool-size=28G --rmm-async --local-directory=/tmp/abarghi --scheduler-file=/scripts/mg_utils/dask_scheduler.json --memory-limit=auto --device-memory-limit=auto
692009 /usr/bin/python2 /usr/local/dcgm-nvdataflow/DcgmNVDataflowPoster.py
693167 /opt/conda/bin/python -c from multiprocessing.resource_tracker import main;main(45)
693170 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=52) --multiprocessing-fork
693175 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=63) --multiprocessing-fork
693178 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=68) --multiprocessing-fork
693182 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=71) --multiprocessing-fork
693187 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=84) --multiprocessing-fork
693191 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=91) --multiprocessing-fork
693195 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=98) --multiprocessing-fork
693198 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=46, pipe_handle=101) --multiprocessing-fork
pkill: killing pid 692009 failed: Operation not permitted
692009 /usr/bin/python2 /usr/local/dcgm-nvdataflow/DcgmNVDataflowPoster.py
693167 /opt/conda/bin/python -c from multiprocessing.resource_tracker import main;main(45)
693170 python
693175 python
693178 python
693182 python
693187 python
693191 python
693195 python
693198 python
693150 /opt/conda/bin/python /opt/conda/bin/dask-cuda-worker --rmm-pool-size=28G --rmm-async --local-directory=/tmp/abarghi --scheduler-file=/scripts/mg_utils/dask_scheduler.json --memory-limit=auto --device-memory-limit=auto
srun: Job 1581196 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 1581196
[2023-12-14 10:49:27,128] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2023-12-14 10:49:27,128] torch.distributed.run: [WARNING] 
[2023-12-14 10:49:27,128] torch.distributed.run: [WARNING] *****************************************
[2023-12-14 10:49:27,128] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-12-14 10:49:27,128] torch.distributed.run: [WARNING] *****************************************
[2023-12-14 10:49:27,197] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[W socket.cpp:436] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[2023-12-14 10:49:27,198] torch.distributed.run: [WARNING] 
[2023-12-14 10:49:27,198] torch.distributed.run: [WARNING] *****************************************
[2023-12-14 10:49:27,198] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-12-14 10:49:27,198] torch.distributed.run: [WARNING] *****************************************
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:436] [c10d] The server socket cannot be initialized on [::]:56505 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
worker initialized
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
[W socket.cpp:663] [c10d] The client socket cannot be initialized to connect to [rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net]:56505 (errno: 97 - Address family not supported by protocol).
worker initialized
worker initialized
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 234, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 208, in main
    from trainers_cugraph_pyg import PyGCuGraphTrainer
  File "/scripts/trainers_cugraph_pyg.py", line 14, in <module>
    from trainers_pyg import PyGTrainer
  File "/scripts/trainers_pyg.py", line 17, in <module>
    from models_pyg import GraphSAGE
  File "/scripts/models_pyg.py", line 16, in <module>
    from torch_geometric.nn import SAGEConv
ModuleNotFoundError: No module named 'torch_geometric'
[2023-12-14 10:49:53,293] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1729546) of binary: /opt/conda/bin/python
[2023-12-14 10:49:53,293] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 695126) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.1', 'console_scripts', 'torchrun')())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/scripts/bench_cugraph_training.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1729547)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1729548)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1729549)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 1729550)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 1729551)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 1729552)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 1729553)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d03-dgx1-066.nsv.rno1.nvmetal.net
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1729546)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.1', 'console_scripts', 'torchrun')())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/scripts/bench_cugraph_training.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d04-dgx1-067.nsv.rno1.nvmetal.net
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 695127)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d04-dgx1-067.nsv.rno1.nvmetal.net
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 695128)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d04-dgx1-067.nsv.rno1.nvmetal.net
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 695129)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d04-dgx1-067.nsv.rno1.nvmetal.net
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 695131)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d04-dgx1-067.nsv.rno1.nvmetal.net
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 695132)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d04-dgx1-067.nsv.rno1.nvmetal.net
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 695133)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d04-dgx1-067.nsv.rno1.nvmetal.net
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 695134)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-12-14_10:49:53
  host      : rno1-m02-d04-dgx1-067.nsv.rno1.nvmetal.net
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 695126)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: rno1-m02-d04-dgx1-067: task 1: Exited with exit code 1
srun: Terminating job step 1581196.2
srun: error: rno1-m02-d03-dgx1-066: task 0: Exited with exit code 1
