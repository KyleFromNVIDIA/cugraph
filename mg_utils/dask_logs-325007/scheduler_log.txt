RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 17:38:36,892 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 17:38:37,402 - distributed.scheduler - INFO - State start
2023-06-26 17:38:37,403 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3p41hask', purging
2023-06-26 17:38:37,403 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-sgxc5bh8', purging
2023-06-26 17:38:37,403 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3dj3fqjd', purging
2023-06-26 17:38:37,404 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-re4o3b_i', purging
2023-06-26 17:38:37,404 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-l2tmcw08', purging
2023-06-26 17:38:37,404 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3kbd6gwq', purging
2023-06-26 17:38:37,404 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-d213aadg', purging
2023-06-26 17:38:37,404 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jz9dzga3', purging
2023-06-26 17:38:37,404 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-q82uhg07', purging
2023-06-26 17:38:37,405 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-572bo4zj', purging
2023-06-26 17:38:37,405 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ijf4lhyn', purging
2023-06-26 17:38:37,405 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-h7hxog4l', purging
2023-06-26 17:38:37,405 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xpsfaxgi', purging
2023-06-26 17:38:37,405 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-l7n4z8a3', purging
2023-06-26 17:38:37,406 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vmwwx54u', purging
2023-06-26 17:38:37,418 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 17:38:37,418 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 17:38:37,419 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 17:38:54,992 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44611', status: init, memory: 0, processing: 0>
2023-06-26 17:38:54,994 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44611
2023-06-26 17:38:54,994 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45062
2023-06-26 17:38:55,723 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43393', status: init, memory: 0, processing: 0>
2023-06-26 17:38:55,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43393
2023-06-26 17:38:55,724 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45072
2023-06-26 17:38:56,181 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43099', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,181 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43099
2023-06-26 17:38:56,181 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45088
2023-06-26 17:38:56,289 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42235', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,289 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42235
2023-06-26 17:38:56,289 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45092
2023-06-26 17:38:56,323 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38281', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38281
2023-06-26 17:38:56,324 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45100
2023-06-26 17:38:56,377 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40327', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,377 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40327
2023-06-26 17:38:56,377 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45108
2023-06-26 17:38:56,783 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34037', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,784 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34037
2023-06-26 17:38:56,784 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45118
2023-06-26 17:38:56,840 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43105', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,840 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43105
2023-06-26 17:38:56,840 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45122
2023-06-26 17:38:56,856 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41711', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,857 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41711
2023-06-26 17:38:56,857 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45126
2023-06-26 17:38:56,882 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40277', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,882 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40277
2023-06-26 17:38:56,882 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45146
2023-06-26 17:38:56,885 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41343', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41343
2023-06-26 17:38:56,885 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45130
2023-06-26 17:38:56,900 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40335', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,900 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40335
2023-06-26 17:38:56,900 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45148
2023-06-26 17:38:56,907 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38299', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,907 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38299
2023-06-26 17:38:56,907 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45158
2023-06-26 17:38:56,913 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34727', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34727
2023-06-26 17:38:56,913 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45164
2023-06-26 17:38:56,923 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36569', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,923 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36569
2023-06-26 17:38:56,923 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45176
2023-06-26 17:38:56,961 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34603', status: init, memory: 0, processing: 0>
2023-06-26 17:38:56,962 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34603
2023-06-26 17:38:56,962 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45182
2023-06-26 17:39:01,503 - distributed.scheduler - INFO - Receive client connection: Client-56682b9d-1448-11ee-b68e-5cff35c1a711
2023-06-26 17:39:01,503 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:37188
2023-06-26 17:39:02,219 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 17:39:12,880 - distributed.scheduler - INFO - Remove client Client-56682b9d-1448-11ee-b68e-5cff35c1a711
2023-06-26 17:39:12,880 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:37188; closing.
2023-06-26 17:39:12,880 - distributed.scheduler - INFO - Remove client Client-56682b9d-1448-11ee-b68e-5cff35c1a711
2023-06-26 17:39:12,881 - distributed.scheduler - INFO - Close client connection: Client-56682b9d-1448-11ee-b68e-5cff35c1a711
2023-06-26 17:39:14,833 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 17:39:14,834 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:40327 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:37746 remote=tcp://10.120.104.11:40327>: Stream is closed
2023-06-26 17:39:14,834 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:34603 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:58336 remote=tcp://10.120.104.11:34603>: Stream is closed
2023-06-26 17:39:14,834 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:43105 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:54684 remote=tcp://10.120.104.11:43105>: Stream is closed
2023-06-26 17:39:14,834 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:43393 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:35890 remote=tcp://10.120.104.11:43393>: Stream is closed
2023-06-26 17:39:14,834 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:42235 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:33198 remote=tcp://10.120.104.11:42235>: Stream is closed
2023-06-26 17:39:14,835 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 17:39:14,835 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:44611 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:55576 remote=tcp://10.120.104.11:44611>: Stream is closed
2023-06-26 17:39:14,835 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:41711 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:53878 remote=tcp://10.120.104.11:41711>: Stream is closed
2023-06-26 17:39:14,835 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:43099 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:48476 remote=tcp://10.120.104.11:43099>: Stream is closed
2023-06-26 17:39:14,836 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:41343 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:48632 remote=tcp://10.120.104.11:41343>: Stream is closed
2023-06-26 17:39:14,836 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:40277 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:37378 remote=tcp://10.120.104.11:40277>: Stream is closed
2023-06-26 17:39:14,836 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:36569 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:43866 remote=tcp://10.120.104.11:36569>: Stream is closed
2023-06-26 17:39:14,836 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:34727 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:48144 remote=tcp://10.120.104.11:34727>: Stream is closed
2023-06-26 17:39:14,836 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:38299 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:59734 remote=tcp://10.120.104.11:38299>: Stream is closed
2023-06-26 17:39:14,836 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:38281 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:34600 remote=tcp://10.120.104.11:38281>: Stream is closed
2023-06-26 17:39:14,836 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:34037 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:34534 remote=tcp://10.120.104.11:34037>: Stream is closed
2023-06-26 17:39:14,836 - distributed.core - INFO - Connection to tcp://10.120.104.11:45122 has been closed.
2023-06-26 17:39:14,836 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43105', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,836 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43105
2023-06-26 17:39:14,837 - distributed.core - INFO - Connection to tcp://10.120.104.11:45182 has been closed.
2023-06-26 17:39:14,837 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34603', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,837 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34603
2023-06-26 17:39:14,837 - distributed.scheduler - ERROR - broadcast to tcp://10.120.104.11:40335 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://10.120.104.11:34400 remote=tcp://10.120.104.11:40335>: Stream is closed
2023-06-26 17:39:14,838 - distributed.core - INFO - Connection to tcp://10.120.104.11:45088 has been closed.
2023-06-26 17:39:14,838 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43099', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,838 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43099
2023-06-26 17:39:14,838 - distributed.core - INFO - Connection to tcp://10.120.104.11:45176 has been closed.
2023-06-26 17:39:14,838 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36569', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,838 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36569
2023-06-26 17:39:14,838 - distributed.core - INFO - Connection to tcp://10.120.104.11:45108 has been closed.
2023-06-26 17:39:14,838 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40327', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,838 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40327
2023-06-26 17:39:14,839 - distributed.core - INFO - Connection to tcp://10.120.104.11:45092 has been closed.
2023-06-26 17:39:14,839 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42235', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,839 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42235
2023-06-26 17:39:14,839 - distributed.core - INFO - Connection to tcp://10.120.104.11:45072 has been closed.
2023-06-26 17:39:14,839 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43393', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,839 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43393
2023-06-26 17:39:14,839 - distributed.core - INFO - Connection to tcp://10.120.104.11:45062 has been closed.
2023-06-26 17:39:14,839 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44611', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,839 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44611
2023-06-26 17:39:14,839 - distributed.core - INFO - Connection to tcp://10.120.104.11:45130 has been closed.
2023-06-26 17:39:14,839 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41343', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,839 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41343
2023-06-26 17:39:14,839 - distributed.core - INFO - Connection to tcp://10.120.104.11:45126 has been closed.
2023-06-26 17:39:14,840 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41711', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,840 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41711
2023-06-26 17:39:14,840 - distributed.core - INFO - Connection to tcp://10.120.104.11:45146 has been closed.
2023-06-26 17:39:14,840 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40277', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,840 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40277
2023-06-26 17:39:14,841 - distributed.core - INFO - Connection to tcp://10.120.104.11:45164 has been closed.
2023-06-26 17:39:14,841 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34727', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,841 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34727
2023-06-26 17:39:14,841 - distributed.core - INFO - Connection to tcp://10.120.104.11:45100 has been closed.
2023-06-26 17:39:14,841 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38281', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,841 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38281
2023-06-26 17:39:14,841 - distributed.core - INFO - Connection to tcp://10.120.104.11:45118 has been closed.
2023-06-26 17:39:14,841 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34037', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,841 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34037
2023-06-26 17:39:14,841 - distributed.core - INFO - Connection to tcp://10.120.104.11:45158 has been closed.
2023-06-26 17:39:14,841 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38299', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,841 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38299
2023-06-26 17:39:14,842 - distributed.core - INFO - Connection to tcp://10.120.104.11:45148 has been closed.
2023-06-26 17:39:14,842 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40335', status: running, memory: 0, processing: 0>
2023-06-26 17:39:14,842 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40335
2023-06-26 17:39:14,842 - distributed.scheduler - INFO - Lost all workers
2023-06-26 17:39:14,842 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 17:39:14,843 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45118>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45118>: Stream is closed
2023-06-26 17:39:14,843 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45164>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45164>: Stream is closed
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45176>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45100>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45100>: Stream is closed
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45158>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45158>: Stream is closed
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45146>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45108>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45148>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45148>: Stream is closed
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45130>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45126>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45092>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,845 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45088>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,845 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45072>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,845 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:45062>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:39:14,846 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 17:39:14,847 - distributed.scheduler - INFO - End scheduler
