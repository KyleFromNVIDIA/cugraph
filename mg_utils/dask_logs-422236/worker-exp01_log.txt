RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 19:17:38,412 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34199'
2023-06-26 19:17:38,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43475'
2023-06-26 19:17:38,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33215'
2023-06-26 19:17:38,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39865'
2023-06-26 19:17:38,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40791'
2023-06-26 19:17:38,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42959'
2023-06-26 19:17:38,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36021'
2023-06-26 19:17:38,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35005'
2023-06-26 19:17:38,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36219'
2023-06-26 19:17:38,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44623'
2023-06-26 19:17:38,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35941'
2023-06-26 19:17:38,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44563'
2023-06-26 19:17:38,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35945'
2023-06-26 19:17:38,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33843'
2023-06-26 19:17:38,447 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34397'
2023-06-26 19:17:38,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43607'
2023-06-26 19:17:40,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,190 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,190 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,221 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,221 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,221 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,221 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,232 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,232 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,234 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,235 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,235 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,245 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:17:40,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:17:40,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,307 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,327 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,362 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,369 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,396 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:40,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:17:46,649 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33885
2023-06-26 19:17:46,649 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33885
2023-06-26 19:17:46,650 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42141
2023-06-26 19:17:46,650 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:46,650 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:46,650 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:46,650 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:46,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c5dirqk6
2023-06-26 19:17:46,650 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40c79cf3-a0fb-4556-9d3c-24d4bef44673
2023-06-26 19:17:46,701 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41745
2023-06-26 19:17:46,702 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41745
2023-06-26 19:17:46,702 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37381
2023-06-26 19:17:46,702 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:46,702 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:46,702 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:46,702 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:46,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-plowr_km
2023-06-26 19:17:46,703 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8196a58c-e0a3-4f00-971f-677d78c14f83
2023-06-26 19:17:46,703 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bcfda8fe-0f9f-4917-9e30-2163d6d67d1b
2023-06-26 19:17:46,718 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39837
2023-06-26 19:17:46,718 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39837
2023-06-26 19:17:46,719 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44841
2023-06-26 19:17:46,719 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:46,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:46,719 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:46,719 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:46,719 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ajvjkz1b
2023-06-26 19:17:46,719 - distributed.worker - INFO - Starting Worker plugin PreImport-0d679d2d-4535-4a93-a021-d0938b093e18
2023-06-26 19:17:46,719 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee149d23-963f-4c21-adf2-bb91c550b57e
2023-06-26 19:17:46,725 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34837
2023-06-26 19:17:46,726 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34837
2023-06-26 19:17:46,726 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41887
2023-06-26 19:17:46,726 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:46,726 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:46,726 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:46,726 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:46,726 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zsiql71v
2023-06-26 19:17:46,727 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b255a00b-23b5-4979-a7df-696d94e9f64f
2023-06-26 19:17:47,027 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32825
2023-06-26 19:17:47,027 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32825
2023-06-26 19:17:47,027 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41239
2023-06-26 19:17:47,027 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,027 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,027 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,027 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,027 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ro_xehw
2023-06-26 19:17:47,028 - distributed.worker - INFO - Starting Worker plugin RMMSetup-71fec1c7-d799-46a5-871a-eaa730731040
2023-06-26 19:17:47,128 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34995
2023-06-26 19:17:47,129 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34995
2023-06-26 19:17:47,129 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45829
2023-06-26 19:17:47,129 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,129 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,129 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,129 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,129 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fd6kiqlz
2023-06-26 19:17:47,130 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cbcdf036-c0f7-4807-8155-54271f850066
2023-06-26 19:17:47,166 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46345
2023-06-26 19:17:47,167 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46345
2023-06-26 19:17:47,167 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45331
2023-06-26 19:17:47,167 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,167 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,167 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,167 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,167 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_21u309l
2023-06-26 19:17:47,167 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3dec75c4-f5b9-4958-a198-6cf4773f1172
2023-06-26 19:17:47,168 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e14f68f-8c85-48a9-9e69-50c38b51afc3
2023-06-26 19:17:47,177 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45037
2023-06-26 19:17:47,177 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45037
2023-06-26 19:17:47,177 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43445
2023-06-26 19:17:47,177 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,177 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,177 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,177 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,177 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x30hi5yk
2023-06-26 19:17:47,177 - distributed.worker - INFO - Starting Worker plugin PreImport-5ad883ec-f36e-4387-894d-63c65af62e86
2023-06-26 19:17:47,178 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7060f464-3f29-4a00-b942-23ff9f6549c0
2023-06-26 19:17:47,180 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43469
2023-06-26 19:17:47,180 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43469
2023-06-26 19:17:47,180 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41471
2023-06-26 19:17:47,180 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,180 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,180 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,180 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,180 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y3v2yicv
2023-06-26 19:17:47,181 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6557b423-b898-49f4-abb4-c446d94218d3
2023-06-26 19:17:47,190 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34185
2023-06-26 19:17:47,190 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34185
2023-06-26 19:17:47,191 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42877
2023-06-26 19:17:47,191 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,191 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,191 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,191 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9t5sc8uh
2023-06-26 19:17:47,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e22aaaad-d1e5-430c-935a-a195eb25c854
2023-06-26 19:17:47,208 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35477
2023-06-26 19:17:47,208 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35477
2023-06-26 19:17:47,208 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46125
2023-06-26 19:17:47,208 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,209 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,209 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,209 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,209 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-66ph4ci3
2023-06-26 19:17:47,209 - distributed.worker - INFO - Starting Worker plugin PreImport-a39ceeb0-85d1-4ba2-9a60-3331010304af
2023-06-26 19:17:47,209 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9409b33-414f-4789-8d11-81dd321881c2
2023-06-26 19:17:47,254 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32903
2023-06-26 19:17:47,254 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32903
2023-06-26 19:17:47,254 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44293
2023-06-26 19:17:47,255 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,255 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,255 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,255 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,255 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-64fap92v
2023-06-26 19:17:47,255 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0453bf97-896f-407d-8bdf-ba33dae81244
2023-06-26 19:17:47,280 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45727
2023-06-26 19:17:47,280 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45727
2023-06-26 19:17:47,280 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35655
2023-06-26 19:17:47,281 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,281 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,281 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,281 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,281 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6r7689mn
2023-06-26 19:17:47,281 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df70b9ec-d8dc-418c-a5ab-0f38245403a9
2023-06-26 19:17:47,283 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d12ec38-8894-40fb-9e2f-be549fcb6eac
2023-06-26 19:17:47,320 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46535
2023-06-26 19:17:47,320 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46535
2023-06-26 19:17:47,320 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38125
2023-06-26 19:17:47,321 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,321 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,321 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,321 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m3dpoza3
2023-06-26 19:17:47,321 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33153
2023-06-26 19:17:47,321 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33153
2023-06-26 19:17:47,321 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38291
2023-06-26 19:17:47,321 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,321 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,321 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,321 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4cq2kpw3
2023-06-26 19:17:47,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bce7df2f-cc30-4273-a8f1-8cd312bcbea1
2023-06-26 19:17:47,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-15574a2e-d5c0-4dd8-932b-eb7e7f276b14
2023-06-26 19:17:47,322 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35511
2023-06-26 19:17:47,322 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35511
2023-06-26 19:17:47,322 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43047
2023-06-26 19:17:47,322 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:17:47,322 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:47,322 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:17:47,322 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:17:47,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-to8o4zur
2023-06-26 19:17:47,322 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28e5585b-860f-41c5-b110-bb92cebb0a7b
2023-06-26 19:17:50,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a3b8116-3c55-4f70-80c1-2036f7cb1be9
2023-06-26 19:17:50,796 - distributed.worker - INFO - Starting Worker plugin PreImport-e1817ce2-4ecd-4370-8c9a-d06c977435ab
2023-06-26 19:17:50,797 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:50,826 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:50,826 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:50,829 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:50,899 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1172653b-967a-4c9c-a314-d15d894f49c7
2023-06-26 19:17:50,899 - distributed.worker - INFO - Starting Worker plugin PreImport-ce9d1142-c0a8-405b-9243-981ad3344613
2023-06-26 19:17:50,900 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:50,923 - distributed.worker - INFO - Starting Worker plugin PreImport-61160b50-7650-4f60-b734-d43ba5fd4539
2023-06-26 19:17:50,924 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:50,932 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:50,932 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:50,934 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:50,949 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:50,949 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:50,951 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,018 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a31136ad-8850-4796-9daf-d5c9f214d1f7
2023-06-26 19:17:51,019 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,044 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,044 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,047 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,107 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1354b20-1aec-46a0-af16-1d21d120b9f5
2023-06-26 19:17:51,107 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,123 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,124 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,125 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,134 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aac1ecc1-e3f2-47dc-bcc2-b1f958191aa3
2023-06-26 19:17:51,135 - distributed.worker - INFO - Starting Worker plugin PreImport-9feddcc3-bdc7-404c-9e60-562112fe32e3
2023-06-26 19:17:51,136 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,167 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,167 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,170 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87360085-c968-4460-b0a8-8506e22d0acb
2023-06-26 19:17:51,172 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e923e9c0-aae0-4f8a-8951-512cb78f0c17
2023-06-26 19:17:51,187 - distributed.worker - INFO - Starting Worker plugin PreImport-9dffbdbc-a7f2-456a-9c04-e387056d906b
2023-06-26 19:17:51,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,188 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,189 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,190 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,204 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,204 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,206 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,239 - distributed.worker - INFO - Starting Worker plugin PreImport-3aa605f9-85f5-4581-b271-b66310e2d0fe
2023-06-26 19:17:51,241 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d04164d9-6095-4a41-99ee-c979a88fffd9
2023-06-26 19:17:51,254 - distributed.worker - INFO - Starting Worker plugin PreImport-dce7a7cb-8de6-4665-b342-bd8f1794ad80
2023-06-26 19:17:51,255 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a71baef-f2c0-4005-8d23-248c44052373
2023-06-26 19:17:51,256 - distributed.worker - INFO - Starting Worker plugin PreImport-82b054e4-fecd-48ce-9cdc-e1b0728107df
2023-06-26 19:17:51,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17b808b8-c343-4bbc-8b83-aece71fe1622
2023-06-26 19:17:51,257 - distributed.worker - INFO - Starting Worker plugin PreImport-1adfb6bb-87b4-4ade-848a-940523eb3ec1
2023-06-26 19:17:51,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,258 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,263 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,263 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,264 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,270 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,270 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,271 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,271 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,272 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,273 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,274 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a33ba2ae-3141-45a2-928f-44046eb296a4
2023-06-26 19:17:51,274 - distributed.worker - INFO - Starting Worker plugin PreImport-792310af-f880-4577-8e99-22687ea78412
2023-06-26 19:17:51,275 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,275 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,276 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,277 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,289 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db0efd92-3eed-4bf0-aed0-1bcfe4cb66d1
2023-06-26 19:17:51,289 - distributed.worker - INFO - Starting Worker plugin PreImport-e1583a19-3e66-4441-a131-828e737d0145
2023-06-26 19:17:51,290 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,291 - distributed.worker - INFO - Starting Worker plugin PreImport-3f470186-c9e7-41f0-b18b-9be055bf0d86
2023-06-26 19:17:51,293 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,302 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b02a96c-70e1-4f37-a789-a292ba1ee530
2023-06-26 19:17:51,302 - distributed.worker - INFO - Starting Worker plugin PreImport-335e6c74-5008-4f3e-8948-71d0382179c4
2023-06-26 19:17:51,303 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,304 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,304 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,304 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,304 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,305 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,307 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,313 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,313 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,315 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:51,322 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:17:51,322 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:17:51,323 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:17:59,943 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,943 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,943 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,944 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,944 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,944 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,944 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,945 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,946 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,946 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,946 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,949 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,949 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,950 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,952 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,953 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:17:59,962 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,962 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,962 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:17:59,963 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,635 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:00,636 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:18:03,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:16,226 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,245 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,329 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,375 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,386 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,392 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,430 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,619 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,666 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,708 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,759 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,804 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,805 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,842 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,878 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:16,954 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:18:23,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:18:23,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:02,054 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,054 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,055 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,060 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,060 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,060 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,060 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:02,061 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:19:05,245 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,251 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,251 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,251 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,251 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,252 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,252 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,252 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,252 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,252 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,252 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,252 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,252 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,253 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,253 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:05,253 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:10,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:10,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,844 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,845 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,845 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,845 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,845 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,845 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,859 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,859 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:19:17,876 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,876 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,876 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,876 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,876 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,876 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:17,877 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:19:21,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:22,385 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,387 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,387 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32825. Reason: scheduler-restart
2023-06-26 19:19:22,388 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,388 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32903. Reason: scheduler-restart
2023-06-26 19:19:22,388 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,388 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,389 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,389 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33153. Reason: scheduler-restart
2023-06-26 19:19:22,389 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33885. Reason: scheduler-restart
2023-06-26 19:19:22,389 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,389 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,390 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34185. Reason: scheduler-restart
2023-06-26 19:19:22,390 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,390 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,391 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,391 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32825
2023-06-26 19:19:22,391 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34837. Reason: scheduler-restart
2023-06-26 19:19:22,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,391 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34995. Reason: scheduler-restart
2023-06-26 19:19:22,391 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35477. Reason: scheduler-restart
2023-06-26 19:19:22,391 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,391 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,392 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35511. Reason: scheduler-restart
2023-06-26 19:19:22,392 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,392 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,392 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,392 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,392 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39837. Reason: scheduler-restart
2023-06-26 19:19:22,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,393 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,393 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,393 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,393 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41745. Reason: scheduler-restart
2023-06-26 19:19:22,393 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,394 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43469. Reason: scheduler-restart
2023-06-26 19:19:22,394 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,394 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,395 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,395 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,395 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:19:22,395 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,397 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45037. Reason: scheduler-restart
2023-06-26 19:19:22,398 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45727. Reason: scheduler-restart
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32903
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33153
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34185
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34837
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34995
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35477
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35511
2023-06-26 19:19:22,401 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39837
2023-06-26 19:19:22,402 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32903
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32903
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33153
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33153
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34185
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34185
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34837
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34837
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34995
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34995
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35477
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35477
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35511
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35511
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39837
2023-06-26 19:19:22,402 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39837
2023-06-26 19:19:22,402 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,403 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,403 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,415 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46535. Reason: scheduler-restart
2023-06-26 19:19:22,416 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46345. Reason: scheduler-restart
2023-06-26 19:19:22,416 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32903
2023-06-26 19:19:22,416 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33153
2023-06-26 19:19:22,416 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 19:19:22,416 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34185
2023-06-26 19:19:22,416 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34837
2023-06-26 19:19:22,416 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34995
2023-06-26 19:19:22,416 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35477
2023-06-26 19:19:22,417 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35511
2023-06-26 19:19:22,417 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39837
2023-06-26 19:19:22,417 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41745
2023-06-26 19:19:22,417 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43469
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45037
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32903
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33153
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45727
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34185
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34837
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34995
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35477
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35511
2023-06-26 19:19:22,418 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39837
2023-06-26 19:19:22,418 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,419 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,419 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41745
2023-06-26 19:19:22,419 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43469
2023-06-26 19:19:22,419 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45037
2023-06-26 19:19:22,420 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45727
2023-06-26 19:19:22,420 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46535
2023-06-26 19:19:22,420 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:19:22,421 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,422 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,428 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,439 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:22,448 - distributed.nanny - INFO - Worker closed
2023-06-26 19:19:24,390 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:25,538 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:26,028 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:26,665 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:26,666 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:26,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:26,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:26,851 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:27,796 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,365 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:28,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:28,573 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,589 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,590 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:28,590 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:28,590 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,592 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,596 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,598 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,601 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,748 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:28,814 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:28,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:28,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:28,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:28,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:28,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:28,871 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:19:29,002 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:29,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:29,370 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:29,370 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:29,541 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,198 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,386 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,386 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,491 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,491 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,498 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,498 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,510 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,511 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,519 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,521 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,565 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:19:30,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:19:30,594 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,599 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36517
2023-06-26 19:19:30,600 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36517
2023-06-26 19:19:30,600 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40537
2023-06-26 19:19:30,600 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:30,600 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:30,600 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:30,600 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:30,600 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tp7xu2wc
2023-06-26 19:19:30,601 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86fa681b-9790-431c-a02f-7bf1be4ddedd
2023-06-26 19:19:30,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,690 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:30,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:19:31,157 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35979
2023-06-26 19:19:31,158 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35979
2023-06-26 19:19:31,158 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42833
2023-06-26 19:19:31,158 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:31,158 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:31,158 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:31,158 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:31,158 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w71i0vsf
2023-06-26 19:19:31,158 - distributed.worker - INFO - Starting Worker plugin PreImport-2c4cab8b-5e5e-485e-8697-3051f34bfa60
2023-06-26 19:19:31,158 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f5896bd-5d3c-42ba-bf43-973586f822ac
2023-06-26 19:19:31,173 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43913
2023-06-26 19:19:31,173 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43913
2023-06-26 19:19:31,173 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39687
2023-06-26 19:19:31,173 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:31,173 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:31,173 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:31,173 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:31,173 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g6v1bucf
2023-06-26 19:19:31,174 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbdbbf6d-935e-4303-b79c-026ac1edb617
2023-06-26 19:19:31,825 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36663
2023-06-26 19:19:31,825 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36663
2023-06-26 19:19:31,825 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38469
2023-06-26 19:19:31,825 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:31,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:31,825 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:31,825 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:31,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-98oqzfsw
2023-06-26 19:19:31,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b329e290-9e11-47b1-9993-257942489d6e
2023-06-26 19:19:31,828 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40119
2023-06-26 19:19:31,828 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40119
2023-06-26 19:19:31,828 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34269
2023-06-26 19:19:31,828 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:31,828 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:31,828 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:31,828 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:31,828 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5o35fvc6
2023-06-26 19:19:31,829 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5740b16-ae64-4413-8666-e3590c633c61
2023-06-26 19:19:32,987 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42297
2023-06-26 19:19:32,987 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42297
2023-06-26 19:19:32,987 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38135
2023-06-26 19:19:32,987 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:32,987 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:32,988 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:32,988 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:32,988 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hyidpy3a
2023-06-26 19:19:32,989 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87cb953a-529d-4f46-85bf-ce0cf3e03486
2023-06-26 19:19:33,512 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e323071-5914-47aa-9ae1-4f8dd1caec02
2023-06-26 19:19:33,513 - distributed.worker - INFO - Starting Worker plugin PreImport-32fc037a-2c23-490c-9859-030c17dc9618
2023-06-26 19:19:33,514 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:33,531 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:33,532 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:33,533 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:33,806 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7630921-ca56-43f7-93b5-5646961a253b
2023-06-26 19:19:33,808 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26acc1ac-6823-4b3c-ad5f-a4c4a09404fc
2023-06-26 19:19:33,808 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:33,808 - distributed.worker - INFO - Starting Worker plugin PreImport-34c19c24-e442-4927-800d-a5ed8835c05e
2023-06-26 19:19:33,809 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:33,822 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:33,822 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:33,824 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:33,825 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:33,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:33,828 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:34,104 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0a9ef4f-a37e-4f16-927b-28ff036e2878
2023-06-26 19:19:34,105 - distributed.worker - INFO - Starting Worker plugin PreImport-864f5f22-304b-4a42-adc3-099ee2ffb485
2023-06-26 19:19:34,106 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:34,124 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:34,124 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:34,127 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:34,132 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b8032a8b-0b14-4dbd-9641-92f883f44fc5
2023-06-26 19:19:34,133 - distributed.worker - INFO - Starting Worker plugin PreImport-9840bf4f-b3c8-40d9-aff4-ebd254b8bd0d
2023-06-26 19:19:34,135 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:34,152 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:34,152 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:34,155 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:35,805 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-43b396b7-7f5a-4196-a624-3c3364565194
2023-06-26 19:19:35,806 - distributed.worker - INFO - Starting Worker plugin PreImport-dd995fcc-df96-4f1e-b370-a6ff9d8d8874
2023-06-26 19:19:35,808 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:35,823 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:35,823 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:35,825 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:36,077 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45945
2023-06-26 19:19:36,077 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45945
2023-06-26 19:19:36,077 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44657
2023-06-26 19:19:36,077 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:36,078 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:36,078 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:36,078 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:36,078 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6a6z423a
2023-06-26 19:19:36,078 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c59fc84-3138-4b7b-ad83-1de0cff27692
2023-06-26 19:19:36,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-924bbf2d-670f-42cd-af23-a916c4e23151
2023-06-26 19:19:36,425 - distributed.worker - INFO - Starting Worker plugin PreImport-e0fefc2a-cf7a-42bb-8ad2-f247a8eb856b
2023-06-26 19:19:36,426 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:36,438 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:36,438 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:36,443 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:36,902 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39689
2023-06-26 19:19:36,902 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39689
2023-06-26 19:19:36,902 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46551
2023-06-26 19:19:36,902 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:36,902 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:36,902 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:36,902 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:36,902 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w9g0t3m6
2023-06-26 19:19:36,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63b1d202-441a-450d-823d-0849a12cca94
2023-06-26 19:19:36,903 - distributed.worker - INFO - Starting Worker plugin PreImport-8b8fca61-02de-4d23-a776-cc542957ef9b
2023-06-26 19:19:36,903 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76876884-4389-4159-8ed4-ed2dfd31aa1a
2023-06-26 19:19:36,908 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34959
2023-06-26 19:19:36,908 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34959
2023-06-26 19:19:36,908 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46123
2023-06-26 19:19:36,908 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:36,908 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:36,908 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:36,908 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:36,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-txxsu3_a
2023-06-26 19:19:36,909 - distributed.worker - INFO - Starting Worker plugin PreImport-4409b868-8e0c-4a55-9fb8-4307efba71bb
2023-06-26 19:19:36,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43aca5f6-0dcf-496a-92d6-2c75f1e876f6
2023-06-26 19:19:37,371 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40681
2023-06-26 19:19:37,371 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40681
2023-06-26 19:19:37,372 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38763
2023-06-26 19:19:37,372 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:37,372 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:37,372 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:37,372 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:37,372 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y03fi40f
2023-06-26 19:19:37,372 - distributed.worker - INFO - Starting Worker plugin PreImport-6e9e727e-b075-4eec-a354-90b7e6fad8ee
2023-06-26 19:19:37,372 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ebf734c-5a77-4d71-aa69-e764be9deec4
2023-06-26 19:19:37,398 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35921
2023-06-26 19:19:37,398 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35921
2023-06-26 19:19:37,398 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41065
2023-06-26 19:19:37,398 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:37,398 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:37,398 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:37,399 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:37,399 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-exdxdx5d
2023-06-26 19:19:37,399 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e36a5d11-cb68-4b0f-bb38-6b650cd3195a
2023-06-26 19:19:37,399 - distributed.worker - INFO - Starting Worker plugin PreImport-2297b8fe-9e71-4aab-bfb0-c00f99bd8151
2023-06-26 19:19:37,399 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d96356db-ea99-4163-a743-33ea703b1e30
2023-06-26 19:19:37,411 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34023
2023-06-26 19:19:37,411 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34023
2023-06-26 19:19:37,411 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32971
2023-06-26 19:19:37,411 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:37,411 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:37,412 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:37,412 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:37,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aluw0hm3
2023-06-26 19:19:37,412 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2884696-9b4d-48a3-b0f1-66a71fb96058
2023-06-26 19:19:37,412 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0933954f-cab1-46fb-be89-97a760c3160a
2023-06-26 19:19:37,415 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38891
2023-06-26 19:19:37,415 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38891
2023-06-26 19:19:37,415 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41511
2023-06-26 19:19:37,415 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:37,415 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:37,415 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:37,415 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:37,415 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-unn2g186
2023-06-26 19:19:37,416 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e131e50f-04fb-4cfd-99eb-6c27d098baee
2023-06-26 19:19:37,428 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46275
2023-06-26 19:19:37,428 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46275
2023-06-26 19:19:37,428 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33775
2023-06-26 19:19:37,428 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:37,428 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:37,428 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:37,428 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:37,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kobf7aub
2023-06-26 19:19:37,429 - distributed.worker - INFO - Starting Worker plugin PreImport-41a1315b-6279-47de-9d0d-bf5e78139e25
2023-06-26 19:19:37,430 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b04200eb-2799-4b95-8456-3250320b395f
2023-06-26 19:19:37,464 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35851
2023-06-26 19:19:37,464 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35851
2023-06-26 19:19:37,464 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44921
2023-06-26 19:19:37,464 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:37,464 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:37,464 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:37,464 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:37,464 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ffpn00n4
2023-06-26 19:19:37,465 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57531557-dad0-4cf8-8c93-70c209bf40ec
2023-06-26 19:19:37,469 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33893
2023-06-26 19:19:37,469 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33893
2023-06-26 19:19:37,469 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37979
2023-06-26 19:19:37,469 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:19:37,469 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:37,469 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:19:37,470 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:19:37,470 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j_s7ex2y
2023-06-26 19:19:37,470 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40887931-c409-431c-aa60-11308399c275
2023-06-26 19:19:37,471 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d8998db-d23e-4845-9210-21bd4b7f02fb
2023-06-26 19:19:39,079 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,094 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,094 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,095 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:39,096 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fdd3b715-5cd5-4c85-94e6-f9c56ac907a6
2023-06-26 19:19:39,097 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,125 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,125 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,127 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:39,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0244ada3-6edf-4464-9646-0d7c900b01db
2023-06-26 19:19:39,543 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,546 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df2e3cec-ace3-4848-a2bf-77a46248e0d1
2023-06-26 19:19:39,546 - distributed.worker - INFO - Starting Worker plugin PreImport-d0d5b826-d26b-4d6d-be99-6b3ecac80497
2023-06-26 19:19:39,546 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,560 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,560 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,561 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:39,563 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,563 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,565 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1049469b-1007-45fd-b4df-1981b8d3320d
2023-06-26 19:19:39,565 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:39,566 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,574 - distributed.worker - INFO - Starting Worker plugin PreImport-d26cd0a4-3035-41c4-8173-c77bd48faaef
2023-06-26 19:19:39,577 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,578 - distributed.worker - INFO - Starting Worker plugin PreImport-46237c75-ca3a-4aea-bb12-5bcfb14f7e4d
2023-06-26 19:19:39,580 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,580 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,581 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,581 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:39,596 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,596 - distributed.worker - INFO - Starting Worker plugin PreImport-769a9f20-cb4f-45ba-9b96-5987b95da917
2023-06-26 19:19:39,596 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4cfcea6-b925-489c-ac59-afceb32dc4b7
2023-06-26 19:19:39,598 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,601 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,601 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,602 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:39,607 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,607 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,608 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,608 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,609 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:39,610 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:39,622 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:19:39,622 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:19:39,624 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:19:48,623 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:48,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:48,836 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:48,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:48,850 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:48,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,271 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,306 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,320 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,364 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,380 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,395 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,420 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,458 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,530 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,601 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,608 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,633 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,742 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:19:49,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:49,754 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,754 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,754 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,755 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:19:49,765 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,765 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,765 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,765 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687807189.766500] [exp01:425291:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:45459) failed: Address already in use
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,766 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,778 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:49,778 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:19:52,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:52,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:53,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:19:54,944 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33893. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:54,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34199'. Reason: nanny-close
2023-06-26 19:19:54,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,945 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43475'. Reason: nanny-close
2023-06-26 19:19:54,946 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,946 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33215'. Reason: nanny-close
2023-06-26 19:19:54,946 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,947 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39865'. Reason: nanny-close
2023-06-26 19:19:54,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,947 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40791'. Reason: nanny-close
2023-06-26 19:19:54,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,947 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36219'. Reason: nanny-close
2023-06-26 19:19:54,948 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,948 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44623'. Reason: nanny-close
2023-06-26 19:19:54,948 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,948 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42959'. Reason: nanny-close
2023-06-26 19:19:54,948 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,949 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35941'. Reason: nanny-close
2023-06-26 19:19:54,949 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,949 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44563'. Reason: nanny-close
2023-06-26 19:19:54,950 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,950 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35945'. Reason: nanny-close
2023-06-26 19:19:54,950 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,950 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33843'. Reason: nanny-close
2023-06-26 19:19:54,950 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34397'. Reason: nanny-close
2023-06-26 19:19:54,951 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36021'. Reason: nanny-close
2023-06-26 19:19:54,951 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,952 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43607'. Reason: nanny-close
2023-06-26 19:19:54,952 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,952 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35005'. Reason: nanny-close
2023-06-26 19:19:54,952 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:19:54,971 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36021 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34130 remote=tcp://10.120.104.11:36021>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36021 after 100 s
2023-06-26 19:19:55,029 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36517. Reason: worker-close
2023-06-26 19:19:55,029 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43913. Reason: worker-close
2023-06-26 19:19:55,029 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38891. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,029 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39689. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,030 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42297. Reason: worker-close
2023-06-26 19:19:55,030 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35921. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,030 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:45380 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py", line 81, in _fd_reader_callback
    worker.progress()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 19:19:55,030 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:45370 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py", line 81, in _fd_reader_callback
    worker.progress()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 19:19:55,032 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40119. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,032 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36663. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,032 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34959. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,032 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35979. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,032 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:45446 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py", line 81, in _fd_reader_callback
    worker.progress()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 19:19:55,033 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45945. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,033 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34023. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,033 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46275. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:19:55,033 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35851. Reason: worker-handle-scheduler-connection-broken
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/contextlib.py", line 279, in helper
    @wraps(func)
KeyboardInterrupt
Exception ignored in: 'ucp._libs.ucx_api._send_callback'
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/contextlib.py", line 279, in helper
    @wraps(func)
KeyboardInterrupt: 
2023-06-26 19:19:55,094 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40681. Reason: nanny-close
2023-06-26 19:19:55,095 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33893
2023-06-26 19:19:55,097 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:8786; closing.

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1510, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 592, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 214, in wait
    await fut
asyncio.exceptions.CancelledError
2023-06-26 19:19:55,135 - distributed.nanny - INFO - Worker closed
[1687807195.196299] [exp01:425326:0]           mpool.c:54   UCX  WARN  object 0x55c9ff0ab380 {flags:0x41 <no debug info>} was not returned to mpool ucp_requests
2023-06-26 19:19:58,153 - distributed.nanny - WARNING - Worker process still alive after 3.1999981689453127 seconds, killing
2023-06-26 19:19:58,153 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 19:19:58,154 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 19:19:58,154 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:19:58,155 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:19:58,155 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:19:58,155 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 19:19:58,158 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:19:58,160 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 19:19:58,160 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:19:58,160 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:19:58,161 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:19:58,161 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 19:19:58,162 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 19:19:58,164 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 19:19:58,945 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,947 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,947 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,947 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,948 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,948 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,948 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,949 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,950 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,950 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,950 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,951 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,951 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,952 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,953 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425346 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425339 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425335 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425332 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425329 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425323 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425320 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425315 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425310 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425291 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425275 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425272 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425255 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425250 parent=422393 started daemon>
2023-06-26 19:19:58,954 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=425239 parent=422393 started daemon>
2023-06-26 19:20:02,511 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 425275 exit status was already read will report exitcode 255
2023-06-26 19:20:03,375 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 425323 exit status was already read will report exitcode 255
