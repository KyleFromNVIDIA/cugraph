RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=12G
             --local-directory=/tmp/
             --scheduler-file=/root/work/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-22 22:40:08,795 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:46073'
2023-06-22 22:40:08,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:45279'
2023-06-22 22:40:08,802 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:44075'
2023-06-22 22:40:08,804 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:37415'
2023-06-22 22:40:08,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:45759'
2023-06-22 22:40:08,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:46383'
2023-06-22 22:40:08,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:39957'
2023-06-22 22:40:08,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:34139'
2023-06-22 22:40:10,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 22:40:10,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 22:40:10,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 22:40:10,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 22:40:10,359 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 22:40:10,359 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 22:40:10,360 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 22:40:10,360 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 22:40:10,360 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 22:40:10,360 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 22:40:10,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 22:40:10,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 22:40:10,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 22:40:10,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 22:40:10,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 22:40:10,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 22:40:10,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 22:40:10,779 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 22:40:10,796 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 22:40:10,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 22:40:10,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 22:40:10,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 22:40:10,811 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 22:40:10,812 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 22:40:13,066 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:34219
2023-06-22 22:40:13,067 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:34219
2023-06-22 22:40:13,067 - distributed.worker - INFO -          dashboard at:        10.33.227.169:41567
2023-06-22 22:40:13,067 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,067 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,067 - distributed.worker - INFO -               Threads:                          1
2023-06-22 22:40:13,067 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 22:40:13,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hck3z5_7
2023-06-22 22:40:13,067 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f5c2bf8-8fdf-4008-85f8-b275154039c6
2023-06-22 22:40:13,068 - distributed.worker - INFO - Starting Worker plugin PreImport-4ae89406-db7f-44ea-a14b-534389cfdaa3
2023-06-22 22:40:13,068 - distributed.worker - INFO - Starting Worker plugin RMMSetup-249561a7-f3ca-40a1-8678-b46588c0f9b3
2023-06-22 22:40:13,071 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:44625
2023-06-22 22:40:13,071 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:44625
2023-06-22 22:40:13,071 - distributed.worker - INFO -          dashboard at:        10.33.227.169:40895
2023-06-22 22:40:13,071 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,071 - distributed.worker - INFO -               Threads:                          1
2023-06-22 22:40:13,071 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 22:40:13,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f26pdshx
2023-06-22 22:40:13,072 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-28cb1230-01ba-431a-a14e-372464707ff3
2023-06-22 22:40:13,073 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:43695
2023-06-22 22:40:13,073 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:43695
2023-06-22 22:40:13,073 - distributed.worker - INFO -          dashboard at:        10.33.227.169:35869
2023-06-22 22:40:13,073 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,073 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,073 - distributed.worker - INFO -               Threads:                          1
2023-06-22 22:40:13,073 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 22:40:13,073 - distributed.worker - INFO - Starting Worker plugin PreImport-bf30a0d9-6403-41ab-b7f1-9198730ec5e7
2023-06-22 22:40:13,073 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nwrirdv5
2023-06-22 22:40:13,073 - distributed.worker - INFO - Starting Worker plugin RMMSetup-476c1bb1-1cb7-4632-ba82-b2389eebffe9
2023-06-22 22:40:13,074 - distributed.worker - INFO - Starting Worker plugin PreImport-1e213305-0b86-4f0e-a410-2b997255b32a
2023-06-22 22:40:13,074 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e2c9f0b-f657-4b96-9d79-2bd6b6d634a8
2023-06-22 22:40:13,074 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25149842-9575-49cd-b9c3-486882c07486
2023-06-22 22:40:13,076 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:42151
2023-06-22 22:40:13,076 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:42151
2023-06-22 22:40:13,076 - distributed.worker - INFO -          dashboard at:        10.33.227.169:41471
2023-06-22 22:40:13,076 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,076 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,076 - distributed.worker - INFO -               Threads:                          1
2023-06-22 22:40:13,076 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 22:40:13,077 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xtxw_a3j
2023-06-22 22:40:13,077 - distributed.worker - INFO - Starting Worker plugin RMMSetup-98c6e760-2f92-4eeb-8e92-9e0fdd42234c
2023-06-22 22:40:13,127 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:36997
2023-06-22 22:40:13,127 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:36997
2023-06-22 22:40:13,127 - distributed.worker - INFO -          dashboard at:        10.33.227.169:39023
2023-06-22 22:40:13,127 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,127 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,127 - distributed.worker - INFO -               Threads:                          1
2023-06-22 22:40:13,127 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 22:40:13,127 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3kyxzaje
2023-06-22 22:40:13,128 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27c2961d-258a-4441-9731-e72c334bdbda
2023-06-22 22:40:13,127 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:39251
2023-06-22 22:40:13,128 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:39251
2023-06-22 22:40:13,128 - distributed.worker - INFO -          dashboard at:        10.33.227.169:41285
2023-06-22 22:40:13,128 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,128 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,128 - distributed.worker - INFO -               Threads:                          1
2023-06-22 22:40:13,128 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 22:40:13,128 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aj2lvy76
2023-06-22 22:40:13,128 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a93ab3e9-fb92-4d51-8d24-a47a9a3edb7c
2023-06-22 22:40:13,128 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:41231
2023-06-22 22:40:13,129 - distributed.worker - INFO - Starting Worker plugin PreImport-8f24450c-937a-4707-b894-5bc45fa340be
2023-06-22 22:40:13,129 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:41231
2023-06-22 22:40:13,129 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce36452d-7313-4f22-96bb-468e77d362b0
2023-06-22 22:40:13,129 - distributed.worker - INFO -          dashboard at:        10.33.227.169:39339
2023-06-22 22:40:13,129 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,129 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,129 - distributed.worker - INFO -               Threads:                          1
2023-06-22 22:40:13,129 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 22:40:13,129 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e6l4vprm
2023-06-22 22:40:13,129 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:34817
2023-06-22 22:40:13,129 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:34817
2023-06-22 22:40:13,129 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44541
2023-06-22 22:40:13,129 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,129 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,129 - distributed.worker - INFO - Starting Worker plugin PreImport-97f15643-eaf7-4308-8748-70667fd4132a
2023-06-22 22:40:13,129 - distributed.worker - INFO -               Threads:                          1
2023-06-22 22:40:13,129 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22986771-11b1-440c-a1ef-0e92cb84a5aa
2023-06-22 22:40:13,130 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 22:40:13,130 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7bd02ae3-76e3-4f55-a592-bf9adcc050f2
2023-06-22 22:40:13,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-we5djf1p
2023-06-22 22:40:13,131 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c25ae7dd-f7a8-4d8f-8fef-4edf33485484
2023-06-22 22:40:13,231 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,264 - distributed.worker - INFO - Starting Worker plugin PreImport-db1f66fb-db2b-4f88-8c14-bdae0f805ea0
2023-06-22 22:40:13,264 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e807012-4750-471a-bebd-ef0ff09f2710
2023-06-22 22:40:13,265 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,265 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,266 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,327 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-427a44d2-98ed-4376-acb4-7053af1eb936
2023-06-22 22:40:13,327 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,327 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e18b000-11d2-4952-8cff-8b65a4ccd0ca
2023-06-22 22:40:13,327 - distributed.worker - INFO - Starting Worker plugin PreImport-9130177b-0222-442d-9428-273a8a179dd5
2023-06-22 22:40:13,327 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,327 - distributed.worker - INFO - Starting Worker plugin PreImport-7e4510cc-f740-4779-8af3-89b858a61d20
2023-06-22 22:40:13,328 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,328 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,513 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,513 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,516 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 22:40:13,523 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,523 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,524 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,524 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,524 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 22:40:13,525 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,525 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,525 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 22:40:13,526 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,526 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 22:40:13,527 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,527 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,528 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,528 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,528 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 22:40:13,529 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 22:40:13,529 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 22:40:13,530 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 22:40:13,530 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 22:40:13,532 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 22:40:13,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 22:40:13,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 22:40:13,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 22:40:13,621 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 22:40:13,621 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 22:40:13,621 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 22:40:13,621 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 22:40:13,621 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 22:40:13,693 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 22:40:13,693 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 22:40:13,693 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 22:40:13,694 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 22:40:13,694 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 22:40:13,694 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 22:40:13,694 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 22:40:13,694 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 22:40:24,590 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 22:40:24,599 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 22:40:24,612 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 22:40:24,628 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 22:40:24,698 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 22:40:24,755 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 22:40:24,827 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 22:40:24,880 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 22:40:30,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:40:30,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:40:30,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:40:30,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:40:31,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:40:31,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:40:31,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:40:31,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:41:04,622 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:04,623 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:04,627 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:04,627 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:04,628 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:04,628 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:04,628 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:04,628 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,094 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,095 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-f1aa1c11-27b4-418a-b6ab-f5e246245df4
Function:  execute_task
args:      ((<function apply at 0x7f5456a02cb0>, <function _call_plc_uniform_neighbor_sample at 0x7f4f9872e7a0>, [b'\x1af\xc7P>FA\x9c\x81\x86o\xaak\xad\xe4E', <pylibcugraph.graphs.MGGraph object at 0x7f4de81532b0>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', -2298781059982575324], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small')"

2023-06-22 22:41:05,095 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,095 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,095 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,095 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,095 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,096 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-5347e8a3-7199-4e4e-8771-93c3a4cd5847
Function:  execute_task
args:      ((<function apply at 0x7f6927ebacb0>, <function _call_plc_uniform_neighbor_sample at 0x7f646c412680>, [b'\x1af\xc7P>FA\x9c\x81\x86o\xaak\xad\xe4E', <pylibcugraph.graphs.MGGraph object at 0x7f62a7266890>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', -176079554367522710], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small')"

2023-06-22 22:41:05,097 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,098 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 22:41:05,265 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-c42d13e5-45c8-420a-93ba-9af2fce4cd97
Function:  execute_task
args:      ((<function apply at 0x7fc8d4056cb0>, <function _call_plc_uniform_neighbor_sample at 0x7fc41862a950>, [b'\x1af\xc7P>FA\x9c\x81\x86o\xaak\xad\xe4E', <pylibcugraph.graphs.MGGraph object at 0x7fc24ed10e70>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', 1420373675458518080], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small')"

2023-06-22 22:41:05,266 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-3f1e0d95-7b6e-492f-88d1-b677c056aa16
Function:  execute_task
args:      ((<function apply at 0x7f7d04e42cb0>, <function _call_plc_uniform_neighbor_sample at 0x7f7880110ca0>, [b'\x1af\xc7P>FA\x9c\x81\x86o\xaak\xad\xe4E', <pylibcugraph.graphs.MGGraph object at 0x7f768da36d50>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', -1236413095387823831], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small')"

2023-06-22 22:41:05,268 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-1bad606d-6a10-41f4-b262-90957ca1982f
Function:  execute_task
args:      ((<function apply at 0x7ff34ba7ecb0>, <function _call_plc_uniform_neighbor_sample at 0x7fee6e726710>, [b'\x1af\xc7P>FA\x9c\x81\x86o\xaak\xad\xe4E', <pylibcugraph.graphs.MGGraph object at 0x7fece4282850>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', -4640781603379597072], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small')"

2023-06-22 22:41:05,269 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-03d1e8dd-4cd9-4029-a828-3327705b2c5a
Function:  execute_task
args:      ((<function apply at 0x7fc462632cb0>, <function _call_plc_uniform_neighbor_sample at 0x7fbfbc443010>, [b'\x1af\xc7P>FA\x9c\x81\x86o\xaak\xad\xe4E', <pylibcugraph.graphs.MGGraph object at 0x7fbde31209b0>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', 4016463125833753362], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small')"

2023-06-22 22:41:05,271 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-414070b8-4ed4-4503-b8bb-e827c4656267
Function:  execute_task
args:      ((<function apply at 0x7f7c0acf2cb0>, <function _call_plc_uniform_neighbor_sample at 0x7f7758212950>, [b'\x1af\xc7P>FA\x9c\x81\x86o\xaak\xad\xe4E', <pylibcugraph.graphs.MGGraph object at 0x7f759c040a90>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', -7341088115413029842], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small')"

2023-06-22 22:41:05,274 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-3ef001d4-6963-4d84-8605-fd5b10ab4340
Function:  execute_task
args:      ((<function apply at 0x7f7e53792cb0>, <function _call_plc_uniform_neighbor_sample at 0x7f799abe29e0>, [b'\x1af\xc7P>FA\x9c\x81\x86o\xaak\xad\xe4E', <pylibcugraph.graphs.MGGraph object at 0x7f77ca572070>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', 2613709341580929077], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small')"

2023-06-22 22:46:46,046 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:41231. Reason: worker-handle-scheduler-connection-broken
2023-06-22 22:46:46,047 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:34817. Reason: worker-handle-scheduler-connection-broken
2023-06-22 22:46:46,047 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:39251. Reason: worker-handle-scheduler-connection-broken
2023-06-22 22:46:46,047 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:44625. Reason: worker-handle-scheduler-connection-broken
2023-06-22 22:46:46,047 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:43695. Reason: worker-handle-scheduler-connection-broken
2023-06-22 22:46:46,047 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:36997. Reason: worker-handle-scheduler-connection-broken
2023-06-22 22:46:46,047 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:42151. Reason: worker-handle-scheduler-connection-broken
2023-06-22 22:46:46,047 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:34219. Reason: worker-handle-scheduler-connection-broken
2023-06-22 22:46:46,051 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:46073'. Reason: nanny-close
2023-06-22 22:46:46,052 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 22:46:46,053 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:44075'. Reason: nanny-close
2023-06-22 22:46:46,054 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 22:46:46,055 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:37415'. Reason: nanny-close
2023-06-22 22:46:46,055 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 22:46:46,055 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:45759'. Reason: nanny-close
2023-06-22 22:46:46,055 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 22:46:46,056 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:45279'. Reason: nanny-close
2023-06-22 22:46:46,056 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 22:46:46,056 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:46383'. Reason: nanny-close
2023-06-22 22:46:46,057 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 22:46:46,057 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:39957'. Reason: nanny-close
2023-06-22 22:46:46,057 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 22:46:46,058 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:34139'. Reason: nanny-close
2023-06-22 22:46:46,058 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 22:46:46,059 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f8df83e3b50>, <Task finished name='Task-167053' coro=<BaseTCPListener._handle_stream() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py:598> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/tcpserver.py", line 387, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 604, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 640, in contact_address
    host, port = self.get_host_port()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 621, in get_host_port
    self._check_started()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 596, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
2023-06-22 22:46:46,063 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f8df82d1240>, <Task finished name='Task-167054' coro=<BaseTCPListener._handle_stream() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py:598> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/tcpserver.py", line 387, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 604, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 640, in contact_address
    host, port = self.get_host_port()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 621, in get_host_port
    self._check_started()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 596, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
2023-06-22 22:46:46,064 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f8df82d1360>, <Task finished name='Task-167055' coro=<BaseTCPListener._handle_stream() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py:598> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/tcpserver.py", line 387, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 604, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 640, in contact_address
    host, port = self.get_host_port()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 621, in get_host_port
    self._check_started()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 596, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
2023-06-22 22:46:46,064 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f8df82d1e10>, <Task finished name='Task-167056' coro=<BaseTCPListener._handle_stream() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py:598> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/tcpserver.py", line 387, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 604, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 640, in contact_address
    host, port = self.get_host_port()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 621, in get_host_port
    self._check_started()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 596, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
2023-06-22 22:46:46,064 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f8df82d1fc0>, <Task finished name='Task-167057' coro=<BaseTCPListener._handle_stream() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py:598> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/tcpserver.py", line 387, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 604, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 640, in contact_address
    host, port = self.get_host_port()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 621, in get_host_port
    self._check_started()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 596, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
2023-06-22 22:46:46,064 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f8df82d2050>, <Task finished name='Task-167058' coro=<BaseTCPListener._handle_stream() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py:598> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/tcpserver.py", line 387, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 604, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 640, in contact_address
    host, port = self.get_host_port()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 621, in get_host_port
    self._check_started()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 596, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
2023-06-22 22:46:46,065 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f8df82d0e50>, <Task finished name='Task-167059' coro=<BaseTCPListener._handle_stream() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py:598> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/tcpserver.py", line 387, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 604, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 640, in contact_address
    host, port = self.get_host_port()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 621, in get_host_port
    self._check_started()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 596, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
2023-06-22 22:46:46,065 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f8df82d11b0>, <Task finished name='Task-167060' coro=<BaseTCPListener._handle_stream() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py:598> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/tcpserver.py", line 387, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 604, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 640, in contact_address
    host, port = self.get_host_port()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 621, in get_host_port
    self._check_started()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 596, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
2023-06-22 22:46:49,259 - distributed.nanny - WARNING - Worker process still alive after 3.1999847412109377 seconds, killing
2023-06-22 22:46:49,259 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 22:46:49,260 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 22:46:49,261 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 22:46:49,263 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 22:46:49,264 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 22:46:49,264 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 22:46:49,266 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 22:46:50,091 - distributed.nanny - INFO - Worker process 1478376 was killed by signal 9
2023-06-22 22:46:50,092 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 22:46:50,094 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 22:46:50,094 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 22:46:50,094 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 22:46:50,094 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 22:46:50,095 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 22:46:50,095 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 22:46:50,095 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 22:46:50,097 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1478400 parent=1478350 started daemon>
2023-06-22 22:46:50,097 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1478397 parent=1478350 started daemon>
2023-06-22 22:46:50,097 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1478394 parent=1478350 started daemon>
2023-06-22 22:46:50,098 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1478389 parent=1478350 started daemon>
2023-06-22 22:46:50,098 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1478385 parent=1478350 started daemon>
2023-06-22 22:46:50,098 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1478382 parent=1478350 started daemon>
2023-06-22 22:46:50,098 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1478379 parent=1478350 started daemon>
2023-06-22 22:46:50,675 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1478385 exit status was already read will report exitcode 255
2023-06-22 22:46:51,102 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1478389 exit status was already read will report exitcode 255
