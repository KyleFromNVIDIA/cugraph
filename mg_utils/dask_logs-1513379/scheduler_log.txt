RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 23:24:40,788 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 23:24:41,768 - distributed.scheduler - INFO - State start
2023-06-22 23:24:41,770 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-setc3eme', purging
2023-06-22 23:24:41,770 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qjodgtx8', purging
2023-06-22 23:24:41,770 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7x0d54x2', purging
2023-06-22 23:24:41,771 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-je82477p', purging
2023-06-22 23:24:41,771 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ann9ibnn', purging
2023-06-22 23:24:41,771 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pl6p8isu', purging
2023-06-22 23:24:41,771 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5d6ju5kb', purging
2023-06-22 23:24:41,783 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 23:24:41,784 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 23:24:41,784 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 23:24:52,347 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:33361', status: init, memory: 0, processing: 0>
2023-06-22 23:24:52,610 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:33361
2023-06-22 23:24:52,611 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33518
2023-06-22 23:24:52,612 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:45977', status: init, memory: 0, processing: 0>
2023-06-22 23:24:52,612 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:45977
2023-06-22 23:24:52,612 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33526
2023-06-22 23:24:52,621 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:44757', status: init, memory: 0, processing: 0>
2023-06-22 23:24:52,621 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:44757
2023-06-22 23:24:52,621 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33530
2023-06-22 23:24:52,622 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:40097', status: init, memory: 0, processing: 0>
2023-06-22 23:24:52,622 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:40097
2023-06-22 23:24:52,622 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33536
2023-06-22 23:24:52,623 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:41103', status: init, memory: 0, processing: 0>
2023-06-22 23:24:52,623 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:41103
2023-06-22 23:24:52,623 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33540
2023-06-22 23:24:52,624 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:35579', status: init, memory: 0, processing: 0>
2023-06-22 23:24:52,624 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:35579
2023-06-22 23:24:52,624 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33576
2023-06-22 23:24:52,625 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:38735', status: init, memory: 0, processing: 0>
2023-06-22 23:24:52,625 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:38735
2023-06-22 23:24:52,625 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33564
2023-06-22 23:24:52,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:36011', status: init, memory: 0, processing: 0>
2023-06-22 23:24:52,626 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:36011
2023-06-22 23:24:52,626 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33552
2023-06-22 23:24:58,028 - distributed.scheduler - INFO - Receive client connection: Client-009bd992-1154-11ee-9857-d8c49778ced7
2023-06-22 23:24:58,029 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:33656
2023-06-22 23:24:58,116 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 23:25:49,802 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:25:51,969 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 23:25:55,649 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:26:02,239 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-22 23:26:02,241 - distributed.scheduler - INFO - Remove client Client-009bd992-1154-11ee-9857-d8c49778ced7
2023-06-22 23:26:02,247 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:33656; closing.
2023-06-22 23:26:02,248 - distributed.scheduler - INFO - Remove client Client-009bd992-1154-11ee-9857-d8c49778ced7
2023-06-22 23:26:02,249 - distributed.scheduler - INFO - Close client connection: Client-009bd992-1154-11ee-9857-d8c49778ced7
2023-06-22 23:26:14,622 - tornado.application - ERROR - Uncaught exception GET /tasks/ws (10.20.237.237)
HTTPServerRequest(protocol='http', host='10.33.227.169:8787', method='GET', uri='/tasks/ws', version='HTTP/1.1', remote_ip='10.20.237.237')
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/websocket.py", line 937, in _accept_connection
    open_result = handler.open(*handler.open_args, **handler.open_kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/web.py", line 3290, in wrapper
    return method(self, *args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/bokeh/server/views/ws.py", line 149, in open
    raise ProtocolError("Token is expired.")
bokeh.protocol.exceptions.ProtocolError: Token is expired.
2023-06-22 23:27:52,713 - distributed.scheduler - ERROR - broadcast to tcp://10.33.227.169:36247 failed: OSError: Timed out trying to connect to tcp://10.33.227.169:36247 after 100 s
2023-06-22 23:27:52,713 - tornado.application - ERROR - Uncaught exception GET /info/logs/tcp%3A%2F%2F10.33.227.169%3A36247.html (10.20.237.237)
HTTPServerRequest(protocol='http', host='10.33.227.169:8787', method='GET', uri='/info/logs/tcp%3A%2F%2F10.33.227.169%3A36247.html', version='HTTP/1.1', remote_ip='10.20.237.237')
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 336, in connect
    comm = await wait_for(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 504, in connect
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7ff39a87e380>: ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/web.py", line 1786, in _execute
    result = await result
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/http/scheduler/info.py", line 127, in get
    logs = await self.server.get_worker_logs(workers=[worker])
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 7878, in get_worker_logs
    results = await self.broadcast(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 6124, in broadcast
    results = await All(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 249, in All
    result = await tasks.next()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 6099, in send_message
    comm = await self.rpc.connect(addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 362, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.33.227.169:36247 after 100 s
2023-06-22 23:28:37,842 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 23:28:37,843 - distributed.core - INFO - Connection to tcp://10.33.227.169:33540 has been closed.
2023-06-22 23:28:37,843 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:41103', status: running, memory: 0, processing: 0>
2023-06-22 23:28:37,845 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:41103
2023-06-22 23:28:37,847 - distributed.core - INFO - Connection to tcp://10.33.227.169:33518 has been closed.
2023-06-22 23:28:37,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:33361', status: running, memory: 0, processing: 0>
2023-06-22 23:28:37,848 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:33361
2023-06-22 23:28:37,848 - distributed.core - INFO - Connection to tcp://10.33.227.169:33526 has been closed.
2023-06-22 23:28:37,848 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:45977', status: running, memory: 0, processing: 0>
2023-06-22 23:28:37,848 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:45977
2023-06-22 23:28:37,848 - distributed.core - INFO - Connection to tcp://10.33.227.169:33530 has been closed.
2023-06-22 23:28:37,848 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:44757', status: running, memory: 0, processing: 0>
2023-06-22 23:28:37,849 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:44757
2023-06-22 23:28:37,849 - distributed.core - INFO - Connection to tcp://10.33.227.169:33552 has been closed.
2023-06-22 23:28:37,849 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:36011', status: running, memory: 0, processing: 0>
2023-06-22 23:28:37,849 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:36011
2023-06-22 23:28:37,850 - distributed.core - INFO - Connection to tcp://10.33.227.169:33536 has been closed.
2023-06-22 23:28:37,850 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:40097', status: running, memory: 0, processing: 0>
2023-06-22 23:28:37,850 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:40097
2023-06-22 23:28:37,851 - distributed.core - INFO - Connection to tcp://10.33.227.169:33576 has been closed.
2023-06-22 23:28:37,851 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:35579', status: running, memory: 0, processing: 0>
2023-06-22 23:28:37,851 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:35579
2023-06-22 23:28:37,851 - distributed.core - INFO - Connection to tcp://10.33.227.169:33564 has been closed.
2023-06-22 23:28:37,851 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:38735', status: running, memory: 0, processing: 0>
2023-06-22 23:28:37,851 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:38735
2023-06-22 23:28:37,852 - distributed.scheduler - INFO - Lost all workers
2023-06-22 23:28:37,852 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 23:28:37,852 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33518>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 23:28:37,853 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33576>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33576>: Stream is closed
2023-06-22 23:28:37,853 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33552>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 23:28:37,853 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33564>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33564>: Stream is closed
2023-06-22 23:28:37,854 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33536>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33536>: Stream is closed
2023-06-22 23:28:37,854 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33530>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 23:28:37,854 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:33526>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 23:28:37,855 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 23:28:37,857 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 23:28:37,858 - distributed.scheduler - INFO - End scheduler
