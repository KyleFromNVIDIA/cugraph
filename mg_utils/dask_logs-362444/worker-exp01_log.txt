RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 18:20:58,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40081'
2023-06-26 18:20:58,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41623'
2023-06-26 18:20:58,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43261'
2023-06-26 18:20:58,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34415'
2023-06-26 18:20:58,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42383'
2023-06-26 18:20:58,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40059'
2023-06-26 18:20:58,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35933'
2023-06-26 18:20:58,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45321'
2023-06-26 18:20:58,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39409'
2023-06-26 18:20:58,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44141'
2023-06-26 18:20:58,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45085'
2023-06-26 18:20:58,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45451'
2023-06-26 18:20:58,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33287'
2023-06-26 18:20:58,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39983'
2023-06-26 18:20:58,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39573'
2023-06-26 18:20:58,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40281'
2023-06-26 18:21:00,133 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,240 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,240 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,281 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,297 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,297 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:21:00,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:21:00,358 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,429 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,460 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,462 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,475 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,487 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,508 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,512 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,515 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,520 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:00,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:21:06,849 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40703
2023-06-26 18:21:06,850 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40703
2023-06-26 18:21:06,850 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33633
2023-06-26 18:21:06,850 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:06,850 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:06,850 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:06,850 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:06,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m15wgkqp
2023-06-26 18:21:06,851 - distributed.worker - INFO - Starting Worker plugin PreImport-90bdf0aa-d266-4f48-9378-206666d55a9c
2023-06-26 18:21:06,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33ddf376-d065-4886-b899-8ebfb190adf4
2023-06-26 18:21:06,853 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37863
2023-06-26 18:21:06,853 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37863
2023-06-26 18:21:06,853 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37941
2023-06-26 18:21:06,853 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:06,853 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:06,853 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:06,853 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:06,853 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-821p2tab
2023-06-26 18:21:06,854 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2eb3ae10-f0f2-4200-bfbd-2305a9097a2d
2023-06-26 18:21:07,148 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35649
2023-06-26 18:21:07,148 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35649
2023-06-26 18:21:07,148 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33341
2023-06-26 18:21:07,148 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,148 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,148 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,148 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,148 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ng6aavkt
2023-06-26 18:21:07,149 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4515bba4-a5a6-4b96-8d87-df6dc447d74c
2023-06-26 18:21:07,200 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38121
2023-06-26 18:21:07,201 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38121
2023-06-26 18:21:07,201 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34077
2023-06-26 18:21:07,201 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,201 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,201 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,201 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,201 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mb5r52s2
2023-06-26 18:21:07,201 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5dfb3088-39f7-4044-867d-e95159584ef9
2023-06-26 18:21:07,247 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39643
2023-06-26 18:21:07,247 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39643
2023-06-26 18:21:07,247 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41061
2023-06-26 18:21:07,247 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,247 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,248 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6n6x3xrn
2023-06-26 18:21:07,248 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36599
2023-06-26 18:21:07,248 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36599
2023-06-26 18:21:07,248 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37995
2023-06-26 18:21:07,248 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c54b4e66-fd28-4570-a0b2-25e30e66d9b4
2023-06-26 18:21:07,248 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,248 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,248 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h4wh4j51
2023-06-26 18:21:07,249 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14e68390-be65-4f0d-b768-582d7ec1029d
2023-06-26 18:21:07,271 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36055
2023-06-26 18:21:07,272 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36055
2023-06-26 18:21:07,272 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39469
2023-06-26 18:21:07,272 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,272 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,272 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,272 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-woixd6od
2023-06-26 18:21:07,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb31b8af-79fe-40b2-851c-535479f90c66
2023-06-26 18:21:07,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8bf5cef8-1cea-489f-af7a-89cded816597
2023-06-26 18:21:07,298 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39491
2023-06-26 18:21:07,298 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39491
2023-06-26 18:21:07,299 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40981
2023-06-26 18:21:07,299 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,299 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,299 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,299 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_6wotbvi
2023-06-26 18:21:07,299 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fb3c788-1443-48e5-a2b4-5b4c9449b832
2023-06-26 18:21:07,324 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37253
2023-06-26 18:21:07,325 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37253
2023-06-26 18:21:07,325 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40855
2023-06-26 18:21:07,325 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,325 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,325 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,325 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,325 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m9_q_e_c
2023-06-26 18:21:07,327 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00edae6c-5f5a-43d6-b4ce-9ce97194b348
2023-06-26 18:21:07,327 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14a9f828-9a67-42aa-9975-5e7ea7105845
2023-06-26 18:21:07,356 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41667
2023-06-26 18:21:07,356 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41667
2023-06-26 18:21:07,356 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42957
2023-06-26 18:21:07,356 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,356 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,356 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,356 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,356 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1fad58d8
2023-06-26 18:21:07,357 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44f63496-7e05-4b1c-afc9-d5a65ab4b4d1
2023-06-26 18:21:07,384 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44383
2023-06-26 18:21:07,384 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44383
2023-06-26 18:21:07,384 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39255
2023-06-26 18:21:07,384 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,384 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,384 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,384 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,384 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6s343yyy
2023-06-26 18:21:07,385 - distributed.worker - INFO - Starting Worker plugin PreImport-f9a1b288-c9a5-4d14-bf23-4d2135632c46
2023-06-26 18:21:07,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-496bcbaa-3b7f-4f02-bb4c-97fea950c732
2023-06-26 18:21:07,409 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42905
2023-06-26 18:21:07,409 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42905
2023-06-26 18:21:07,409 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42371
2023-06-26 18:21:07,409 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,409 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,409 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,410 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,410 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cruvpvs9
2023-06-26 18:21:07,411 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7066ba45-a1ba-4efe-a7c4-cf241fc338ee
2023-06-26 18:21:07,510 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41571
2023-06-26 18:21:07,510 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41571
2023-06-26 18:21:07,510 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42343
2023-06-26 18:21:07,510 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,510 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,510 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,510 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,510 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ertohlgd
2023-06-26 18:21:07,511 - distributed.worker - INFO - Starting Worker plugin PreImport-e1573875-4e00-4025-a917-bb05ecf95f0a
2023-06-26 18:21:07,511 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fa8328a-db9d-4adb-a11c-192d96284a84
2023-06-26 18:21:07,525 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37449
2023-06-26 18:21:07,526 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37449
2023-06-26 18:21:07,526 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41497
2023-06-26 18:21:07,526 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,526 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,526 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,526 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-50prtcqf
2023-06-26 18:21:07,528 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef9a1340-5373-486b-9c7a-daa3ec9514f6
2023-06-26 18:21:07,528 - distributed.worker - INFO - Starting Worker plugin RMMSetup-41fa3c27-0298-43ee-9de9-bf6157a2dd0e
2023-06-26 18:21:07,546 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39493
2023-06-26 18:21:07,546 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39493
2023-06-26 18:21:07,546 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34121
2023-06-26 18:21:07,546 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,546 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,546 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,546 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dezt6goj
2023-06-26 18:21:07,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2804043-8c17-42a3-b0eb-6259e22965bc
2023-06-26 18:21:07,587 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45357
2023-06-26 18:21:07,587 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45357
2023-06-26 18:21:07,587 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39551
2023-06-26 18:21:07,587 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:21:07,587 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:07,587 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:21:07,587 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:21:07,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c6607soy
2023-06-26 18:21:07,588 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74cc1873-82e1-4ad3-a81e-7e43163be32a
2023-06-26 18:21:10,684 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e829c56-dadb-4ee6-8958-801f350df088
2023-06-26 18:21:10,684 - distributed.worker - INFO - Starting Worker plugin PreImport-7e92b778-ba79-4003-b5f0-d803d95e28b8
2023-06-26 18:21:10,685 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:10,701 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:10,701 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:10,703 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:10,938 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc4df872-ddc3-4ca2-9322-5ff6a5d9d673
2023-06-26 18:21:10,940 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:10,967 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:10,967 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:10,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee36e42d-f335-4426-b086-bb2255fe3679
2023-06-26 18:21:10,968 - distributed.worker - INFO - Starting Worker plugin PreImport-b48ede1d-b3ee-4631-ab43-55b4c50fa331
2023-06-26 18:21:10,968 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:10,969 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:10,985 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:10,985 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:10,987 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,025 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c71b71d-ccf3-4915-a401-df49a13e4b34
2023-06-26 18:21:11,026 - distributed.worker - INFO - Starting Worker plugin PreImport-8758ac32-4d08-4d5a-859e-517e741551ef
2023-06-26 18:21:11,026 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,034 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65c1df54-3c11-48fb-8134-c4f60f157f89
2023-06-26 18:21:11,034 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,045 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,045 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,047 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,050 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,050 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,052 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,057 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39f678f4-4626-4664-87b7-8dc15637097f
2023-06-26 18:21:11,057 - distributed.worker - INFO - Starting Worker plugin PreImport-cab654c0-75d5-426c-9617-4a8ca188af18
2023-06-26 18:21:11,059 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,073 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-568229b1-6565-4e91-a366-aa2d290b088c
2023-06-26 18:21:11,073 - distributed.worker - INFO - Starting Worker plugin PreImport-87f125df-fd45-4a33-96ba-c708ddb3caf9
2023-06-26 18:21:11,074 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,086 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,086 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,089 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,089 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,089 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,090 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,126 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56907877-951d-47af-bba3-e757064cfdf2
2023-06-26 18:21:11,127 - distributed.worker - INFO - Starting Worker plugin PreImport-696f7fd5-a925-4583-8c90-1a7a8eacb44c
2023-06-26 18:21:11,127 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,134 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80fad230-3b5b-4cc7-a5c7-67ab54fb3112
2023-06-26 18:21:11,134 - distributed.worker - INFO - Starting Worker plugin PreImport-9dbf3c24-f58c-480a-a5ca-d13a86da976f
2023-06-26 18:21:11,135 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,141 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,141 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,142 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,148 - distributed.worker - INFO - Starting Worker plugin PreImport-9a596040-65b0-4710-a1de-4ccbe6c4d78c
2023-06-26 18:21:11,149 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,153 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,153 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,155 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,165 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,165 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,168 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,208 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b74cc4fa-e346-4a49-8a48-e33ce404de60
2023-06-26 18:21:11,209 - distributed.worker - INFO - Starting Worker plugin PreImport-d55c192b-d89c-48af-bcd5-944111523160
2023-06-26 18:21:11,210 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,214 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56717e70-3e41-4546-8017-22eb38717e7c
2023-06-26 18:21:11,214 - distributed.worker - INFO - Starting Worker plugin PreImport-e9bd61da-881d-4933-ad07-a052474612e6
2023-06-26 18:21:11,214 - distributed.worker - INFO - Starting Worker plugin PreImport-264127d1-471f-41a8-9d29-47fe4a278ee4
2023-06-26 18:21:11,215 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,215 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,226 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,227 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,228 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,231 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,231 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,233 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,238 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,240 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b05a935-84f9-48d9-9ea8-d0bc9786d49d
2023-06-26 18:21:11,241 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,241 - distributed.worker - INFO - Starting Worker plugin PreImport-362e3473-7b30-43bb-86fd-59ce149d073d
2023-06-26 18:21:11,241 - distributed.worker - INFO - Starting Worker plugin PreImport-4b58e37d-6f61-4d23-a1f5-98f6b372f05a
2023-06-26 18:21:11,242 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,243 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,244 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3df051e-8847-47d2-8a84-01eb01ee9aa5
2023-06-26 18:21:11,246 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,264 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,264 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,265 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,265 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,266 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,267 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:11,267 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:21:11,267 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:21:11,270 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:21:20,638 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,638 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,638 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,639 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,639 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,639 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,639 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,639 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,639 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,639 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,641 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,641 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,643 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,643 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,645 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,645 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:21:20,654 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,654 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,654 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,654 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,654 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,654 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,655 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:20,657 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,340 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,341 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,341 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,341 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,341 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,341 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:21,341 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:21:24,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:36,846 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:36,872 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,258 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,278 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,304 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,317 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,377 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,378 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,390 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,393 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,444 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,481 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,489 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,494 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:37,873 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:21:44,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:21:44,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:22,606 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,606 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,606 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,609 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,611 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,611 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,611 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,611 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,611 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,612 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,612 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,612 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,612 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,612 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,612 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:22,612 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:22:25,805 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,812 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,813 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:25,814 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:22:33,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:22:33,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,279 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,280 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,280 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,280 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:23:02,293 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,293 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,293 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,293 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,293 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,293 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,293 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,293 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,294 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,294 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,294 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,294 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,294 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,294 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,294 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,294 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:23:02,305 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,305 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,305 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:02,306 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:23:06,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,732 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,735 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35649. Reason: scheduler-restart
2023-06-26 18:23:06,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,736 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36055. Reason: scheduler-restart
2023-06-26 18:23:06,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,736 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,736 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36599. Reason: scheduler-restart
2023-06-26 18:23:06,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,737 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,737 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37253. Reason: scheduler-restart
2023-06-26 18:23:06,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,737 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37449. Reason: scheduler-restart
2023-06-26 18:23:06,737 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,737 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37863. Reason: scheduler-restart
2023-06-26 18:23:06,737 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,738 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38121. Reason: scheduler-restart
2023-06-26 18:23:06,739 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39491. Reason: scheduler-restart
2023-06-26 18:23:06,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,739 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,739 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39493. Reason: scheduler-restart
2023-06-26 18:23:06,739 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,739 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,740 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,740 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39643. Reason: scheduler-restart
2023-06-26 18:23:06,740 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,740 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,740 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,740 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,740 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,740 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,741 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40703. Reason: scheduler-restart
2023-06-26 18:23:06,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,741 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,741 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,742 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,742 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,742 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:23:06,744 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41571. Reason: scheduler-restart
2023-06-26 18:23:06,744 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,745 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,746 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,748 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41667. Reason: scheduler-restart
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36055
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36599
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37253
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37449
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37863
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38121
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39493
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39491
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39643
2023-06-26 18:23:06,749 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,749 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,758 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45357. Reason: scheduler-restart
2023-06-26 18:23:06,758 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42905. Reason: scheduler-restart
2023-06-26 18:23:06,758 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36055
2023-06-26 18:23:06,758 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36599
2023-06-26 18:23:06,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37253
2023-06-26 18:23:06,759 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44383. Reason: scheduler-restart
2023-06-26 18:23:06,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37449
2023-06-26 18:23:06,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37863
2023-06-26 18:23:06,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38121
2023-06-26 18:23:06,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39493
2023-06-26 18:23:06,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39491
2023-06-26 18:23:06,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39643
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40703
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36055
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36055
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41571
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36599
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36599
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37253
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37449
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37253
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37863
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37449
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38121
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37863
2023-06-26 18:23:06,760 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39493
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38121
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39493
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39491
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39491
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39643
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39643
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36055
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36599
2023-06-26 18:23:06,760 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37253
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37449
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37863
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40703
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38121
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41571
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40703
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39493
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39491
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41571
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39643
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41667
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41667
2023-06-26 18:23:06,761 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,762 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,762 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40703
2023-06-26 18:23:06,762 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41571
2023-06-26 18:23:06,763 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41667
2023-06-26 18:23:06,763 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f5d2cb12740, tag: 0x2206a9aa4b510ed7>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f5d2cb12740, tag: 0x2206a9aa4b510ed7>: 
2023-06-26 18:23:06,763 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,769 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,782 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:23:06,788 - distributed.nanny - INFO - Worker closed
2023-06-26 18:23:06,791 - distributed.nanny - INFO - Worker closed
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-15281' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
2023-06-26 18:23:09,662 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:10,411 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:10,412 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:10,711 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:10,994 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:11,624 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:11,627 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:11,628 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,067 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:13,067 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:13,087 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,088 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,090 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,092 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,095 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:13,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:13,098 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,099 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:13,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:13,101 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:13,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:13,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:13,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:13,107 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,108 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:23:13,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:13,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:13,143 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:13,143 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:13,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:13,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:13,275 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:13,276 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:13,277 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:13,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:13,283 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:13,324 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:13,325 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:13,350 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:14,906 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:14,906 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:14,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:14,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:14,978 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:14,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:14,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:14,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:14,997 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:14,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:15,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:15,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:15,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:15,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:15,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:23:15,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:23:15,081 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:15,088 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:15,155 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:15,155 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:15,172 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:15,206 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:15,213 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:15,213 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:23:16,730 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39829
2023-06-26 18:23:16,730 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39829
2023-06-26 18:23:16,731 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39467
2023-06-26 18:23:16,731 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:16,731 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:16,731 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:16,731 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:16,731 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lx1x9d5_
2023-06-26 18:23:16,731 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8b20e93-fccc-4ef7-84a8-37d951b8d2cc
2023-06-26 18:23:16,751 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32845
2023-06-26 18:23:16,751 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32845
2023-06-26 18:23:16,751 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41659
2023-06-26 18:23:16,751 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:16,751 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:16,751 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:16,751 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:16,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vfythfnt
2023-06-26 18:23:16,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fdba5ca-daaf-4f64-aebe-fe97bd6d2537
2023-06-26 18:23:16,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c59c9b6c-c967-4090-9fa5-0d0efb9c932a
2023-06-26 18:23:16,964 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44911
2023-06-26 18:23:16,964 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44911
2023-06-26 18:23:16,964 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36535
2023-06-26 18:23:16,964 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:16,964 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:16,964 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:16,964 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:16,964 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1x1yd_q2
2023-06-26 18:23:16,965 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30850b91-f4da-46ac-b6ca-0043651abe98
2023-06-26 18:23:16,998 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35877
2023-06-26 18:23:16,998 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35877
2023-06-26 18:23:16,998 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37145
2023-06-26 18:23:16,998 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:16,998 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:16,998 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:16,998 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:16,998 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i9249tsk
2023-06-26 18:23:16,999 - distributed.worker - INFO - Starting Worker plugin RMMSetup-865be1ce-6dcf-4ed7-80f1-5ef62d973b3a
2023-06-26 18:23:17,000 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37869
2023-06-26 18:23:17,001 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37869
2023-06-26 18:23:17,001 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41743
2023-06-26 18:23:17,001 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:17,001 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:17,001 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:17,002 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:17,002 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uldu0cbz
2023-06-26 18:23:17,003 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d266756f-e2e6-46fb-8484-fd281317bd07
2023-06-26 18:23:17,025 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34589
2023-06-26 18:23:17,025 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34589
2023-06-26 18:23:17,025 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36315
2023-06-26 18:23:17,026 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:17,026 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:17,026 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:17,026 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:17,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-83bb93ef
2023-06-26 18:23:17,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-255d8754-5357-494c-b42c-d58f0daa4fd9
2023-06-26 18:23:17,122 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36181
2023-06-26 18:23:17,122 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36181
2023-06-26 18:23:17,122 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40045
2023-06-26 18:23:17,122 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:17,122 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:17,122 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:17,122 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:17,122 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ufu1q3us
2023-06-26 18:23:17,122 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35833
2023-06-26 18:23:17,123 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35833
2023-06-26 18:23:17,123 - distributed.worker - INFO - Starting Worker plugin PreImport-51e38e85-c6f9-4ac9-834f-fb06c547ed82
2023-06-26 18:23:17,123 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43909
2023-06-26 18:23:17,123 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8c6cc23-dd6c-4480-bc5d-4e09520fa6cf
2023-06-26 18:23:17,123 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:17,123 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:17,123 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:17,123 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:17,123 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qytq8978
2023-06-26 18:23:17,124 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6a6c060-7075-435b-b19c-04dd82864e12
2023-06-26 18:23:20,685 - distributed.worker - INFO - Starting Worker plugin PreImport-94edd244-4829-4125-9022-780c991a2888
2023-06-26 18:23:20,689 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,710 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:20,711 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,713 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:20,763 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-155068cb-f55c-4120-8dca-e45d991476c8
2023-06-26 18:23:20,764 - distributed.worker - INFO - Starting Worker plugin PreImport-eaa1763a-d6c8-46ac-b404-60ff34c5a1bd
2023-06-26 18:23:20,765 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,776 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:20,776 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,777 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:20,794 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98eaf3e4-6e8a-4f72-a4ab-6a3e0d126f8f
2023-06-26 18:23:20,795 - distributed.worker - INFO - Starting Worker plugin PreImport-f95ec8f5-ddf5-44fa-9aa8-7d61a41c2d5f
2023-06-26 18:23:20,796 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,817 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:20,817 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,819 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:20,847 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b67bb80f-4b53-4339-8ba6-5cd770f7584a
2023-06-26 18:23:20,847 - distributed.worker - INFO - Starting Worker plugin PreImport-4ab731e4-838e-4e2c-aaa5-dc219bf364c9
2023-06-26 18:23:20,848 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,859 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:20,859 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,860 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:20,877 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea5e4d73-fdd6-4b6d-b80c-8fb60ec743e1
2023-06-26 18:23:20,878 - distributed.worker - INFO - Starting Worker plugin PreImport-f8a6971d-e8d8-479f-b0ec-8467577a01b3
2023-06-26 18:23:20,880 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,885 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8c2aa3c-c5af-4827-b2e5-716e2a4d4daf
2023-06-26 18:23:20,886 - distributed.worker - INFO - Starting Worker plugin PreImport-9e1a96ec-9a5e-47a1-9368-721e602b584e
2023-06-26 18:23:20,889 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,895 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:20,896 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,898 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:20,901 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de6e8e4f-711d-4379-8d2d-ddb896a5d6f5
2023-06-26 18:23:20,902 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,906 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:20,906 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,907 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45cb290c-e7e9-4acd-86c1-65abbbbf00cb
2023-06-26 18:23:20,908 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:20,909 - distributed.worker - INFO - Starting Worker plugin PreImport-e8405328-bca5-43f2-8199-2bfd96de0344
2023-06-26 18:23:20,911 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:20,911 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,911 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,912 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:20,925 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:20,925 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:20,927 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:21,583 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35023
2023-06-26 18:23:21,583 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35023
2023-06-26 18:23:21,583 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37675
2023-06-26 18:23:21,583 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:21,583 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:21,583 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:21,583 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:21,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fuut69cx
2023-06-26 18:23:21,584 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd2c9cb9-b33f-4c21-a4d2-fc8d75cf271b
2023-06-26 18:23:21,611 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44629
2023-06-26 18:23:21,612 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44629
2023-06-26 18:23:21,612 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34439
2023-06-26 18:23:21,612 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:21,612 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:21,612 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:21,612 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:21,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-efh56f7u
2023-06-26 18:23:21,612 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-71fcc7dc-c205-4168-82e9-5d0e83e7d91f
2023-06-26 18:23:21,612 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60f3a770-ee0e-403a-baa5-d6176282c55e
2023-06-26 18:23:21,935 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46225
2023-06-26 18:23:21,936 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46225
2023-06-26 18:23:21,936 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42565
2023-06-26 18:23:21,936 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:21,936 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:21,936 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:21,936 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:21,936 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8ijb96ys
2023-06-26 18:23:21,937 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f7cdc74-ffa9-40e9-a63c-419baa75e216
2023-06-26 18:23:21,947 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41891
2023-06-26 18:23:21,948 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41891
2023-06-26 18:23:21,948 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40775
2023-06-26 18:23:21,948 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:21,948 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:21,948 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:21,948 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:21,948 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-khou4nwe
2023-06-26 18:23:21,948 - distributed.worker - INFO - Starting Worker plugin PreImport-666caaa5-055a-4c8e-98e9-ad74de48a5c8
2023-06-26 18:23:21,949 - distributed.worker - INFO - Starting Worker plugin RMMSetup-287dfae6-db51-414d-b677-3fcf2b0205dd
2023-06-26 18:23:22,254 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42843
2023-06-26 18:23:22,254 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42843
2023-06-26 18:23:22,254 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44593
2023-06-26 18:23:22,254 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:22,254 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:22,254 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:22,254 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:22,254 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3ow8sovq
2023-06-26 18:23:22,255 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec93e426-f055-41c0-9fcb-281b60319b01
2023-06-26 18:23:22,259 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37885
2023-06-26 18:23:22,259 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37885
2023-06-26 18:23:22,259 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40533
2023-06-26 18:23:22,260 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:22,260 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:22,260 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:22,260 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:22,260 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0dy0iyzg
2023-06-26 18:23:22,260 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bfc55046-59d5-4f35-94bd-689c92b3b232
2023-06-26 18:23:22,260 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b611f5fe-fb85-4b8d-a770-59e3cc9a5f3a
2023-06-26 18:23:22,264 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33927
2023-06-26 18:23:22,264 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33927
2023-06-26 18:23:22,265 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34973
2023-06-26 18:23:22,265 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:22,265 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:22,265 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:22,265 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:22,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ob6kx3nr
2023-06-26 18:23:22,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc66f680-b9dc-4d81-bc5b-2d6d44c86e05
2023-06-26 18:23:22,275 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38705
2023-06-26 18:23:22,275 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38705
2023-06-26 18:23:22,275 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34223
2023-06-26 18:23:22,275 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:23:22,275 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:22,275 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:23:22,276 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:23:22,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jrm6gr44
2023-06-26 18:23:22,276 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1fd422b8-d3c8-4560-869c-de471c546b41
2023-06-26 18:23:23,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98d7c55b-8452-4406-a5df-0457447e5b25
2023-06-26 18:23:23,874 - distributed.worker - INFO - Starting Worker plugin PreImport-a90ca349-a445-4ab6-9fe8-a58f298c8007
2023-06-26 18:23:23,875 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:23,897 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:23,897 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:23,899 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:23,995 - distributed.worker - INFO - Starting Worker plugin PreImport-4e75a3fb-a3ec-4379-9fbc-bec220bed8d5
2023-06-26 18:23:23,996 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,009 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:24,010 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,011 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:24,034 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29c52bd0-053d-43b3-98c4-200b217b7044
2023-06-26 18:23:24,035 - distributed.worker - INFO - Starting Worker plugin PreImport-5f746a48-0b44-4dc7-8c07-7b7a9bd9d217
2023-06-26 18:23:24,036 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,052 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:24,052 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,053 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:24,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21e16e4b-8339-40c6-a6f5-5cba1c596aa2
2023-06-26 18:23:24,091 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,114 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:24,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,117 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:24,152 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3f84513-c6f8-44e4-ad9d-2245a29fd599
2023-06-26 18:23:24,152 - distributed.worker - INFO - Starting Worker plugin PreImport-5798fdb5-e0e9-498f-a7a8-e4bfbf942620
2023-06-26 18:23:24,154 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,173 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:24,173 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,175 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:24,178 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22348394-299b-4919-a947-4aff8c97040d
2023-06-26 18:23:24,179 - distributed.worker - INFO - Starting Worker plugin PreImport-8f8ab65c-32ac-4f95-876b-d8518f3e8d84
2023-06-26 18:23:24,181 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,199 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c3bee92-a211-424f-9257-daa24e4e6342
2023-06-26 18:23:24,200 - distributed.worker - INFO - Starting Worker plugin PreImport-dedeef12-dff7-4239-87fe-550d709fa76d
2023-06-26 18:23:24,201 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:24,201 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,202 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,203 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:24,215 - distributed.worker - INFO - Starting Worker plugin PreImport-13f01ee0-c50d-472a-b626-ebc486be37a5
2023-06-26 18:23:24,218 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,221 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:24,221 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,223 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:24,239 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:23:24,239 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:23:24,242 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:23:33,279 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,605 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,605 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,638 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,686 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,795 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,812 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,818 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,890 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,904 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,919 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:33,977 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:33,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:34,007 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:34,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:34,024 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:34,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:34,056 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:34,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:35,101 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:23:35,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,114 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,124 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687803815.125073] [exp01:365903:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:49936) failed: Address already in use
2023-06-26 18:23:35,125 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,125 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,125 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,125 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:23:35,137 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,137 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,137 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,137 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,137 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,137 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:35,138 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:23:38,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:43,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:48,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:52,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:52,087 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,087 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,087 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,087 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,087 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,087 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,087 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:52,088 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:23:59,384 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44911. Reason: worker-close
2023-06-26 18:23:59,384 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36181. Reason: worker-close
2023-06-26 18:23:59,384 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38705. Reason: worker-close
2023-06-26 18:23:59,384 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35833. Reason: worker-close
2023-06-26 18:23:59,384 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39829. Reason: worker-close
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32845. Reason: worker-close
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33927. Reason: worker-close
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37869. Reason: worker-close
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34589. Reason: worker-close
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44629. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35877. Reason: worker-close
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37885. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41891. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46225. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42843. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:23:59,385 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35023. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:23:59,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40081'. Reason: nanny-close
2023-06-26 18:23:59,386 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,386 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41623'. Reason: nanny-close
2023-06-26 18:23:59,385 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53044 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53066 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53056 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53064 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,387 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53226 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53026 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53202 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53018 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53080 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,388 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43261'. Reason: nanny-close
2023-06-26 18:23:59,388 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,386 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:53028 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,388 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34415'. Reason: nanny-close
2023-06-26 18:23:59,389 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,389 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42383'. Reason: nanny-close
2023-06-26 18:23:59,389 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,389 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40059'. Reason: nanny-close
2023-06-26 18:23:59,390 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,390 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35933'. Reason: nanny-close
2023-06-26 18:23:59,390 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45321'. Reason: nanny-close
2023-06-26 18:23:59,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39409'. Reason: nanny-close
2023-06-26 18:23:59,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44141'. Reason: nanny-close
2023-06-26 18:23:59,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45085'. Reason: nanny-close
2023-06-26 18:23:59,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45451'. Reason: nanny-close
2023-06-26 18:23:59,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33287'. Reason: nanny-close
2023-06-26 18:23:59,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39983'. Reason: nanny-close
2023-06-26 18:23:59,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,395 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39573'. Reason: nanny-close
2023-06-26 18:23:59,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,396 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40281'. Reason: nanny-close
2023-06-26 18:23:59,396 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:23:59,404 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42383 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40264 remote=tcp://10.120.104.11:42383>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42383 after 100 s
2023-06-26 18:23:59,406 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41623 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39368 remote=tcp://10.120.104.11:41623>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41623 after 100 s
2023-06-26 18:23:59,407 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43261 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55986 remote=tcp://10.120.104.11:43261>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43261 after 100 s
2023-06-26 18:23:59,407 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39983 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34274 remote=tcp://10.120.104.11:39983>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39983 after 100 s
2023-06-26 18:23:59,409 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34415 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:51994 remote=tcp://10.120.104.11:34415>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34415 after 100 s
2023-06-26 18:23:59,409 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45321 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60438 remote=tcp://10.120.104.11:45321>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45321 after 100 s
2023-06-26 18:23:59,409 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45451 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59294 remote=tcp://10.120.104.11:45451>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45451 after 100 s
2023-06-26 18:23:59,410 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39573 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39794 remote=tcp://10.120.104.11:39573>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39573 after 100 s
2023-06-26 18:23:59,411 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44141 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35746 remote=tcp://10.120.104.11:44141>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44141 after 100 s
2023-06-26 18:23:59,412 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40059 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40654 remote=tcp://10.120.104.11:40059>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40059 after 100 s
2023-06-26 18:23:59,413 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35933 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:54732 remote=tcp://10.120.104.11:35933>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35933 after 100 s
2023-06-26 18:23:59,413 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39409 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39320 remote=tcp://10.120.104.11:39409>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39409 after 100 s
2023-06-26 18:23:59,414 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45085 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37650 remote=tcp://10.120.104.11:45085>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45085 after 100 s
2023-06-26 18:23:59,416 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33287 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50766 remote=tcp://10.120.104.11:33287>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33287 after 100 s
2023-06-26 18:23:59,418 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40281 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33014 remote=tcp://10.120.104.11:40281>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40281 after 100 s
2023-06-26 18:24:02,597 - distributed.nanny - WARNING - Worker process still alive after 3.199996795654297 seconds, killing
2023-06-26 18:24:02,598 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,598 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,599 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,599 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,600 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,601 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:24:02,601 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:24:02,602 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,603 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:24:02,604 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:24:02,604 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,604 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,605 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:02,605 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:24:02,609 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:24:03,387 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,389 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,389 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,389 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,390 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,390 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,390 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,392 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,392 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,392 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,394 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,394 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,394 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,396 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,396 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,396 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365959 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365956 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365948 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365943 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365938 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365934 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365931 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365928 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365903 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365900 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365897 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365884 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365879 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365869 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365866 parent=362598 started daemon>
2023-06-26 18:24:03,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=365856 parent=362598 started daemon>
2023-06-26 18:24:06,559 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 365931 exit status was already read will report exitcode 255
2023-06-26 18:24:06,864 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 365869 exit status was already read will report exitcode 255
2023-06-26 18:24:07,447 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 365856 exit status was already read will report exitcode 255
2023-06-26 18:24:07,736 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 365956 exit status was already read will report exitcode 255
2023-06-26 18:24:09,549 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 365897 exit status was already read will report exitcode 255
2023-06-26 18:24:10,937 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 365903 exit status was already read will report exitcode 255
