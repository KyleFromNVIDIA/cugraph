RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 18:20:51,508 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 18:20:52,021 - distributed.scheduler - INFO - State start
2023-06-26 18:20:52,022 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-b40e7e3b', purging
2023-06-26 18:20:52,022 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-hjscym3k', purging
2023-06-26 18:20:52,023 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ernfo_v7', purging
2023-06-26 18:20:52,023 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pk9e4c36', purging
2023-06-26 18:20:52,023 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8l7bcbnb', purging
2023-06-26 18:20:52,023 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-kb2retz8', purging
2023-06-26 18:20:52,023 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8chbbf3l', purging
2023-06-26 18:20:52,023 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dmrzq1po', purging
2023-06-26 18:20:52,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ib_hey2a', purging
2023-06-26 18:20:52,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cy040uc3', purging
2023-06-26 18:20:52,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xe8yn86j', purging
2023-06-26 18:20:52,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cpsrbzxu', purging
2023-06-26 18:20:52,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-msmbxo57', purging
2023-06-26 18:20:52,025 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0rdb6dyd', purging
2023-06-26 18:20:52,025 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ejxob_qb', purging
2023-06-26 18:20:52,025 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dx8x3j1_', purging
2023-06-26 18:20:52,037 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 18:20:52,038 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 18:20:52,038 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 18:21:10,698 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38121', status: init, memory: 0, processing: 0>
2023-06-26 18:21:10,701 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38121
2023-06-26 18:21:10,701 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59664
2023-06-26 18:21:10,966 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40703', status: init, memory: 0, processing: 0>
2023-06-26 18:21:10,966 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40703
2023-06-26 18:21:10,966 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59672
2023-06-26 18:21:10,985 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37863', status: init, memory: 0, processing: 0>
2023-06-26 18:21:10,985 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37863
2023-06-26 18:21:10,985 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59678
2023-06-26 18:21:11,045 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41667', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,045 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41667
2023-06-26 18:21:11,045 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59694
2023-06-26 18:21:11,050 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44383', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44383
2023-06-26 18:21:11,050 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59704
2023-06-26 18:21:11,085 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35649', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,086 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35649
2023-06-26 18:21:11,086 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59710
2023-06-26 18:21:11,088 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39493', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39493
2023-06-26 18:21:11,088 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59720
2023-06-26 18:21:11,140 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39643', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,140 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39643
2023-06-26 18:21:11,140 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59728
2023-06-26 18:21:11,152 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42905', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,152 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42905
2023-06-26 18:21:11,152 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59738
2023-06-26 18:21:11,165 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36055', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,165 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36055
2023-06-26 18:21:11,165 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59750
2023-06-26 18:21:11,226 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36599', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,226 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36599
2023-06-26 18:21:11,226 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59760
2023-06-26 18:21:11,231 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37449', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,231 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37449
2023-06-26 18:21:11,231 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59768
2023-06-26 18:21:11,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39491', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,238 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39491
2023-06-26 18:21:11,238 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59754
2023-06-26 18:21:11,263 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37253', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,263 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37253
2023-06-26 18:21:11,263 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59790
2023-06-26 18:21:11,264 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:45357', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:45357
2023-06-26 18:21:11,264 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59774
2023-06-26 18:21:11,266 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41571', status: init, memory: 0, processing: 0>
2023-06-26 18:21:11,267 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41571
2023-06-26 18:21:11,267 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:59792
2023-06-26 18:21:20,622 - distributed.scheduler - INFO - Receive client connection: Client-3fd70b61-144e-11ee-89f2-5cff35c1a711
2023-06-26 18:21:20,623 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44146
2023-06-26 18:21:21,325 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 18:22:12,666 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:23:06,710 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 18:23:06,713 - distributed.scheduler - INFO - Restarting workers and releasing all keys.
2023-06-26 18:23:06,736 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59710; closing.
2023-06-26 18:23:06,736 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35649', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35649
2023-06-26 18:23:06,738 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59750; closing.
2023-06-26 18:23:06,738 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59760; closing.
2023-06-26 18:23:06,739 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36055', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,739 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36055
2023-06-26 18:23:06,739 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36599', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,739 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36599
2023-06-26 18:23:06,739 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59790; closing.
2023-06-26 18:23:06,740 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37253', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,740 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37253
2023-06-26 18:23:06,740 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59768; closing.
2023-06-26 18:23:06,740 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59678; closing.
2023-06-26 18:23:06,741 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37449', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37449
2023-06-26 18:23:06,741 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37863', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37863
2023-06-26 18:23:06,742 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59664; closing.
2023-06-26 18:23:06,742 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59720; closing.
2023-06-26 18:23:06,742 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59754; closing.
2023-06-26 18:23:06,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38121', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38121
2023-06-26 18:23:06,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39493', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,743 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39493
2023-06-26 18:23:06,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39491', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,743 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39491
2023-06-26 18:23:06,743 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59728; closing.
2023-06-26 18:23:06,744 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39643', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,744 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39643
2023-06-26 18:23:06,745 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59672; closing.
2023-06-26 18:23:06,745 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40703', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,745 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40703
2023-06-26 18:23:06,748 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59792; closing.
2023-06-26 18:23:06,748 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41571', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,748 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41571
2023-06-26 18:23:06,759 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59694; closing.
2023-06-26 18:23:06,760 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41667', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,760 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41667
2023-06-26 18:23:06,761 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59704; closing.
2023-06-26 18:23:06,761 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44383', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44383
2023-06-26 18:23:06,762 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59774; closing.
2023-06-26 18:23:06,762 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:45357', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,762 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45357
2023-06-26 18:23:06,763 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:59738; closing.
2023-06-26 18:23:06,763 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42905', status: closing, memory: 0, processing: 0>
2023-06-26 18:23:06,763 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42905
2023-06-26 18:23:06,763 - distributed.scheduler - INFO - Lost all workers
2023-06-26 18:23:20,709 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:32845', status: init, memory: 0, processing: 0>
2023-06-26 18:23:20,710 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:32845
2023-06-26 18:23:20,710 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53018
2023-06-26 18:23:20,775 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39829', status: init, memory: 0, processing: 0>
2023-06-26 18:23:20,776 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39829
2023-06-26 18:23:20,776 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53026
2023-06-26 18:23:20,817 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35877', status: init, memory: 0, processing: 0>
2023-06-26 18:23:20,817 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35877
2023-06-26 18:23:20,817 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53028
2023-06-26 18:23:20,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44911', status: init, memory: 0, processing: 0>
2023-06-26 18:23:20,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44911
2023-06-26 18:23:20,859 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53044
2023-06-26 18:23:20,895 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37869', status: init, memory: 0, processing: 0>
2023-06-26 18:23:20,895 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37869
2023-06-26 18:23:20,895 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53056
2023-06-26 18:23:20,904 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35833', status: init, memory: 0, processing: 0>
2023-06-26 18:23:20,905 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35833
2023-06-26 18:23:20,905 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53064
2023-06-26 18:23:20,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36181', status: init, memory: 0, processing: 0>
2023-06-26 18:23:20,910 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36181
2023-06-26 18:23:20,910 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53066
2023-06-26 18:23:20,924 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34589', status: init, memory: 0, processing: 0>
2023-06-26 18:23:20,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34589
2023-06-26 18:23:20,924 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53080
2023-06-26 18:23:23,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35023', status: init, memory: 0, processing: 0>
2023-06-26 18:23:23,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35023
2023-06-26 18:23:23,896 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53172
2023-06-26 18:23:24,009 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44629', status: init, memory: 0, processing: 0>
2023-06-26 18:23:24,009 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44629
2023-06-26 18:23:24,009 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53176
2023-06-26 18:23:24,051 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46225', status: init, memory: 0, processing: 0>
2023-06-26 18:23:24,052 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46225
2023-06-26 18:23:24,052 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53182
2023-06-26 18:23:24,114 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41891', status: init, memory: 0, processing: 0>
2023-06-26 18:23:24,114 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41891
2023-06-26 18:23:24,114 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53192
2023-06-26 18:23:24,172 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38705', status: init, memory: 0, processing: 0>
2023-06-26 18:23:24,172 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38705
2023-06-26 18:23:24,172 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53202
2023-06-26 18:23:24,200 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42843', status: init, memory: 0, processing: 0>
2023-06-26 18:23:24,200 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42843
2023-06-26 18:23:24,201 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53210
2023-06-26 18:23:24,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33927', status: init, memory: 0, processing: 0>
2023-06-26 18:23:24,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33927
2023-06-26 18:23:24,221 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53226
2023-06-26 18:23:24,239 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37885', status: init, memory: 0, processing: 0>
2023-06-26 18:23:24,239 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37885
2023-06-26 18:23:24,239 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:53230
2023-06-26 18:23:24,250 - distributed.scheduler - INFO - Restarting finished.
2023-06-26 18:23:35,129 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 18:23:54,506 - distributed.scheduler - INFO - Remove client Client-3fd70b61-144e-11ee-89f2-5cff35c1a711
2023-06-26 18:23:54,506 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44146; closing.
2023-06-26 18:23:54,507 - distributed.scheduler - INFO - Remove client Client-3fd70b61-144e-11ee-89f2-5cff35c1a711
2023-06-26 18:23:54,507 - distributed.scheduler - INFO - Close client connection: Client-3fd70b61-144e-11ee-89f2-5cff35c1a711
2023-06-26 18:23:59,385 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 18:23:59,385 - distributed.core - INFO - Connection to tcp://10.120.104.11:53210 has been closed.
2023-06-26 18:23:59,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42843', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,385 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42843
2023-06-26 18:23:59,387 - distributed.core - INFO - Connection to tcp://10.120.104.11:53176 has been closed.
2023-06-26 18:23:59,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44629', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,387 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44629
2023-06-26 18:23:59,387 - distributed.core - INFO - Connection to tcp://10.120.104.11:53230 has been closed.
2023-06-26 18:23:59,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37885', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,387 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37885
2023-06-26 18:23:59,387 - distributed.core - INFO - Connection to tcp://10.120.104.11:53192 has been closed.
2023-06-26 18:23:59,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41891', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,388 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41891
2023-06-26 18:23:59,388 - distributed.core - INFO - Connection to tcp://10.120.104.11:53182 has been closed.
2023-06-26 18:23:59,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46225', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,388 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46225
2023-06-26 18:23:59,388 - distributed.core - INFO - Connection to tcp://10.120.104.11:53172 has been closed.
2023-06-26 18:23:59,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35023', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,388 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35023
2023-06-26 18:23:59,389 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 18:23:59,390 - distributed.core - INFO - Connection to tcp://10.120.104.11:53044 has been closed.
2023-06-26 18:23:59,390 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44911', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44911
2023-06-26 18:23:59,390 - distributed.core - INFO - Connection to tcp://10.120.104.11:53066 has been closed.
2023-06-26 18:23:59,390 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36181', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,390 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36181
2023-06-26 18:23:59,391 - distributed.core - INFO - Connection to tcp://10.120.104.11:53064 has been closed.
2023-06-26 18:23:59,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35833', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,391 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35833
2023-06-26 18:23:59,391 - distributed.core - INFO - Connection to tcp://10.120.104.11:53202 has been closed.
2023-06-26 18:23:59,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38705', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,391 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38705
2023-06-26 18:23:59,391 - distributed.core - INFO - Connection to tcp://10.120.104.11:53026 has been closed.
2023-06-26 18:23:59,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39829', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,392 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39829
2023-06-26 18:23:59,392 - distributed.core - INFO - Connection to tcp://10.120.104.11:53056 has been closed.
2023-06-26 18:23:59,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37869', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,392 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37869
2023-06-26 18:23:59,392 - distributed.core - INFO - Connection to tcp://10.120.104.11:53080 has been closed.
2023-06-26 18:23:59,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34589', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,392 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34589
2023-06-26 18:23:59,392 - distributed.core - INFO - Connection to tcp://10.120.104.11:53018 has been closed.
2023-06-26 18:23:59,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:32845', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,393 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32845
2023-06-26 18:23:59,393 - distributed.core - INFO - Connection to tcp://10.120.104.11:53226 has been closed.
2023-06-26 18:23:59,393 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33927', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,393 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33927
2023-06-26 18:23:59,393 - distributed.core - INFO - Connection to tcp://10.120.104.11:53028 has been closed.
2023-06-26 18:23:59,393 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35877', status: running, memory: 0, processing: 0>
2023-06-26 18:23:59,393 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35877
2023-06-26 18:23:59,393 - distributed.scheduler - INFO - Lost all workers
2023-06-26 18:23:59,393 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53018>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53018>: Stream is closed
2023-06-26 18:23:59,396 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53226>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53226>: Stream is closed
2023-06-26 18:23:59,396 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53080>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53080>: Stream is closed
2023-06-26 18:23:59,396 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53172>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,396 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53064>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53064>: Stream is closed
2023-06-26 18:23:59,396 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53028>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53028>: Stream is closed
2023-06-26 18:23:59,396 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53066>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53066>: Stream is closed
2023-06-26 18:23:59,396 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53056>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53056>: Stream is closed
2023-06-26 18:23:59,397 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53230>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,397 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53202>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53202>: Stream is closed
2023-06-26 18:23:59,397 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53026>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53026>: Stream is closed
2023-06-26 18:23:59,397 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53192>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,397 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53176>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,397 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53044>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53044>: Stream is closed
2023-06-26 18:23:59,397 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:53182>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:23:59,398 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 18:23:59,402 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 18:23:59,403 - distributed.scheduler - INFO - End scheduler
