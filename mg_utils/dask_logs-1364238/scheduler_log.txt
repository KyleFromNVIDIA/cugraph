RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 20:08:15,721 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 20:08:16,314 - distributed.scheduler - INFO - State start
2023-06-22 20:08:16,325 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 20:08:16,326 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 20:08:16,327 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 20:08:27,255 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:35889', status: init, memory: 0, processing: 0>
2023-06-22 20:08:27,532 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:35889
2023-06-22 20:08:27,532 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:46354
2023-06-22 20:08:27,544 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:39895', status: init, memory: 0, processing: 0>
2023-06-22 20:08:27,544 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:39895
2023-06-22 20:08:27,544 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:46388
2023-06-22 20:08:27,545 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:40513', status: init, memory: 0, processing: 0>
2023-06-22 20:08:27,545 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:40513
2023-06-22 20:08:27,545 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:46424
2023-06-22 20:08:27,545 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:37411', status: init, memory: 0, processing: 0>
2023-06-22 20:08:27,546 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:37411
2023-06-22 20:08:27,546 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:46394
2023-06-22 20:08:27,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:38113', status: init, memory: 0, processing: 0>
2023-06-22 20:08:27,547 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:38113
2023-06-22 20:08:27,547 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:46410
2023-06-22 20:08:27,547 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:42147', status: init, memory: 0, processing: 0>
2023-06-22 20:08:27,548 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:42147
2023-06-22 20:08:27,548 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:46366
2023-06-22 20:08:27,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:42573', status: init, memory: 0, processing: 0>
2023-06-22 20:08:27,549 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:42573
2023-06-22 20:08:27,549 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:46374
2023-06-22 20:08:27,549 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:37445', status: init, memory: 0, processing: 0>
2023-06-22 20:08:27,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:37445
2023-06-22 20:08:27,550 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:46428
2023-06-22 20:08:33,971 - distributed.scheduler - INFO - Receive client connection: Client-90c3764e-1138-11ee-91d8-d8c49778ced7
2023-06-22 20:08:33,972 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45946
2023-06-22 20:08:34,070 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 20:09:23,201 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:09:25,434 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 20:09:29,267 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:09:30,304 - distributed.scheduler - INFO - Remove client Client-90c3764e-1138-11ee-91d8-d8c49778ced7
2023-06-22 20:09:30,307 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:45946; closing.
2023-06-22 20:09:30,307 - distributed.scheduler - INFO - Remove client Client-90c3764e-1138-11ee-91d8-d8c49778ced7
2023-06-22 20:09:30,309 - distributed.scheduler - INFO - Close client connection: Client-90c3764e-1138-11ee-91d8-d8c49778ced7
2023-06-22 20:11:20,207 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 20:11:20,208 - distributed.core - INFO - Connection to tcp://10.33.227.169:46354 has been closed.
2023-06-22 20:11:20,208 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:35889', status: running, memory: 0, processing: 0>
2023-06-22 20:11:20,209 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:35889
2023-06-22 20:11:20,210 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 20:11:20,210 - distributed.core - INFO - Connection to tcp://10.33.227.169:46388 has been closed.
2023-06-22 20:11:20,210 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:39895', status: running, memory: 0, processing: 0>
2023-06-22 20:11:20,211 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:39895
2023-06-22 20:11:20,211 - distributed.core - INFO - Connection to tcp://10.33.227.169:46424 has been closed.
2023-06-22 20:11:20,211 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:40513', status: running, memory: 0, processing: 0>
2023-06-22 20:11:20,211 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:40513
2023-06-22 20:11:20,211 - distributed.core - INFO - Connection to tcp://10.33.227.169:46366 has been closed.
2023-06-22 20:11:20,211 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:42147', status: running, memory: 0, processing: 0>
2023-06-22 20:11:20,211 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:42147
2023-06-22 20:11:20,212 - distributed.core - INFO - Connection to tcp://10.33.227.169:46374 has been closed.
2023-06-22 20:11:20,212 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:42573', status: running, memory: 0, processing: 0>
2023-06-22 20:11:20,212 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:42573
2023-06-22 20:11:20,213 - distributed.core - INFO - Connection to tcp://10.33.227.169:46410 has been closed.
2023-06-22 20:11:20,213 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:38113', status: running, memory: 0, processing: 0>
2023-06-22 20:11:20,213 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:38113
2023-06-22 20:11:20,213 - distributed.core - INFO - Connection to tcp://10.33.227.169:46394 has been closed.
2023-06-22 20:11:20,213 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:37411', status: running, memory: 0, processing: 0>
2023-06-22 20:11:20,213 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:37411
2023-06-22 20:11:20,214 - distributed.core - INFO - Connection to tcp://10.33.227.169:46428 has been closed.
2023-06-22 20:11:20,214 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:37445', status: running, memory: 0, processing: 0>
2023-06-22 20:11:20,214 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:37445
2023-06-22 20:11:20,214 - distributed.scheduler - INFO - Lost all workers
2023-06-22 20:11:20,214 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46394>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46394>: Stream is closed
2023-06-22 20:11:20,215 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46428>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46428>: Stream is closed
2023-06-22 20:11:20,216 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46410>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46410>: Stream is closed
2023-06-22 20:11:20,216 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46388>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 20:11:20,216 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46424>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 20:11:20,216 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46366>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 20:11:20,216 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:46374>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 20:11:20,217 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 20:11:20,220 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 20:11:20,220 - distributed.scheduler - INFO - End scheduler
