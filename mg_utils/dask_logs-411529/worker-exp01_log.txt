RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 19:06:54,304 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37737'
2023-06-26 19:06:54,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44883'
2023-06-26 19:06:54,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34717'
2023-06-26 19:06:54,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44081'
2023-06-26 19:06:54,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36603'
2023-06-26 19:06:54,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46113'
2023-06-26 19:06:54,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41465'
2023-06-26 19:06:54,320 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34115'
2023-06-26 19:06:54,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43145'
2023-06-26 19:06:54,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42117'
2023-06-26 19:06:54,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36551'
2023-06-26 19:06:54,328 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43685'
2023-06-26 19:06:54,330 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43415'
2023-06-26 19:06:54,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44355'
2023-06-26 19:06:54,337 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34635'
2023-06-26 19:06:54,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37955'
2023-06-26 19:06:55,883 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:55,884 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:55,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:55,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:55,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:55,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:55,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:55,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:55,999 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:55,999 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,011 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,011 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,055 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,058 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,061 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,061 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,066 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,066 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:06:56,066 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,066 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:06:56,165 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,169 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,174 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,177 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,186 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,186 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,189 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,214 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,233 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,236 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,243 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,244 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:06:56,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:07:02,469 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41581
2023-06-26 19:07:02,470 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41581
2023-06-26 19:07:02,470 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36897
2023-06-26 19:07:02,470 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,470 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,470 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39165
2023-06-26 19:07:02,470 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,470 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39165
2023-06-26 19:07:02,470 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39007
2023-06-26 19:07:02,470 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,470 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ksb36mtq
2023-06-26 19:07:02,470 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,470 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,470 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,470 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,470 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-if_r8sn8
2023-06-26 19:07:02,471 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a206c3f2-8369-4aea-81ed-57ecf2d820ce
2023-06-26 19:07:02,471 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c9e3e16-1c01-48c4-a55b-e0127a3f6264
2023-06-26 19:07:02,471 - distributed.worker - INFO - Starting Worker plugin PreImport-ceff2464-0f13-4ddb-bb7f-4d3a06d72d2a
2023-06-26 19:07:02,471 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b1c9eb0-badf-4a30-aabb-08783859c607
2023-06-26 19:07:02,476 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41853
2023-06-26 19:07:02,476 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41853
2023-06-26 19:07:02,476 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39809
2023-06-26 19:07:02,476 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,476 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,476 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,476 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_y7yfoon
2023-06-26 19:07:02,477 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f14db1d-1fed-4963-8ea7-82f0b1dccf9d
2023-06-26 19:07:02,693 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43631
2023-06-26 19:07:02,694 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43631
2023-06-26 19:07:02,694 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41291
2023-06-26 19:07:02,694 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,694 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,694 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,694 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,694 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kbhtxml0
2023-06-26 19:07:02,694 - distributed.worker - INFO - Starting Worker plugin PreImport-091626f1-8a1e-433f-a078-48912d02a174
2023-06-26 19:07:02,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c80590ee-c313-4f2e-a4e4-38aed2072f19
2023-06-26 19:07:02,709 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46179
2023-06-26 19:07:02,709 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46179
2023-06-26 19:07:02,709 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35251
2023-06-26 19:07:02,709 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,709 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,709 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,709 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5lks82af
2023-06-26 19:07:02,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c884776-af95-42b7-b5b5-9a01a15d6191
2023-06-26 19:07:02,710 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37519
2023-06-26 19:07:02,711 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37519
2023-06-26 19:07:02,711 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35187
2023-06-26 19:07:02,711 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,711 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,711 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,711 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,711 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k3p96itt
2023-06-26 19:07:02,711 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cec88fc0-107a-4ad5-b70e-68d58388ef5d
2023-06-26 19:07:02,850 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39373
2023-06-26 19:07:02,850 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39373
2023-06-26 19:07:02,850 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38171
2023-06-26 19:07:02,850 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,850 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,850 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,850 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gqhxcu0l
2023-06-26 19:07:02,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ade1793-b54a-46ad-a7d2-ae5a9bcd27a8
2023-06-26 19:07:02,853 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44087
2023-06-26 19:07:02,853 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44087
2023-06-26 19:07:02,853 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42237
2023-06-26 19:07:02,853 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,853 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,853 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37181
2023-06-26 19:07:02,853 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,853 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37181
2023-06-26 19:07:02,853 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,853 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-43yizvty
2023-06-26 19:07:02,853 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38549
2023-06-26 19:07:02,853 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,853 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,853 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,854 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,854 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dc1p0lzq
2023-06-26 19:07:02,854 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2b185f5-d732-4547-b4d9-dc3ee0486811
2023-06-26 19:07:02,855 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d034ec7-1e70-4e38-a05f-dbe130df3a19
2023-06-26 19:07:02,862 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38047
2023-06-26 19:07:02,862 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38047
2023-06-26 19:07:02,862 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41719
2023-06-26 19:07:02,862 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:02,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:02,862 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:02,862 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:02,862 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l9pzu7cg
2023-06-26 19:07:02,863 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d40c002-f887-4329-b7de-d4891044c98f
2023-06-26 19:07:02,863 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbf50487-e8dc-4ed4-88bb-721287c6d483
2023-06-26 19:07:03,209 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43177
2023-06-26 19:07:03,209 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43177
2023-06-26 19:07:03,209 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41197
2023-06-26 19:07:03,209 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:03,209 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:03,209 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:03,209 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:03,209 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g3813tp0
2023-06-26 19:07:03,210 - distributed.worker - INFO - Starting Worker plugin PreImport-68a00f90-06df-43a8-8ae6-59a5f3f29c0b
2023-06-26 19:07:03,210 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84d00c49-47a0-49c5-9c7d-b505a1679cc1
2023-06-26 19:07:03,212 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42747
2023-06-26 19:07:03,212 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42747
2023-06-26 19:07:03,212 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33341
2023-06-26 19:07:03,212 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:03,212 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:03,212 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:03,212 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:03,212 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9gt3jx25
2023-06-26 19:07:03,212 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c6631e5-82a3-40df-ad30-825cb7a4cc5a
2023-06-26 19:07:03,234 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35151
2023-06-26 19:07:03,234 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35151
2023-06-26 19:07:03,234 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32803
2023-06-26 19:07:03,234 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:03,234 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:03,234 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:03,234 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:03,234 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-69q7gue_
2023-06-26 19:07:03,234 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42683
2023-06-26 19:07:03,235 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42683
2023-06-26 19:07:03,235 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44727
2023-06-26 19:07:03,235 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d11c08ca-95cb-405b-a68b-83cf268a1386
2023-06-26 19:07:03,235 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:03,235 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:03,235 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:03,235 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:03,235 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-351ccsw7
2023-06-26 19:07:03,236 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d1a636b-3540-4888-994e-cd6a53d2b8bc
2023-06-26 19:07:03,238 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37423
2023-06-26 19:07:03,238 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37423
2023-06-26 19:07:03,238 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37977
2023-06-26 19:07:03,238 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:03,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:03,238 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:03,238 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:03,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ebja8273
2023-06-26 19:07:03,238 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d48e962-c830-4f53-972c-ea1651c92ba2
2023-06-26 19:07:03,240 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39355
2023-06-26 19:07:03,241 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39355
2023-06-26 19:07:03,241 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33253
2023-06-26 19:07:03,241 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:07:03,241 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:03,241 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:07:03,241 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:07:03,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s1o12tl3
2023-06-26 19:07:03,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7160293c-1cd1-4fab-8440-f21cebf24096
2023-06-26 19:07:03,244 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe05e6f2-022a-4cd8-b187-0ad54955ac0a
2023-06-26 19:07:06,146 - distributed.worker - INFO - Starting Worker plugin PreImport-16b2005f-c911-4afd-989f-9183a60315cd
2023-06-26 19:07:06,147 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,176 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,176 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,177 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,267 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f9fb60c-c95e-4e58-80bd-b09fe6bf11a2
2023-06-26 19:07:06,268 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,287 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,287 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,289 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,478 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5eaf3f29-893a-4fe6-ba77-8810ee1f0784
2023-06-26 19:07:06,478 - distributed.worker - INFO - Starting Worker plugin PreImport-c6ab225e-b5e8-4fab-b615-2aac2c9a1c1f
2023-06-26 19:07:06,479 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,498 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,498 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,500 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee2a1145-e3b0-478b-a29c-ecb06778ae9e
2023-06-26 19:07:06,553 - distributed.worker - INFO - Starting Worker plugin PreImport-5584e839-9527-4356-91ad-0634dd95c224
2023-06-26 19:07:06,554 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,573 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,573 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,575 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,603 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c432cdcd-d0f6-49e5-993f-8d8e4f713fa0
2023-06-26 19:07:06,604 - distributed.worker - INFO - Starting Worker plugin PreImport-0f4424a2-dde6-4a1c-8750-71f6abf5789e
2023-06-26 19:07:06,604 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,615 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,615 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,617 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,626 - distributed.worker - INFO - Starting Worker plugin PreImport-84887901-6ad9-4e02-b9a8-1137936388c7
2023-06-26 19:07:06,628 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,650 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,651 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,652 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,695 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-021d4cab-6d25-47dc-84c0-acfc6e796dfb
2023-06-26 19:07:06,695 - distributed.worker - INFO - Starting Worker plugin PreImport-4de62407-f631-4f01-9291-915005f2761a
2023-06-26 19:07:06,696 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,711 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,711 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,712 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,720 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-225444d0-3f1d-43bc-af16-5764fb5bbb3b
2023-06-26 19:07:06,721 - distributed.worker - INFO - Starting Worker plugin PreImport-2e5be9b5-cdf4-4987-92c2-179d3cd29053
2023-06-26 19:07:06,722 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,743 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,743 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,746 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,759 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e45674f-0e13-491a-861d-947f9face2b6
2023-06-26 19:07:06,759 - distributed.worker - INFO - Starting Worker plugin PreImport-6f67e73b-6d3e-4c4c-aa7a-49c9dd869f45
2023-06-26 19:07:06,760 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,776 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,776 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,777 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,784 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0f26013-143d-4aae-91ca-cd847e93e1e0
2023-06-26 19:07:06,784 - distributed.worker - INFO - Starting Worker plugin PreImport-6d0c6a2b-0f33-44bd-b312-a12ddaa0910c
2023-06-26 19:07:06,786 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,791 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f103cf64-d941-41f4-89ed-ef26e509eb31
2023-06-26 19:07:06,792 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,808 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,808 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,811 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,812 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,813 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,815 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,852 - distributed.worker - INFO - Starting Worker plugin PreImport-0c676281-e0d9-489e-90cf-b141adfec4f6
2023-06-26 19:07:06,853 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee41399d-5747-4259-b852-9826febac1f3
2023-06-26 19:07:06,861 - distributed.worker - INFO - Starting Worker plugin PreImport-186ae95f-c9f8-4575-a9ef-0a3aeb5c4981
2023-06-26 19:07:06,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a5fcccfd-cf47-415b-aa4a-535302d70d6e
2023-06-26 19:07:06,868 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,873 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,873 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,874 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,874 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f90be0f4-2e36-4389-b500-a5f325783fd8
2023-06-26 19:07:06,874 - distributed.worker - INFO - Starting Worker plugin PreImport-41047ce3-e295-4364-be09-63bdecdd7a0a
2023-06-26 19:07:06,875 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,875 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,875 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,880 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,880 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,882 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,886 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb397823-e130-4508-ba64-24e7b61cf331
2023-06-26 19:07:06,886 - distributed.worker - INFO - Starting Worker plugin PreImport-317aa77b-e84e-47f9-a610-1a739aa5162c
2023-06-26 19:07:06,888 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,889 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,889 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,891 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:06,906 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:07:06,906 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:07:06,908 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:07:43,941 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,941 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,941 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,941 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,941 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,941 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,942 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,942 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,942 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,942 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,942 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,942 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,942 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,945 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,945 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,947 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,956 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,957 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,957 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,957 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,957 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:43,957 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:07:44,641 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,641 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,641 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,641 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,641 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,641 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,641 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:44,642 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:07:47,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:07:59,286 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,474 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,493 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,526 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,641 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,771 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,857 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,860 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,904 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,938 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,969 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,971 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:07:59,996 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:08:00,111 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:08:00,125 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:08:00,179 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:08:06,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:06,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:45,356 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,357 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,357 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,358 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,362 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,362 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,362 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,364 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,365 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,365 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,365 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,366 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,366 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,366 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,366 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:45,368 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 19:08:48,453 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,459 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,460 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,460 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,460 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,460 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:48,461 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:08:53,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:08:53,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:01,366 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,366 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,366 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,367 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,381 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,395 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,395 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,395 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,395 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:01,395 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:05,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:05,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:05,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:06,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,201 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,201 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35151. Reason: scheduler-restart
2023-06-26 19:09:06,202 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,202 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37181. Reason: scheduler-restart
2023-06-26 19:09:06,202 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,202 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,203 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,203 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37423. Reason: scheduler-restart
2023-06-26 19:09:06,203 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37519. Reason: scheduler-restart
2023-06-26 19:09:06,203 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,203 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,204 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,204 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38047. Reason: scheduler-restart
2023-06-26 19:09:06,204 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,204 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39165. Reason: scheduler-restart
2023-06-26 19:09:06,205 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,205 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,205 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35151
2023-06-26 19:09:06,205 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39355. Reason: scheduler-restart
2023-06-26 19:09:06,205 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,205 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39373. Reason: scheduler-restart
2023-06-26 19:09:06,205 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,205 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,206 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,206 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41581. Reason: scheduler-restart
2023-06-26 19:09:06,206 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,206 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,206 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,206 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,206 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,206 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,206 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41853. Reason: scheduler-restart
2023-06-26 19:09:06,207 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42683. Reason: scheduler-restart
2023-06-26 19:09:06,207 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,207 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,207 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,207 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,208 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,208 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,208 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,208 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,208 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,209 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,209 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,210 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 19:09:06,214 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42747. Reason: scheduler-restart
2023-06-26 19:09:06,214 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43177. Reason: scheduler-restart
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37181
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37519
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37423
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38047
2023-06-26 19:09:06,215 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39165
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39373
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41581
2023-06-26 19:09:06,215 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41853
2023-06-26 19:09:06,219 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37181
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37519
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37423
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38047
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39165
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39373
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41581
2023-06-26 19:09:06,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41853
2023-06-26 19:09:06,220 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42683
2023-06-26 19:09:06,221 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,222 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,222 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46179. Reason: scheduler-restart
2023-06-26 19:09:06,223 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,225 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37181
2023-06-26 19:09:06,225 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37519
2023-06-26 19:09:06,225 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37423
2023-06-26 19:09:06,225 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38047
2023-06-26 19:09:06,225 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39165
2023-06-26 19:09:06,225 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 19:09:06,225 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39373
2023-06-26 19:09:06,225 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41581
2023-06-26 19:09:06,226 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41853
2023-06-26 19:09:06,226 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42683
2023-06-26 19:09:06,226 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43177
2023-06-26 19:09:06,227 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,227 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44087. Reason: scheduler-restart
2023-06-26 19:09:06,229 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,230 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,231 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37181
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37519
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37423
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38047
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39165
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39373
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41581
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41853
2023-06-26 19:09:06,232 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42683
2023-06-26 19:09:06,243 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42683
2023-06-26 19:09:06,244 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43631. Reason: scheduler-restart
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37181
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37519
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37423
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38047
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43177
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39165
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39373
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41581
2023-06-26 19:09:06,245 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41853
2023-06-26 19:09:06,245 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,246 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42683
2023-06-26 19:09:06,247 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,247 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43177
2023-06-26 19:09:06,247 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46179
2023-06-26 19:09:06,248 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42747
2023-06-26 19:09:06,248 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43177
2023-06-26 19:09:06,248 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,250 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46179
2023-06-26 19:09:06,250 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42747
2023-06-26 19:09:06,250 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43631
2023-06-26 19:09:06,251 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 19:09:06,254 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:06,254 - distributed.nanny - INFO - Worker closed
2023-06-26 19:09:07,976 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:09,605 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:09,924 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:10,240 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:10,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:10,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:10,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:12,236 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,240 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:12,240 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:12,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:12,491 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,492 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,494 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,496 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,502 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:12,502 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:12,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:12,503 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:12,524 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,525 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,527 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,532 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,534 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,536 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,542 - distributed.nanny - WARNING - Restarting worker
2023-06-26 19:09:12,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:12,753 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:13,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:13,839 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,071 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45981
2023-06-26 19:09:14,071 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45981
2023-06-26 19:09:14,071 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38377
2023-06-26 19:09:14,071 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:14,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:14,071 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:14,071 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:14,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zy6xjr7o
2023-06-26 19:09:14,071 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86722f5c-2ae8-4502-ba11-13ed71241ed2
2023-06-26 19:09:14,072 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41129
2023-06-26 19:09:14,072 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41129
2023-06-26 19:09:14,072 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45233
2023-06-26 19:09:14,072 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:14,072 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:14,072 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:14,072 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:14,072 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xtw0r7nw
2023-06-26 19:09:14,073 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1cbf47a1-0bb1-4286-898f-f4a82430554b
2023-06-26 19:09:14,124 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,124 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,210 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,210 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,216 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,216 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,220 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,239 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,239 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,242 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 19:09:14,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 19:09:14,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,304 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,389 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,398 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,398 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,429 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 19:09:14,599 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33445
2023-06-26 19:09:14,599 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33445
2023-06-26 19:09:14,599 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37073
2023-06-26 19:09:14,599 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:14,599 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:14,599 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:14,599 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:14,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kolklt8u
2023-06-26 19:09:14,600 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e53ddff-fb5b-4f0b-aad3-784123c77db3
2023-06-26 19:09:14,602 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43263
2023-06-26 19:09:14,603 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43263
2023-06-26 19:09:14,603 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40923
2023-06-26 19:09:14,603 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:14,603 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:14,603 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:14,603 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:14,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-el5aki6a
2023-06-26 19:09:14,603 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40d9315a-8aeb-402a-918c-ff6b2eed59a8
2023-06-26 19:09:16,575 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-262d97e8-8b1c-4d4d-9042-fa320292dfba
2023-06-26 19:09:16,576 - distributed.worker - INFO - Starting Worker plugin PreImport-e4454d0e-7905-4718-b70f-b7400885365d
2023-06-26 19:09:16,577 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:16,606 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:16,606 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:16,609 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:16,720 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46dad9cf-809a-4e51-8b47-ca724022e8fa
2023-06-26 19:09:16,720 - distributed.worker - INFO - Starting Worker plugin PreImport-deef2aa3-6ddd-438c-b406-a03b2407e911
2023-06-26 19:09:16,722 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:16,740 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:16,740 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:16,742 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:17,361 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8969042-c304-49e1-b6fd-12f374fa4c6f
2023-06-26 19:09:17,362 - distributed.worker - INFO - Starting Worker plugin PreImport-7a652014-5356-4925-850a-131f6990cb37
2023-06-26 19:09:17,364 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:17,380 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:17,380 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:17,383 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:17,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1390302c-fb3f-4dbd-a393-e73756923b9b
2023-06-26 19:09:17,383 - distributed.worker - INFO - Starting Worker plugin PreImport-5c8e4490-7f89-4261-82b9-10a86915e1c0
2023-06-26 19:09:17,386 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:17,403 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:17,403 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:17,406 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:18,396 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37753
2023-06-26 19:09:18,396 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37753
2023-06-26 19:09:18,396 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42617
2023-06-26 19:09:18,396 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:18,396 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:18,396 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:18,396 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:18,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-on559o4f
2023-06-26 19:09:18,397 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3d706fa-6016-4cf8-8ae4-ebf741282761
2023-06-26 19:09:19,651 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b869cd7-145e-4078-a9ce-51a32374ce65
2023-06-26 19:09:19,652 - distributed.worker - INFO - Starting Worker plugin PreImport-e289c062-1052-43c9-90d1-ef5b5378599b
2023-06-26 19:09:19,652 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:19,661 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:19,661 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:19,665 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:20,019 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32855
2023-06-26 19:09:20,020 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32855
2023-06-26 19:09:20,020 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42011
2023-06-26 19:09:20,020 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:20,020 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:20,020 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:20,020 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:20,020 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ch1nc_i
2023-06-26 19:09:20,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e571c063-8ec5-4ba0-b307-b0ed2767b2fc
2023-06-26 19:09:21,168 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35639
2023-06-26 19:09:21,169 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35639
2023-06-26 19:09:21,169 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45443
2023-06-26 19:09:21,169 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,169 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,169 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,169 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q5zxw3t0
2023-06-26 19:09:21,169 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da2e7028-5d36-4159-9864-afaa045f1d7a
2023-06-26 19:09:21,169 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e5c326d-d896-4135-8b33-8a9ea3b3242e
2023-06-26 19:09:21,184 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39115
2023-06-26 19:09:21,184 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39115
2023-06-26 19:09:21,184 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35141
2023-06-26 19:09:21,184 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,184 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,184 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,184 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,185 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mwp6ct4t
2023-06-26 19:09:21,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54669ca2-5d70-428f-8221-c39270bcfbf6
2023-06-26 19:09:21,196 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37681
2023-06-26 19:09:21,196 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37681
2023-06-26 19:09:21,196 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45859
2023-06-26 19:09:21,196 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,196 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,196 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,196 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,196 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ydbsd12f
2023-06-26 19:09:21,196 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37273
2023-06-26 19:09:21,196 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37273
2023-06-26 19:09:21,196 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44535
2023-06-26 19:09:21,196 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,196 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,196 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,196 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,196 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hm7pq548
2023-06-26 19:09:21,196 - distributed.worker - INFO - Starting Worker plugin PreImport-88a6cfd8-500b-4331-9967-ce76667b6795
2023-06-26 19:09:21,196 - distributed.worker - INFO - Starting Worker plugin RMMSetup-332bff6e-e846-4331-a2e7-0eff941213a8
2023-06-26 19:09:21,197 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fec3b2ca-b35f-4022-ad23-bedbcc2777b5
2023-06-26 19:09:21,200 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34121
2023-06-26 19:09:21,201 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34121
2023-06-26 19:09:21,201 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36691
2023-06-26 19:09:21,201 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,201 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,201 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,201 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,201 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xco6ohhu
2023-06-26 19:09:21,202 - distributed.worker - INFO - Starting Worker plugin PreImport-934f04ee-a3c1-4488-8067-fce198b4227c
2023-06-26 19:09:21,202 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ebe2aef-8c02-4693-ae7f-7550939c1213
2023-06-26 19:09:21,221 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45861
2023-06-26 19:09:21,221 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45861
2023-06-26 19:09:21,221 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40733
2023-06-26 19:09:21,221 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,221 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,221 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,221 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,221 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b3vj_ti9
2023-06-26 19:09:21,222 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce0028b7-c913-4226-a3ce-05110e1ac13b
2023-06-26 19:09:21,223 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44237
2023-06-26 19:09:21,223 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44237
2023-06-26 19:09:21,223 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43011
2023-06-26 19:09:21,223 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,223 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,223 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,223 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,223 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k594d921
2023-06-26 19:09:21,225 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7abf2605-1b20-4eba-81da-37c6f496f399
2023-06-26 19:09:21,226 - distributed.worker - INFO - Starting Worker plugin RMMSetup-203b5c77-c1db-41fc-b904-32ce7ed2aa48
2023-06-26 19:09:21,237 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42667
2023-06-26 19:09:21,238 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42667
2023-06-26 19:09:21,238 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39379
2023-06-26 19:09:21,238 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,238 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,238 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45497
2023-06-26 19:09:21,238 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45497
2023-06-26 19:09:21,238 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,238 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35323
2023-06-26 19:09:21,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rb__ol0f
2023-06-26 19:09:21,238 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,238 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,238 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ug83h2tc
2023-06-26 19:09:21,239 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a051b6c0-8350-44f6-8123-4eb15f3b499e
2023-06-26 19:09:21,240 - distributed.worker - INFO - Starting Worker plugin PreImport-c392d5cc-c7a0-4e07-ac7e-8dda5dc93bce
2023-06-26 19:09:21,240 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb5f96ae-c04a-4f0b-bfff-eb2d31ac0370
2023-06-26 19:09:21,246 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41883
2023-06-26 19:09:21,246 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41883
2023-06-26 19:09:21,246 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34701
2023-06-26 19:09:21,246 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,246 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,246 - distributed.worker - INFO -               Threads:                          1
2023-06-26 19:09:21,247 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 19:09:21,247 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cjb4c7it
2023-06-26 19:09:21,247 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9484e5ab-da32-41ce-91f3-a0d523be5005
2023-06-26 19:09:21,248 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2a1abaf-6ca6-472c-b70f-62bfe7178129
2023-06-26 19:09:21,615 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db1d9a0e-120b-4fd8-aea0-34df07afb986
2023-06-26 19:09:21,616 - distributed.worker - INFO - Starting Worker plugin PreImport-ff28e08f-b133-4d95-9f05-ec3c922a9d98
2023-06-26 19:09:21,617 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,634 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:21,634 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:21,640 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,615 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7210ebc3-0d92-43e3-933d-451871bc530c
2023-06-26 19:09:23,616 - distributed.worker - INFO - Starting Worker plugin PreImport-f6b61516-72cb-415a-810a-a6ff003691a7
2023-06-26 19:09:23,616 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,639 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,639 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,640 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,656 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3a358d1-6910-4f35-9f05-e30857afbd1f
2023-06-26 19:09:23,657 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,671 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,671 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,672 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d72c2c3-4232-40f9-853e-bbdfaaa5ef93
2023-06-26 19:09:23,688 - distributed.worker - INFO - Starting Worker plugin PreImport-0df0bf1b-c11c-4736-b422-76f9362664b6
2023-06-26 19:09:23,688 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,694 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af2c6d02-5765-4de3-b298-3e29b1b196fc
2023-06-26 19:09:23,695 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,701 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,701 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,703 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,716 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,716 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,718 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,734 - distributed.worker - INFO - Starting Worker plugin PreImport-bdc4e058-cb2f-404d-8c60-d625994b8c3f
2023-06-26 19:09:23,737 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,740 - distributed.worker - INFO - Starting Worker plugin PreImport-5a5b6286-587b-456d-aae2-31823a2340ad
2023-06-26 19:09:23,741 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,743 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bade569-9df4-4586-884c-6b9bd37ffec3
2023-06-26 19:09:23,745 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,756 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a2b8394-6054-4357-b856-afb9388c8159
2023-06-26 19:09:23,757 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,757 - distributed.worker - INFO - Starting Worker plugin PreImport-9a5bb22e-b7dc-4c0f-a252-8aed6a4413c9
2023-06-26 19:09:23,757 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,757 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,759 - distributed.worker - INFO - Starting Worker plugin PreImport-b6d379a7-8065-4680-a8a7-168b9952f15e
2023-06-26 19:09:23,759 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,760 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,762 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,762 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,763 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bafb040d-542e-406d-b299-4e7e6c6e060f
2023-06-26 19:09:23,764 - distributed.worker - INFO - Starting Worker plugin PreImport-3f75f817-3b0c-4ef7-aaa0-a83e7aacdd61
2023-06-26 19:09:23,764 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,765 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,770 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,770 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,771 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,771 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,772 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,773 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,774 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,774 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,775 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 19:09:23,775 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 19:09:23,776 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:23,776 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 19:09:33,157 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,209 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,287 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,304 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,378 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,387 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,469 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,472 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,509 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,524 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,552 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,560 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,678 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,699 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,751 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,835 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 19:09:33,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,846 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,847 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,847 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 19:09:33,855 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,855 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,855 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,855 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687806573.856158] [exp01:414918:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:36631) failed: Address already in use
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 19:09:33,856 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687806573.856757] [exp01:414991:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:59954) failed: Address already in use
2023-06-26 19:09:33,866 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:33,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 19:09:37,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:42,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,637 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:47,640 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 19:09:59,470 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,470 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,470 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,470 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:09:59,471 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37273. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32855. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44237. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42667. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41883. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45981. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43263. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37681. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39115. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45497. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45861. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34121. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35639. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37753. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33445. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,028 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41129. Reason: worker-handle-scheduler-connection-broken
2023-06-26 19:10:16,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37737'. Reason: nanny-close
2023-06-26 19:10:16,029 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,030 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44883'. Reason: nanny-close
2023-06-26 19:10:16,031 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,031 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34717'. Reason: nanny-close
2023-06-26 19:10:16,031 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44081'. Reason: nanny-close
2023-06-26 19:10:16,032 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36603'. Reason: nanny-close
2023-06-26 19:10:16,032 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46113'. Reason: nanny-close
2023-06-26 19:10:16,033 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,033 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41465'. Reason: nanny-close
2023-06-26 19:10:16,033 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,033 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34115'. Reason: nanny-close
2023-06-26 19:10:16,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43145'. Reason: nanny-close
2023-06-26 19:10:16,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42117'. Reason: nanny-close
2023-06-26 19:10:16,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36551'. Reason: nanny-close
2023-06-26 19:10:16,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43685'. Reason: nanny-close
2023-06-26 19:10:16,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43415'. Reason: nanny-close
2023-06-26 19:10:16,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44355'. Reason: nanny-close
2023-06-26 19:10:16,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34635'. Reason: nanny-close
2023-06-26 19:10:16,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,038 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37955'. Reason: nanny-close
2023-06-26 19:10:16,038 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 19:10:16,043 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44883 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47928 remote=tcp://10.120.104.11:44883>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44883 after 100 s
2023-06-26 19:10:16,044 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41465 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:43148 remote=tcp://10.120.104.11:41465>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41465 after 100 s
2023-06-26 19:10:16,047 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43415 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33748 remote=tcp://10.120.104.11:43415>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43415 after 100 s
2023-06-26 19:10:16,049 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36603 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:49002 remote=tcp://10.120.104.11:36603>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36603 after 100 s
2023-06-26 19:10:16,050 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37737 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60044 remote=tcp://10.120.104.11:37737>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37737 after 100 s
2023-06-26 19:10:16,051 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34115 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33286 remote=tcp://10.120.104.11:34115>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34115 after 100 s
2023-06-26 19:10:16,051 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46113 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:54920 remote=tcp://10.120.104.11:46113>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46113 after 100 s
2023-06-26 19:10:16,053 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43145 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46044 remote=tcp://10.120.104.11:43145>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43145 after 100 s
2023-06-26 19:10:16,053 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34717 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39834 remote=tcp://10.120.104.11:34717>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34717 after 100 s
2023-06-26 19:10:16,054 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37955 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:38960 remote=tcp://10.120.104.11:37955>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37955 after 100 s
2023-06-26 19:10:16,055 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44355 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:43976 remote=tcp://10.120.104.11:44355>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44355 after 100 s
2023-06-26 19:10:16,056 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42117 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48894 remote=tcp://10.120.104.11:42117>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42117 after 100 s
2023-06-26 19:10:16,058 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43685 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:49974 remote=tcp://10.120.104.11:43685>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43685 after 100 s
2023-06-26 19:10:16,058 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34635 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48326 remote=tcp://10.120.104.11:34635>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34635 after 100 s
2023-06-26 19:10:16,058 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36551 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50628 remote=tcp://10.120.104.11:36551>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36551 after 100 s
2023-06-26 19:10:16,061 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44081 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50436 remote=tcp://10.120.104.11:44081>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44081 after 100 s
2023-06-26 19:10:19,239 - distributed.nanny - WARNING - Worker process still alive after 3.199996795654297 seconds, killing
2023-06-26 19:10:19,239 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 19:10:19,240 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,240 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,240 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,241 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,241 - distributed.nanny - WARNING - Worker process still alive after 3.1999978637695317 seconds, killing
2023-06-26 19:10:19,242 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,243 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,243 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,244 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,244 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,245 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,245 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 19:10:19,246 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 19:10:19,247 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 19:10:20,030 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,032 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,032 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,033 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,033 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,033 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,034 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,034 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,034 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,035 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,036 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,036 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,037 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,037 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,037 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,039 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 19:10:20,040 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414991 parent=411677 started daemon>
2023-06-26 19:10:20,040 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414988 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414985 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414982 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414979 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414976 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414973 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414966 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414963 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414960 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414957 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414952 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414922 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414918 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414915 parent=411677 started daemon>
2023-06-26 19:10:20,041 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=414875 parent=411677 started daemon>
2023-06-26 19:10:25,658 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 414982 exit status was already read will report exitcode 255
2023-06-26 19:10:25,907 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 414915 exit status was already read will report exitcode 255
2023-06-26 19:10:26,916 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 414922 exit status was already read will report exitcode 255
