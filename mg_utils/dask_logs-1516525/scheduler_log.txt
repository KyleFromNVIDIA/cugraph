RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 23:28:39,424 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 23:28:39,952 - distributed.scheduler - INFO - State start
2023-06-22 23:28:39,964 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 23:28:39,965 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 23:28:39,966 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 23:28:51,047 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:34961', status: init, memory: 0, processing: 0>
2023-06-22 23:28:51,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:34961
2023-06-22 23:28:51,050 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43214
2023-06-22 23:28:51,051 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:37865', status: init, memory: 0, processing: 0>
2023-06-22 23:28:51,052 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:37865
2023-06-22 23:28:51,052 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43230
2023-06-22 23:28:51,052 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:40983', status: init, memory: 0, processing: 0>
2023-06-22 23:28:51,052 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:40983
2023-06-22 23:28:51,052 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43222
2023-06-22 23:28:51,053 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:41881', status: init, memory: 0, processing: 0>
2023-06-22 23:28:51,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:41881
2023-06-22 23:28:51,053 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43220
2023-06-22 23:28:51,054 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:45503', status: init, memory: 0, processing: 0>
2023-06-22 23:28:51,054 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:45503
2023-06-22 23:28:51,054 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43236
2023-06-22 23:28:51,055 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:36455', status: init, memory: 0, processing: 0>
2023-06-22 23:28:51,055 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:36455
2023-06-22 23:28:51,056 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43248
2023-06-22 23:28:51,097 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:46157', status: init, memory: 0, processing: 0>
2023-06-22 23:28:51,098 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:46157
2023-06-22 23:28:51,098 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43264
2023-06-22 23:28:51,098 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:35971', status: init, memory: 0, processing: 0>
2023-06-22 23:28:51,099 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:35971
2023-06-22 23:28:51,099 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43260
2023-06-22 23:28:53,149 - distributed.scheduler - INFO - Receive client connection: Client-8cc07a9a-1154-11ee-a45f-d8c49778ced7
2023-06-22 23:28:53,150 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43370
2023-06-22 23:28:53,228 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 23:29:43,851 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:29:46,056 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 23:29:49,814 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:29:56,506 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-22 23:29:56,508 - distributed.scheduler - INFO - Remove client Client-8cc07a9a-1154-11ee-a45f-d8c49778ced7
2023-06-22 23:29:56,515 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:43370; closing.
2023-06-22 23:29:56,515 - distributed.scheduler - INFO - Remove client Client-8cc07a9a-1154-11ee-a45f-d8c49778ced7
2023-06-22 23:29:56,516 - distributed.scheduler - INFO - Close client connection: Client-8cc07a9a-1154-11ee-a45f-d8c49778ced7
2023-06-22 23:36:35,930 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 23:36:35,931 - distributed.core - INFO - Connection to tcp://10.33.227.169:43264 has been closed.
2023-06-22 23:36:35,932 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:46157', status: running, memory: 0, processing: 0>
2023-06-22 23:36:35,933 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:46157
2023-06-22 23:36:35,934 - distributed.core - INFO - Connection to tcp://10.33.227.169:43230 has been closed.
2023-06-22 23:36:35,934 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:37865', status: running, memory: 0, processing: 0>
2023-06-22 23:36:35,934 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:37865
2023-06-22 23:36:35,935 - distributed.core - INFO - Connection to tcp://10.33.227.169:43222 has been closed.
2023-06-22 23:36:35,935 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:40983', status: running, memory: 0, processing: 0>
2023-06-22 23:36:35,935 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:40983
2023-06-22 23:36:35,935 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 23:36:35,937 - distributed.core - INFO - Connection to tcp://10.33.227.169:43220 has been closed.
2023-06-22 23:36:35,937 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:41881', status: running, memory: 0, processing: 0>
2023-06-22 23:36:35,937 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:41881
2023-06-22 23:36:35,939 - distributed.core - INFO - Connection to tcp://10.33.227.169:43248 has been closed.
2023-06-22 23:36:35,939 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:36455', status: running, memory: 0, processing: 0>
2023-06-22 23:36:35,939 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:36455
2023-06-22 23:36:35,939 - distributed.core - INFO - Connection to tcp://10.33.227.169:43214 has been closed.
2023-06-22 23:36:35,939 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:34961', status: running, memory: 0, processing: 0>
2023-06-22 23:36:35,939 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:34961
2023-06-22 23:36:35,940 - distributed.core - INFO - Connection to tcp://10.33.227.169:43236 has been closed.
2023-06-22 23:36:35,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:45503', status: running, memory: 0, processing: 0>
2023-06-22 23:36:35,940 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:45503
2023-06-22 23:36:35,940 - distributed.core - INFO - Connection to tcp://10.33.227.169:43260 has been closed.
2023-06-22 23:36:35,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:35971', status: running, memory: 0, processing: 0>
2023-06-22 23:36:35,941 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:35971
2023-06-22 23:36:35,941 - distributed.scheduler - INFO - Lost all workers
2023-06-22 23:36:35,941 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43214>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43214>: Stream is closed
2023-06-22 23:36:35,942 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43260>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43260>: Stream is closed
2023-06-22 23:36:35,943 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43248>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43248>: Stream is closed
2023-06-22 23:36:35,943 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43220>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 23:36:35,943 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43236>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:43236>: Stream is closed
2023-06-22 23:36:35,945 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 23:36:35,947 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 23:36:35,948 - distributed.scheduler - INFO - End scheduler
