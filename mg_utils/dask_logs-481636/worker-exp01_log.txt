RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 20:20:28,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33131'
2023-06-26 20:20:28,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33427'
2023-06-26 20:20:28,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37519'
2023-06-26 20:20:28,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37575'
2023-06-26 20:20:28,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37735'
2023-06-26 20:20:28,633 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45791'
2023-06-26 20:20:28,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44107'
2023-06-26 20:20:28,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45575'
2023-06-26 20:20:28,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38853'
2023-06-26 20:20:28,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42787'
2023-06-26 20:20:28,644 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35701'
2023-06-26 20:20:28,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40671'
2023-06-26 20:20:28,648 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45615'
2023-06-26 20:20:28,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37109'
2023-06-26 20:20:28,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39729'
2023-06-26 20:20:28,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40445'
2023-06-26 20:20:30,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,355 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,355 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,359 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,359 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:20:30,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:20:30,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,445 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,463 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,484 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,502 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,535 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,552 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,552 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,561 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:30,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:20:37,367 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40231
2023-06-26 20:20:37,367 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40231
2023-06-26 20:20:37,367 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42967
2023-06-26 20:20:37,367 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,367 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,367 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,367 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,367 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rn38m3r7
2023-06-26 20:20:37,368 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e7751de-d8b6-4680-b741-5924a01d799d
2023-06-26 20:20:37,394 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33317
2023-06-26 20:20:37,394 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33317
2023-06-26 20:20:37,394 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43479
2023-06-26 20:20:37,395 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,395 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,395 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,395 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,395 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dww8p5zk
2023-06-26 20:20:37,395 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7fb50ec2-80b6-4408-a7b2-82305d5bd68c
2023-06-26 20:20:37,411 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34265
2023-06-26 20:20:37,411 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34265
2023-06-26 20:20:37,411 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41377
2023-06-26 20:20:37,411 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,411 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,411 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,411 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-af3raj90
2023-06-26 20:20:37,412 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fca0e8e-dafe-4878-9277-98a973b49f5d
2023-06-26 20:20:37,413 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38399
2023-06-26 20:20:37,413 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38399
2023-06-26 20:20:37,413 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46791
2023-06-26 20:20:37,413 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,413 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,413 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tup9xs_v
2023-06-26 20:20:37,414 - distributed.worker - INFO - Starting Worker plugin PreImport-c7ca852d-88c4-4dbd-80d9-a1c407c0b45a
2023-06-26 20:20:37,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5c3b6b7-922c-4079-8e1a-f63d80ea492e
2023-06-26 20:20:37,451 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45819
2023-06-26 20:20:37,452 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45819
2023-06-26 20:20:37,452 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46135
2023-06-26 20:20:37,452 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,452 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,452 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,452 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,452 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-duzwbn1q
2023-06-26 20:20:37,452 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d72aa90-0f95-48cc-bbe6-84f180101736
2023-06-26 20:20:37,478 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46723
2023-06-26 20:20:37,479 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46723
2023-06-26 20:20:37,479 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39943
2023-06-26 20:20:37,479 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,479 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,479 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,479 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,479 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9h7ywaoa
2023-06-26 20:20:37,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-816f782f-2017-4398-8388-58d9cf93ca6e
2023-06-26 20:20:37,480 - distributed.worker - INFO - Starting Worker plugin RMMSetup-024d1eac-8b93-4b22-88e0-111b1de3e73b
2023-06-26 20:20:37,529 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43623
2023-06-26 20:20:37,529 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43623
2023-06-26 20:20:37,529 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35363
2023-06-26 20:20:37,529 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,529 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,529 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,529 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,529 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hqbws4vh
2023-06-26 20:20:37,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1dbbae98-6339-4bd3-b18d-9a1040a5fef9
2023-06-26 20:20:37,530 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1195c0e9-dee2-4082-bac7-5d43db6e0335
2023-06-26 20:20:37,539 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43051
2023-06-26 20:20:37,539 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43051
2023-06-26 20:20:37,539 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36681
2023-06-26 20:20:37,539 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,539 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,539 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,539 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ypgqy74
2023-06-26 20:20:37,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ada278ce-3ff4-4ffe-b24f-0d4a8c1689f3
2023-06-26 20:20:37,564 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36069
2023-06-26 20:20:37,564 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36069
2023-06-26 20:20:37,564 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35941
2023-06-26 20:20:37,564 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,564 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,564 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,564 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,564 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-88h_64h8
2023-06-26 20:20:37,565 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be3c633c-5ffd-458c-92b7-df379fb76f96
2023-06-26 20:20:37,577 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41435
2023-06-26 20:20:37,577 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41435
2023-06-26 20:20:37,578 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37065
2023-06-26 20:20:37,578 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,578 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,578 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,578 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,578 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ibrsnd80
2023-06-26 20:20:37,579 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b244d340-48ec-4ec5-a2cd-03c5750f7e5b
2023-06-26 20:20:37,589 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39687
2023-06-26 20:20:37,589 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39687
2023-06-26 20:20:37,589 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35829
2023-06-26 20:20:37,589 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,589 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,589 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,589 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,589 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_okz6tiw
2023-06-26 20:20:37,590 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4d4fcf5-2a5f-40b2-b754-90d764ba0747
2023-06-26 20:20:37,590 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44009
2023-06-26 20:20:37,591 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44009
2023-06-26 20:20:37,591 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41893
2023-06-26 20:20:37,591 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,591 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,591 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,591 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4u6li9xj
2023-06-26 20:20:37,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1516ee26-85ea-424a-9b8f-34bed7fff89a
2023-06-26 20:20:37,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c9651854-7094-4df2-9bbe-c6276f20af8a
2023-06-26 20:20:37,693 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44575
2023-06-26 20:20:37,693 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44575
2023-06-26 20:20:37,693 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35939
2023-06-26 20:20:37,693 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,693 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,693 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,693 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1wbpw5pw
2023-06-26 20:20:37,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bab4857b-4952-4f3b-80f9-89a2fc065b9a
2023-06-26 20:20:37,707 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44011
2023-06-26 20:20:37,707 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44011
2023-06-26 20:20:37,707 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34971
2023-06-26 20:20:37,707 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,707 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,707 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,707 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gdmha5qa
2023-06-26 20:20:37,708 - distributed.worker - INFO - Starting Worker plugin PreImport-bd48b2d3-9331-44fd-97fe-66bfc85ef203
2023-06-26 20:20:37,708 - distributed.worker - INFO - Starting Worker plugin RMMSetup-104139ad-100e-46d4-b5a8-65a925fc5e9f
2023-06-26 20:20:37,708 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39783
2023-06-26 20:20:37,708 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39783
2023-06-26 20:20:37,708 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39335
2023-06-26 20:20:37,708 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,708 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,708 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,709 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qfmq3gic
2023-06-26 20:20:37,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa232e79-1ead-4463-918e-da4600eb9e36
2023-06-26 20:20:37,712 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43187
2023-06-26 20:20:37,712 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43187
2023-06-26 20:20:37,713 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35447
2023-06-26 20:20:37,713 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:20:37,713 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:37,713 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:20:37,713 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:20:37,713 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-52piktfd
2023-06-26 20:20:37,714 - distributed.worker - INFO - Starting Worker plugin PreImport-b6f2cb44-e128-46ff-b40d-52515d5ff0a2
2023-06-26 20:20:37,714 - distributed.worker - INFO - Starting Worker plugin RMMSetup-777b242e-c0a2-439a-8a3b-949ce4808595
2023-06-26 20:20:41,127 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-331f0e1d-8a3d-4291-808f-4c0e89e1b72d
2023-06-26 20:20:41,127 - distributed.worker - INFO - Starting Worker plugin PreImport-1115c6ac-0f59-4b25-a607-84bb2e1f543b
2023-06-26 20:20:41,128 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,148 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,148 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,150 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,158 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5588ecca-4d0d-492f-a572-7d4caae60d6f
2023-06-26 20:20:41,160 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,188 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,191 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,203 - distributed.worker - INFO - Starting Worker plugin PreImport-eb0d1879-953b-4659-b37c-7ce9c6fae39b
2023-06-26 20:20:41,205 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1bee98d4-b4b7-4706-9fdf-33466f4cc9ea
2023-06-26 20:20:41,205 - distributed.worker - INFO - Starting Worker plugin PreImport-a2233c92-df1e-4d5e-8402-c2de213a5998
2023-06-26 20:20:41,207 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,229 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,229 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,231 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,246 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,249 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-931b4c59-65c3-4e46-8424-a0b685227f89
2023-06-26 20:20:41,292 - distributed.worker - INFO - Starting Worker plugin PreImport-4fa4b16b-3242-4f24-af6f-758f36fe371e
2023-06-26 20:20:41,293 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,309 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,309 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f89977f-9235-49dd-afc3-2efb494518fe
2023-06-26 20:20:41,310 - distributed.worker - INFO - Starting Worker plugin PreImport-c9bbc052-cae0-47ee-8115-bff0140cb700
2023-06-26 20:20:41,310 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,311 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,341 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,342 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,344 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,382 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f45bc4ae-63dc-4191-ad67-7ea071e69c45
2023-06-26 20:20:41,382 - distributed.worker - INFO - Starting Worker plugin PreImport-f9148b20-d4a0-4bc4-9397-a987e740966c
2023-06-26 20:20:41,383 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,395 - distributed.worker - INFO - Starting Worker plugin PreImport-dc5bf64a-4d2f-4383-9add-25ad9d83fdab
2023-06-26 20:20:41,397 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,400 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,400 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,401 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,408 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6dd38d6-840f-4a11-b86c-ab630d009f04
2023-06-26 20:20:41,408 - distributed.worker - INFO - Starting Worker plugin PreImport-eb67271e-1c8f-4117-b358-6395317fcf32
2023-06-26 20:20:41,409 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,416 - distributed.worker - INFO - Starting Worker plugin PreImport-9fc20610-a5d4-4ffb-9e6e-8a0ca89b9499
2023-06-26 20:20:41,417 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,417 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-71e7c795-7bfb-4918-a730-cb58b8421db9
2023-06-26 20:20:41,417 - distributed.worker - INFO - Starting Worker plugin PreImport-73d73f84-ca00-4dc1-bae4-24e19de2db70
2023-06-26 20:20:41,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,418 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,418 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,419 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b78ef387-cbbb-4916-914e-15e6293ae9c5
2023-06-26 20:20:41,420 - distributed.worker - INFO - Starting Worker plugin PreImport-afd0cb5b-ed68-4f35-a57b-fd4a350670cc
2023-06-26 20:20:41,421 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,423 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,423 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,424 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,431 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,431 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,432 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0eb37a96-04b0-4bd4-a87c-25074cfa6d0f
2023-06-26 20:20:41,432 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,438 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,438 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,440 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,439 - distributed.worker - INFO - Starting Worker plugin PreImport-487d41ac-2d78-41c3-988c-00bbc093e3e2
2023-06-26 20:20:41,440 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,440 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,443 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,443 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,453 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bc4667c-cf4b-474f-9659-7acf45809d56
2023-06-26 20:20:41,454 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,462 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ca85fca-51cb-49fc-be87-deeedcab38e7
2023-06-26 20:20:41,463 - distributed.worker - INFO - Starting Worker plugin PreImport-8898cf58-9ec3-4f5a-8273-ef972c3a5b47
2023-06-26 20:20:41,463 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,465 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,465 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,466 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1e8e69e-e4fd-401d-9198-cc125c96530f
2023-06-26 20:20:41,467 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,467 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,468 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,468 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,468 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,477 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,477 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,479 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:20:41,484 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:20:41,484 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:20:41,485 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44575. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44011. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39783. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38399. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40231. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34265. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43051. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45819. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43187. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41435. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33317. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44009. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36069. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43623. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39687. Reason: worker-close
2023-06-26 20:21:30,510 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46723. Reason: worker-close
2023-06-26 20:21:30,511 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37519'. Reason: nanny-close
2023-06-26 20:21:30,511 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46654 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,513 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,511 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46650 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,511 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46632 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,512 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46656 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,511 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46516 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,512 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46512 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45575'. Reason: nanny-close
2023-06-26 20:21:30,512 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46550 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,513 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46588 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,513 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46528 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,513 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:46626 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:21:30,516 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,516 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38853'. Reason: nanny-close
2023-06-26 20:21:30,516 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42787'. Reason: nanny-close
2023-06-26 20:21:30,517 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33131'. Reason: nanny-close
2023-06-26 20:21:30,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33427'. Reason: nanny-close
2023-06-26 20:21:30,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37575'. Reason: nanny-close
2023-06-26 20:21:30,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37735'. Reason: nanny-close
2023-06-26 20:21:30,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45791'. Reason: nanny-close
2023-06-26 20:21:30,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44107'. Reason: nanny-close
2023-06-26 20:21:30,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35701'. Reason: nanny-close
2023-06-26 20:21:30,521 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40671'. Reason: nanny-close
2023-06-26 20:21:30,521 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45615'. Reason: nanny-close
2023-06-26 20:21:30,521 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,522 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37109'. Reason: nanny-close
2023-06-26 20:21:30,522 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,522 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39729'. Reason: nanny-close
2023-06-26 20:21:30,522 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,523 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40445'. Reason: nanny-close
2023-06-26 20:21:30,523 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:21:30,881 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40445 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45840 remote=tcp://10.120.104.11:40445>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40445 after 100 s
2023-06-26 20:21:30,885 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37575 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37986 remote=tcp://10.120.104.11:37575>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37575 after 100 s
2023-06-26 20:21:30,886 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33427 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34526 remote=tcp://10.120.104.11:33427>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33427 after 100 s
2023-06-26 20:21:30,893 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37109 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33836 remote=tcp://10.120.104.11:37109>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37109 after 100 s
2023-06-26 20:21:30,895 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45615 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52152 remote=tcp://10.120.104.11:45615>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45615 after 100 s
2023-06-26 20:21:30,896 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38853 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47818 remote=tcp://10.120.104.11:38853>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38853 after 100 s
2023-06-26 20:21:30,896 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45791 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34290 remote=tcp://10.120.104.11:45791>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45791 after 100 s
2023-06-26 20:21:30,897 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44107 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:44616 remote=tcp://10.120.104.11:44107>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44107 after 100 s
2023-06-26 20:21:30,898 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39729 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:44982 remote=tcp://10.120.104.11:39729>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39729 after 100 s
2023-06-26 20:21:30,899 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35701 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33330 remote=tcp://10.120.104.11:35701>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35701 after 100 s
2023-06-26 20:21:30,899 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40671 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56866 remote=tcp://10.120.104.11:40671>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40671 after 100 s
2023-06-26 20:21:30,900 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42787 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:51936 remote=tcp://10.120.104.11:42787>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42787 after 100 s
2023-06-26 20:21:30,907 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45575 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50960 remote=tcp://10.120.104.11:45575>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45575 after 100 s
2023-06-26 20:21:30,916 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33131 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36918 remote=tcp://10.120.104.11:33131>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33131 after 100 s
2023-06-26 20:21:30,918 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37735 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:49160 remote=tcp://10.120.104.11:37735>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37735 after 100 s
2023-06-26 20:21:33,725 - distributed.nanny - WARNING - Worker process still alive after 3.1999899291992193 seconds, killing
2023-06-26 20:21:33,725 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-26 20:21:33,725 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,726 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,727 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 20:21:33,727 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,727 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,728 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,728 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,728 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,728 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,729 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 20:21:33,729 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,729 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,729 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:33,729 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:21:34,514 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,693 - distributed.nanny - INFO - Worker process 481831 was killed by signal 9
2023-06-26 20:21:34,704 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,704 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,704 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,704 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,705 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,705 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,705 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,705 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,705 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,706 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,706 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,706 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,706 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,706 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481845 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481842 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481839 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481837 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481834 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481827 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481824 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481821 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481818 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481815 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481812 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481809 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481806 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481803 parent=481769 started daemon>
2023-06-26 20:21:34,709 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=481800 parent=481769 started daemon>
2023-06-26 20:21:36,398 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 481839 exit status was already read will report exitcode 255
2023-06-26 20:21:37,837 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 481818 exit status was already read will report exitcode 255
2023-06-26 20:21:38,724 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 481824 exit status was already read will report exitcode 255
