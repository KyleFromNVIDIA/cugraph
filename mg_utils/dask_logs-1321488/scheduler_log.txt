RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 19:00:19,534 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 19:00:19,995 - distributed.scheduler - INFO - State start
2023-06-22 19:00:19,996 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dr8l_5eq', purging
2023-06-22 19:00:19,996 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fx8aa58o', purging
2023-06-22 19:00:19,996 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4l2a9nvu', purging
2023-06-22 19:00:19,997 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pokeo_iq', purging
2023-06-22 19:00:19,997 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cs0mlhhd', purging
2023-06-22 19:00:19,997 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ohitemiq', purging
2023-06-22 19:00:19,997 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-y1d8h609', purging
2023-06-22 19:00:19,997 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7cfne_ge', purging
2023-06-22 19:00:20,006 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 19:00:20,007 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 19:00:20,007 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 19:00:30,371 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:36119', status: init, memory: 0, processing: 0>
2023-06-22 19:00:30,660 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:36119
2023-06-22 19:00:30,660 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48592
2023-06-22 19:00:30,837 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:33997', status: init, memory: 0, processing: 0>
2023-06-22 19:00:30,838 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:33997
2023-06-22 19:00:30,838 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48600
2023-06-22 19:00:30,929 - distributed.scheduler - INFO - Receive client connection: Client-0f149e86-112f-11ee-aa66-d8c49778ced7
2023-06-22 19:00:30,930 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48614
2023-06-22 19:00:31,014 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 19:00:31,152 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:45341', status: init, memory: 0, processing: 0>
2023-06-22 19:00:31,152 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:45341
2023-06-22 19:00:31,152 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48628
2023-06-22 19:00:31,153 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:42669', status: init, memory: 0, processing: 0>
2023-06-22 19:00:31,153 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:42669
2023-06-22 19:00:31,153 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48652
2023-06-22 19:00:31,153 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:44143', status: init, memory: 0, processing: 0>
2023-06-22 19:00:31,154 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:44143
2023-06-22 19:00:31,154 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48636
2023-06-22 19:00:31,154 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:45927', status: init, memory: 0, processing: 0>
2023-06-22 19:00:31,155 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:45927
2023-06-22 19:00:31,155 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48662
2023-06-22 19:00:31,156 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:33089', status: init, memory: 0, processing: 0>
2023-06-22 19:00:31,157 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:33089
2023-06-22 19:00:31,157 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48674
2023-06-22 19:00:31,158 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:42255', status: init, memory: 0, processing: 0>
2023-06-22 19:00:31,158 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:42255
2023-06-22 19:00:31,158 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48684
2023-06-22 19:00:46,380 - distributed.scheduler - INFO - Remove client Client-0f149e86-112f-11ee-aa66-d8c49778ced7
2023-06-22 19:00:46,381 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:48614; closing.
2023-06-22 19:00:46,381 - distributed.scheduler - INFO - Remove client Client-0f149e86-112f-11ee-aa66-d8c49778ced7
2023-06-22 19:00:46,382 - distributed.scheduler - INFO - Close client connection: Client-0f149e86-112f-11ee-aa66-d8c49778ced7
2023-06-22 19:01:06,084 - distributed.scheduler - INFO - Receive client connection: Client-2408e474-112f-11ee-ae3a-d8c49778ced7
2023-06-22 19:01:06,084 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37490
2023-06-22 19:01:06,095 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 19:01:49,002 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 19:01:51,220 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 19:01:55,072 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 19:01:58,773 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-22 19:01:58,775 - distributed.scheduler - INFO - Remove client Client-2408e474-112f-11ee-ae3a-d8c49778ced7
2023-06-22 19:01:58,779 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:37490; closing.
2023-06-22 19:01:58,780 - distributed.scheduler - INFO - Remove client Client-2408e474-112f-11ee-ae3a-d8c49778ced7
2023-06-22 19:01:58,781 - distributed.scheduler - INFO - Close client connection: Client-2408e474-112f-11ee-ae3a-d8c49778ced7
2023-06-22 19:11:51,710 - distributed.scheduler - INFO - Receive client connection: Client-a4db49a8-1130-11ee-8699-d8c49778ced7
2023-06-22 19:11:51,711 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:48308
2023-06-22 19:11:51,720 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 19:12:13,340 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 19:12:13,759 - distributed.scheduler - INFO - Receive client connection: Client-worker-b2009392-1130-11ee-aaf8-d8c49778ced7
2023-06-22 19:12:13,760 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45458
2023-06-22 19:12:13,783 - distributed.scheduler - INFO - Receive client connection: Client-worker-b203ccb3-1130-11ee-aaec-d8c49778ced7
2023-06-22 19:12:13,783 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45482
2023-06-22 19:12:13,785 - distributed.scheduler - INFO - Receive client connection: Client-worker-b203d8f0-1130-11ee-aaff-d8c49778ced7
2023-06-22 19:12:13,785 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45498
2023-06-22 19:12:13,786 - distributed.scheduler - INFO - Receive client connection: Client-worker-b203d673-1130-11ee-aaf2-d8c49778ced7
2023-06-22 19:12:13,786 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45500
2023-06-22 19:12:13,786 - distributed.scheduler - INFO - Receive client connection: Client-worker-b203dab2-1130-11ee-ab01-d8c49778ced7
2023-06-22 19:12:13,787 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45512
2023-06-22 19:12:13,787 - distributed.scheduler - INFO - Receive client connection: Client-worker-b203d9ab-1130-11ee-aaf5-d8c49778ced7
2023-06-22 19:12:13,787 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45526
2023-06-22 19:12:13,788 - distributed.scheduler - INFO - Receive client connection: Client-worker-b203c58a-1130-11ee-aafb-d8c49778ced7
2023-06-22 19:12:13,788 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45472
2023-06-22 19:12:13,789 - distributed.scheduler - INFO - Receive client connection: Client-worker-b203e071-1130-11ee-aaef-d8c49778ced7
2023-06-22 19:12:13,789 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:45542
2023-06-22 19:15:33,832 - distributed.scheduler - ERROR - Couldn't gather keys {'Series-3b1793f4-72a9-4a3e-9593-36afb53f4ba2': ['tcp://10.33.227.169:42669']} state: ['memory'] workers: ['tcp://10.33.227.169:42669']
NoneType: None
2023-06-22 19:15:33,832 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:42669', status: running, memory: 3, processing: 1>
2023-06-22 19:15:33,833 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:42669
2023-06-22 19:15:33,837 - distributed.scheduler - ERROR - Shut down workers that don't have promised key: ['tcp://10.33.227.169:42669'], Series-3b1793f4-72a9-4a3e-9593-36afb53f4ba2
NoneType: None
2023-06-22 19:15:33,839 - distributed.scheduler - ERROR - Couldn't gather keys {'Series-3b1793f4-72a9-4a3e-9593-36afb53f4ba2': ['tcp://10.33.227.169:42669']} state: ['processing'] workers: ['tcp://10.33.227.169:42669']
NoneType: None
2023-06-22 19:15:33,840 - distributed.scheduler - ERROR - Couldn't gather keys {'Series-3b1793f4-72a9-4a3e-9593-36afb53f4ba2': ['tcp://10.33.227.169:42669']} state: ['processing'] workers: ['tcp://10.33.227.169:42669']
NoneType: None
2023-06-22 19:15:33,840 - distributed.scheduler - ERROR - Shut down workers that don't have promised key: ['tcp://10.33.227.169:42669'], Series-3b1793f4-72a9-4a3e-9593-36afb53f4ba2
NoneType: None
2023-06-22 19:15:33,840 - distributed.scheduler - ERROR - Shut down workers that don't have promised key: ['tcp://10.33.227.169:42669'], Series-3b1793f4-72a9-4a3e-9593-36afb53f4ba2
NoneType: None
2023-06-22 19:20:20,010 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://10.33.227.169:33089', status: running, memory: 2, processing: 1>
2023-06-22 19:20:20,011 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:33089', status: running, memory: 2, processing: 1>
2023-06-22 19:20:20,011 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:33089
2023-06-22 19:20:20,012 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://10.33.227.169:36119', status: running, memory: 2, processing: 1>
2023-06-22 19:20:20,012 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:36119', status: running, memory: 2, processing: 1>
2023-06-22 19:20:20,012 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:36119
2023-06-22 19:20:20,013 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://10.33.227.169:44143', status: running, memory: 2, processing: 1>
2023-06-22 19:20:20,013 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:44143', status: running, memory: 2, processing: 1>
2023-06-22 19:20:20,013 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:44143
2023-06-22 19:25:20,009 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://10.33.227.169:42255', status: running, memory: 3, processing: 1>
2023-06-22 19:25:20,010 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:42255', status: running, memory: 3, processing: 1>
2023-06-22 19:25:20,010 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:42255
2023-06-22 19:25:56,583 - distributed.scheduler - INFO - Remove client Client-a4db49a8-1130-11ee-8699-d8c49778ced7
2023-06-22 19:25:56,588 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:48308; closing.
2023-06-22 19:25:56,591 - distributed.scheduler - INFO - Remove client Client-a4db49a8-1130-11ee-8699-d8c49778ced7
2023-06-22 19:25:56,592 - distributed.scheduler - INFO - Close client connection: Client-a4db49a8-1130-11ee-8699-d8c49778ced7
2023-06-22 19:36:38,847 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 19:36:38,848 - distributed.core - INFO - Connection to tcp://10.33.227.169:48600 has been closed.
2023-06-22 19:36:38,848 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:33997', status: running, memory: 1, processing: 0>
2023-06-22 19:36:38,849 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:33997
2023-06-22 19:36:38,850 - distributed.core - INFO - Connection to tcp://10.33.227.169:48662 has been closed.
2023-06-22 19:36:38,850 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:45927', status: running, memory: 0, processing: 1>
2023-06-22 19:36:38,850 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:45927
2023-06-22 19:36:38,850 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 19:36:38,854 - distributed.core - INFO - Connection to tcp://10.33.227.169:45542 has been closed.
2023-06-22 19:36:38,854 - distributed.core - INFO - Connection to tcp://10.33.227.169:45472 has been closed.
2023-06-22 19:36:38,854 - distributed.core - INFO - Connection to tcp://10.33.227.169:48628 has been closed.
2023-06-22 19:36:38,854 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:45341', status: running, memory: 0, processing: 1>
2023-06-22 19:36:38,854 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:45341
2023-06-22 19:36:38,854 - distributed.scheduler - INFO - Lost all workers
2023-06-22 19:36:38,855 - distributed.core - INFO - Connection to tcp://10.33.227.169:45512 has been closed.
2023-06-22 19:36:38,855 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:45472>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:45472>: Stream is closed
2023-06-22 19:36:38,857 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:45542>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:45542>: Stream is closed
2023-06-22 19:36:38,857 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:45512>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:45512>: Stream is closed
2023-06-22 19:36:38,857 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:48628>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:48628>: Stream is closed
2023-06-22 19:36:38,858 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 19:36:38,862 - distributed.core - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 4303, in add_worker
    await self.handle_worker(comm, address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 5669, in handle_worker
    await self.handle_stream(comm=comm, extra={"worker": worker})
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 977, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError
2023-06-22 19:36:38,863 - distributed.core - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 4303, in add_worker
    await self.handle_worker(comm, address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 5669, in handle_worker
    await self.handle_stream(comm=comm, extra={"worker": worker})
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 977, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError
2023-06-22 19:36:38,864 - distributed.core - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 4303, in add_worker
    await self.handle_worker(comm, address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 5669, in handle_worker
    await self.handle_stream(comm=comm, extra={"worker": worker})
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 977, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError
2023-06-22 19:36:38,865 - distributed.core - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 4303, in add_worker
    await self.handle_worker(comm, address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 5669, in handle_worker
    await self.handle_stream(comm=comm, extra={"worker": worker})
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 977, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError
2023-06-22 19:36:38,867 - distributed.core - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 4303, in add_worker
    await self.handle_worker(comm, address)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 5669, in handle_worker
    await self.handle_stream(comm=comm, extra={"worker": worker})
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 977, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError
2023-06-22 19:36:38,868 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 19:36:38,869 - distributed.scheduler - INFO - End scheduler
