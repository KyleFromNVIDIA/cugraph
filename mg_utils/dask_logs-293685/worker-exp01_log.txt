RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 16:59:51,783 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43561'
2023-06-26 16:59:51,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45575'
2023-06-26 16:59:51,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36629'
2023-06-26 16:59:51,790 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38585'
2023-06-26 16:59:51,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44879'
2023-06-26 16:59:51,794 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35611'
2023-06-26 16:59:51,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44105'
2023-06-26 16:59:51,798 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45571'
2023-06-26 16:59:51,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46373'
2023-06-26 16:59:51,802 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43545'
2023-06-26 16:59:51,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43531'
2023-06-26 16:59:51,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37653'
2023-06-26 16:59:51,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35751'
2023-06-26 16:59:51,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38689'
2023-06-26 16:59:51,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35477'
2023-06-26 16:59:51,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37969'
2023-06-26 16:59:53,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,441 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,441 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,445 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,451 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,501 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,501 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,503 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,510 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,518 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,518 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,534 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,534 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,534 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:59:53,534 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:59:53,555 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,620 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,622 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,622 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,679 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,688 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,695 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,695 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,695 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:53,715 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:59:59,994 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34645
2023-06-26 16:59:59,995 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34645
2023-06-26 16:59:59,995 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46675
2023-06-26 16:59:59,995 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:59:59,995 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:59:59,995 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:59:59,995 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:59:59,995 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xsfv441x
2023-06-26 16:59:59,995 - distributed.worker - INFO - Starting Worker plugin PreImport-e9fc691b-fb34-46c1-962e-b563dcea6194
2023-06-26 16:59:59,996 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9cbd776-d7fc-4c6b-a1f2-d7b2b4c12141
2023-06-26 17:00:00,003 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45245
2023-06-26 17:00:00,003 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45245
2023-06-26 17:00:00,003 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43799
2023-06-26 17:00:00,003 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,003 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,003 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,003 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,003 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q3jmlm2p
2023-06-26 17:00:00,004 - distributed.worker - INFO - Starting Worker plugin PreImport-3df53ff0-f881-4fd8-b044-0b1dc4706b43
2023-06-26 17:00:00,004 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af3f4373-96ed-45a1-a5f9-d002260c6921
2023-06-26 17:00:00,085 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44501
2023-06-26 17:00:00,085 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44501
2023-06-26 17:00:00,085 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46249
2023-06-26 17:00:00,085 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,085 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,085 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,086 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,086 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-10nq6pw_
2023-06-26 17:00:00,086 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5654dc15-3ee0-418e-a90d-248136b11426
2023-06-26 17:00:00,114 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39871
2023-06-26 17:00:00,115 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39871
2023-06-26 17:00:00,115 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44367
2023-06-26 17:00:00,115 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,115 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,115 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,115 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,115 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tp5gp58x
2023-06-26 17:00:00,115 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0071795-d9d1-4c7b-8032-1a21b42d7d14
2023-06-26 17:00:00,115 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7740797-595c-48a8-b831-4e0ad682ddee
2023-06-26 17:00:00,128 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44769
2023-06-26 17:00:00,129 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44769
2023-06-26 17:00:00,129 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46207
2023-06-26 17:00:00,129 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,129 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,129 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,129 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,129 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qbzpix1f
2023-06-26 17:00:00,130 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53de8dfe-dfa9-4680-adb2-66fae10a266d
2023-06-26 17:00:00,280 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42455
2023-06-26 17:00:00,280 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42455
2023-06-26 17:00:00,280 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45093
2023-06-26 17:00:00,280 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,280 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,280 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,280 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,280 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-38qj53bn
2023-06-26 17:00:00,281 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4688953-8f6c-42c8-8796-83d116e1f397
2023-06-26 17:00:00,301 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38473
2023-06-26 17:00:00,302 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38473
2023-06-26 17:00:00,302 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44155
2023-06-26 17:00:00,302 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,302 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,302 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,302 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jx42d6ll
2023-06-26 17:00:00,303 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e09cabe2-86d7-4fe3-acd6-26a79fb1a5d9
2023-06-26 17:00:00,303 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50af24dc-4ad9-408f-aa48-e58d7fd75673
2023-06-26 17:00:00,382 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37493
2023-06-26 17:00:00,382 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37493
2023-06-26 17:00:00,382 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43621
2023-06-26 17:00:00,382 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,382 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,382 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,382 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,382 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ay84fj_g
2023-06-26 17:00:00,383 - distributed.worker - INFO - Starting Worker plugin RMMSetup-487dcf90-5733-425e-9ec7-41b90a5d141c
2023-06-26 17:00:00,396 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37729
2023-06-26 17:00:00,396 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37729
2023-06-26 17:00:00,396 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45279
2023-06-26 17:00:00,396 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,396 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,396 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,396 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vbiiwuy5
2023-06-26 17:00:00,397 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d231fe1f-8024-40e2-a56e-b3827f48a1e8
2023-06-26 17:00:00,417 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41141
2023-06-26 17:00:00,417 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41141
2023-06-26 17:00:00,417 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44893
2023-06-26 17:00:00,417 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,417 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,417 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5tjspool
2023-06-26 17:00:00,418 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42621
2023-06-26 17:00:00,418 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42621
2023-06-26 17:00:00,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2e8f9d0-43d6-483d-92f1-c6c630fe7e64
2023-06-26 17:00:00,418 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39841
2023-06-26 17:00:00,418 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,418 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,418 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,418 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ldw25j7_
2023-06-26 17:00:00,419 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d88e5881-a9ce-4105-9ef3-244920b9c322
2023-06-26 17:00:00,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a1030d4-6590-48cb-83d9-04ee6cb61292
2023-06-26 17:00:00,471 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40993
2023-06-26 17:00:00,471 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40993
2023-06-26 17:00:00,471 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34211
2023-06-26 17:00:00,471 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,471 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,471 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,472 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,472 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1hmj27mh
2023-06-26 17:00:00,473 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70662f2e-a3b1-4c85-a445-fe5f9d5f7355
2023-06-26 17:00:00,565 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41095
2023-06-26 17:00:00,565 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41095
2023-06-26 17:00:00,566 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43495
2023-06-26 17:00:00,566 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,566 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,566 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,566 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,566 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-skzme4h3
2023-06-26 17:00:00,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a85fed08-c6ea-4427-948b-0ce69306cad9
2023-06-26 17:00:00,599 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41071
2023-06-26 17:00:00,599 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41071
2023-06-26 17:00:00,599 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44669
2023-06-26 17:00:00,599 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,599 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,599 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,599 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u9lml4x4
2023-06-26 17:00:00,600 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05849fa0-03c4-4651-a68f-1d1bf8d6fee6
2023-06-26 17:00:00,609 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46209
2023-06-26 17:00:00,609 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46209
2023-06-26 17:00:00,609 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38325
2023-06-26 17:00:00,609 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,609 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,609 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,609 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,609 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cd4vhhlq
2023-06-26 17:00:00,610 - distributed.worker - INFO - Starting Worker plugin PreImport-1ee0dc0f-229c-4d93-87d9-e230daa85079
2023-06-26 17:00:00,610 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87b6ead8-3c22-4c33-a390-fddd5daefa53
2023-06-26 17:00:00,628 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44887
2023-06-26 17:00:00,628 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44887
2023-06-26 17:00:00,628 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34235
2023-06-26 17:00:00,628 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:00:00,629 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:00,629 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:00:00,629 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:00:00,629 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wkrb_0h5
2023-06-26 17:00:00,629 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51ff62d8-7d6c-463c-9eef-6b82d2420244
2023-06-26 17:00:03,814 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5130979-f3f5-45f0-ac57-809d08fd3393
2023-06-26 17:00:03,814 - distributed.worker - INFO - Starting Worker plugin PreImport-d2d36e51-b72a-4068-a895-b117cfb7bec2
2023-06-26 17:00:03,815 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:03,838 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:03,838 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:03,840 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:03,975 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49ebaebc-d025-442e-86e2-c0ef14e05d36
2023-06-26 17:00:03,975 - distributed.worker - INFO - Starting Worker plugin PreImport-d015f3e4-8c5e-43a5-a656-0a1248c7906c
2023-06-26 17:00:03,975 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:03,993 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:03,993 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:03,994 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,016 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2dc3a41-1da3-4a47-bb54-61127e76b053
2023-06-26 17:00:04,017 - distributed.worker - INFO - Starting Worker plugin PreImport-b8eb9914-e1d4-431e-a168-142287d5f9b7
2023-06-26 17:00:04,018 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,043 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,044 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,046 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,103 - distributed.worker - INFO - Starting Worker plugin PreImport-10e2c4bd-1820-4ab9-80cc-b31d39383405
2023-06-26 17:00:04,105 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,126 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00fc4b2a-7447-4f22-a764-af277f569795
2023-06-26 17:00:04,126 - distributed.worker - INFO - Starting Worker plugin PreImport-f903fc91-a6b6-4dd9-a05e-f7de0947e8ec
2023-06-26 17:00:04,127 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,129 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,129 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,131 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,143 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,143 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,144 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,155 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb8f6e6e-d81f-41d6-b3e2-38dde19c1b19
2023-06-26 17:00:04,155 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,166 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-555783d5-9354-4caa-97a7-22f64daebe1c
2023-06-26 17:00:04,167 - distributed.worker - INFO - Starting Worker plugin PreImport-f851a8ef-b6e8-4ba5-bfb6-522f52a4fa9d
2023-06-26 17:00:04,170 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,171 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,172 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,171 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,172 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-049cf6d6-bbf1-4a1c-ba68-ec1baf46d57e
2023-06-26 17:00:04,176 - distributed.worker - INFO - Starting Worker plugin PreImport-cee7769a-2fe0-4a99-bc4c-607f8b0fd597
2023-06-26 17:00:04,178 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,184 - distributed.worker - INFO - Starting Worker plugin PreImport-6dd96005-5d7a-42c8-91c3-8f590fc52e94
2023-06-26 17:00:04,184 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,202 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,202 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,202 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,203 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,205 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,205 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,205 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,205 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,207 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e8dd3a2-ef72-4e6d-93e3-528da209fead
2023-06-26 17:00:04,208 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,208 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,213 - distributed.worker - INFO - Starting Worker plugin PreImport-f18ca20a-7c09-4c56-95b5-ad6e015f3548
2023-06-26 17:00:04,214 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,221 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,221 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,222 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,225 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db129a45-8b4c-4149-b600-d9f72bc55adb
2023-06-26 17:00:04,225 - distributed.worker - INFO - Starting Worker plugin PreImport-470d0369-3068-43bc-996b-3afa464dc55b
2023-06-26 17:00:04,226 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,229 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,229 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,230 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,238 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,239 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,259 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-241bd123-6d14-4bbe-bf1e-ba481ff36292
2023-06-26 17:00:04,266 - distributed.worker - INFO - Starting Worker plugin PreImport-ef29736a-224f-40ef-81f5-36978d11e8d3
2023-06-26 17:00:04,268 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,276 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a52464b1-5408-41b5-a083-73a0e9d5cdb3
2023-06-26 17:00:04,276 - distributed.worker - INFO - Starting Worker plugin PreImport-668ca3c9-44bf-40ab-b044-b26dc2169b66
2023-06-26 17:00:04,277 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,279 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e3cdb00-4392-4605-8d24-f50c98d09ada
2023-06-26 17:00:04,280 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,281 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dba38612-1d5f-42db-9680-9d8db17897a6
2023-06-26 17:00:04,282 - distributed.worker - INFO - Starting Worker plugin PreImport-a71c2c4a-5568-4b48-82bd-a73325968ad1
2023-06-26 17:00:04,282 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,282 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,283 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,284 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,289 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,290 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,291 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,297 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,297 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,298 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:04,300 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:00:04,300 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:00:04,302 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:00:31,299 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,299 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,300 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,300 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,300 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,300 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,300 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,300 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,301 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,302 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,302 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,305 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,306 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,306 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,306 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,308 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:00:31,318 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,318 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,318 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,318 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,318 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,318 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:31,319 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,064 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,064 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,064 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,064 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,064 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,064 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:32,064 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:00:35,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:46,908 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,110 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,143 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,202 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,322 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,339 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,372 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,437 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,461 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,473 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,515 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,516 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,574 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,586 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:47,651 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:48,676 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:00:54,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:00:54,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:33,661 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,662 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,662 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,663 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,664 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,664 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,664 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,667 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,667 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,667 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,667 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,667 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,667 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,667 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,668 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:33,668 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:01:36,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,792 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,793 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,794 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,795 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:36,795 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:01:37,219 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,219 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,225 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:37,227 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:01:41,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:01:41,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,710 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34645. Reason: scheduler-restart
2023-06-26 17:01:41,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,710 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,711 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37493. Reason: scheduler-restart
2023-06-26 17:01:41,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,711 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37729. Reason: scheduler-restart
2023-06-26 17:01:41,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,711 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38473. Reason: scheduler-restart
2023-06-26 17:01:41,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,712 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,712 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,712 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39871. Reason: scheduler-restart
2023-06-26 17:01:41,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,712 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,712 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,712 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,712 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,712 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,712 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,713 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40993. Reason: scheduler-restart
2023-06-26 17:01:41,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,713 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,713 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,713 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41071. Reason: scheduler-restart
2023-06-26 17:01:41,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34645
2023-06-26 17:01:41,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,713 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41095. Reason: scheduler-restart
2023-06-26 17:01:41,713 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,713 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,714 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,714 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,714 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,714 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,714 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41141. Reason: scheduler-restart
2023-06-26 17:01:41,714 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,715 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,715 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,715 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42455. Reason: scheduler-restart
2023-06-26 17:01:41,715 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,715 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,715 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,715 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,716 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,716 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,717 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:01:41,718 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44887. Reason: scheduler-restart
2023-06-26 17:01:41,719 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44501. Reason: scheduler-restart
2023-06-26 17:01:41,721 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42621. Reason: scheduler-restart
2023-06-26 17:01:41,721 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,722 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44769. Reason: scheduler-restart
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37493
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37729
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38473
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37493
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37493
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39871
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37729
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37729
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38473
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40993
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39871
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38473
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39871
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40993
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41071
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40993
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41071
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41071
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41095
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41095
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41095
2023-06-26 17:01:41,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41141
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41141
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41141
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37493
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37729
2023-06-26 17:01:41,723 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,723 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38473
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39871
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40993
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41071
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41095
2023-06-26 17:01:41,723 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41141
2023-06-26 17:01:41,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37493
2023-06-26 17:01:41,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37729
2023-06-26 17:01:41,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38473
2023-06-26 17:01:41,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39871
2023-06-26 17:01:41,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40993
2023-06-26 17:01:41,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41071
2023-06-26 17:01:41,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41095
2023-06-26 17:01:41,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41141
2023-06-26 17:01:41,725 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,731 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46209. Reason: scheduler-restart
2023-06-26 17:01:41,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37493
2023-06-26 17:01:41,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37729
2023-06-26 17:01:41,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38473
2023-06-26 17:01:41,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39871
2023-06-26 17:01:41,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40993
2023-06-26 17:01:41,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41071
2023-06-26 17:01:41,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41095
2023-06-26 17:01:41,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41141
2023-06-26 17:01:41,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42455
2023-06-26 17:01:41,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44887
2023-06-26 17:01:41,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44501
2023-06-26 17:01:41,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42621
2023-06-26 17:01:41,734 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45245. Reason: scheduler-restart
2023-06-26 17:01:41,734 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42455
2023-06-26 17:01:41,734 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,734 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44887
2023-06-26 17:01:41,734 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44501
2023-06-26 17:01:41,734 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42621
2023-06-26 17:01:41,735 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,735 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37493
2023-06-26 17:01:41,735 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37729
2023-06-26 17:01:41,735 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38473
2023-06-26 17:01:41,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39871
2023-06-26 17:01:41,736 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40993
2023-06-26 17:01:41,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41071
2023-06-26 17:01:41,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41095
2023-06-26 17:01:41,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41141
2023-06-26 17:01:41,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42455
2023-06-26 17:01:41,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44887
2023-06-26 17:01:41,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44501
2023-06-26 17:01:41,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42621
2023-06-26 17:01:41,737 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,738 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,740 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44769
2023-06-26 17:01:41,743 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,744 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:01:41,752 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,752 - distributed.nanny - INFO - Worker closed
2023-06-26 17:01:41,759 - distributed.nanny - INFO - Worker closed
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f78851b9640, tag: 0xc11689b78d1fb2a0>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f78851b9640, tag: 0xc11689b78d1fb2a0>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f78851b9540, tag: 0x8453993f9a6ba3a0>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f78851b9540, tag: 0x8453993f9a6ba3a0>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f78851b9700, tag: 0x5a87d0bc02d183d1>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f78851b9700, tag: 0x5a87d0bc02d183d1>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f78851b9400, tag: 0xbeab4db1210eef52>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f78851b9400, tag: 0xbeab4db1210eef52>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f78851b91c0, tag: 0xb4066caecf76ca1a>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f78851b91c0, tag: 0xb4066caecf76ca1a>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-10882' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-11082' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
2023-06-26 17:01:45,532 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:46,167 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:46,692 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:46,698 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:49,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:49,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:49,711 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:49,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:49,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:49,723 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:49,723 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:49,732 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,733 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,739 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,740 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,742 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,745 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,746 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,752 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,753 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,755 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,757 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,771 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:01:49,887 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:49,899 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:49,902 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:49,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,271 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,403 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,417 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,417 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,419 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:01:51,419 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:01:51,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,557 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,588 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,591 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,593 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,600 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,612 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:51,705 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:01:52,061 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40011
2023-06-26 17:01:52,062 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40011
2023-06-26 17:01:52,062 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41331
2023-06-26 17:01:52,062 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:52,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:52,062 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:52,062 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:52,062 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e4zrtdi0
2023-06-26 17:01:52,062 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34547
2023-06-26 17:01:52,062 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34547
2023-06-26 17:01:52,062 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45223
2023-06-26 17:01:52,062 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:52,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:52,062 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:52,062 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:52,062 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6w2y8ufy
2023-06-26 17:01:52,062 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2363b334-2c2f-470e-ab10-01f01fe06c3b
2023-06-26 17:01:52,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa303e8a-85ad-4aed-9bf9-81bb294d42c4
2023-06-26 17:01:52,077 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41175
2023-06-26 17:01:52,077 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41175
2023-06-26 17:01:52,077 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43717
2023-06-26 17:01:52,077 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:52,077 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:52,077 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:52,077 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:52,077 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h3u7pi_a
2023-06-26 17:01:52,077 - distributed.worker - INFO - Starting Worker plugin PreImport-3778e5b9-7820-487e-9495-aea084bcacdd
2023-06-26 17:01:52,078 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3fd0ba25-d92d-49d0-8177-a631749fee3d
2023-06-26 17:01:52,084 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41075
2023-06-26 17:01:52,085 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41075
2023-06-26 17:01:52,085 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40419
2023-06-26 17:01:52,085 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:52,085 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:52,085 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:52,085 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:52,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qtf8hcwu
2023-06-26 17:01:52,086 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34f00120-7d96-4c24-98d6-d8ed3d9bbf20
2023-06-26 17:01:52,086 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c29ef8f-1649-4a87-a0a7-02d4ae9f12d3
2023-06-26 17:01:54,573 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b10c0f8-337c-4b9c-8861-5e2af517c9e4
2023-06-26 17:01:54,575 - distributed.worker - INFO - Starting Worker plugin PreImport-b1917909-f2f0-4001-9d92-fdbcf542275b
2023-06-26 17:01:54,577 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:54,594 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:01:54,594 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:54,597 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:01:54,632 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efb0250b-922b-4127-8b97-d3b557ebc972
2023-06-26 17:01:54,635 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:54,637 - distributed.worker - INFO - Starting Worker plugin PreImport-4bf49c99-2ad3-4036-997b-2689e3314d80
2023-06-26 17:01:54,638 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:54,648 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:01:54,649 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:54,650 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:01:54,655 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:01:54,656 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:54,658 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:01:54,668 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c026774b-46f9-48b3-989a-dafc8d8537c5
2023-06-26 17:01:54,669 - distributed.worker - INFO - Starting Worker plugin PreImport-7c4094d8-2225-4e08-a0d2-619587dadf5b
2023-06-26 17:01:54,671 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:54,687 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:01:54,688 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:54,691 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:01:56,053 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46229
2023-06-26 17:01:56,053 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46229
2023-06-26 17:01:56,053 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44709
2023-06-26 17:01:56,053 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:56,053 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:56,054 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:56,054 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:56,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sy095r0p
2023-06-26 17:01:56,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7ffb9d7-3b6b-4a6b-8de9-4c4b3d0178bb
2023-06-26 17:01:56,920 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-748431b8-cdd8-4079-b5f4-61b8eaa7f150
2023-06-26 17:01:56,921 - distributed.worker - INFO - Starting Worker plugin PreImport-643daf52-4caf-45e5-8e12-9b4083a9f253
2023-06-26 17:01:56,922 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:56,938 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:01:56,938 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:56,944 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:01:57,188 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46047
2023-06-26 17:01:57,188 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46047
2023-06-26 17:01:57,188 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42721
2023-06-26 17:01:57,188 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:57,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:57,188 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:57,188 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:57,188 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v3fr3jjk
2023-06-26 17:01:57,189 - distributed.worker - INFO - Starting Worker plugin PreImport-6820be63-96a1-47c0-a5ca-e43e86ae290d
2023-06-26 17:01:57,189 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbf85341-a527-44b0-b4ee-934b4dccec54
2023-06-26 17:01:58,318 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45705
2023-06-26 17:01:58,318 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45705
2023-06-26 17:01:58,318 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45741
2023-06-26 17:01:58,318 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,318 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,318 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,318 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,318 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oz_xap5v
2023-06-26 17:01:58,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c386707f-fc35-4b10-9a28-56005bbf07e4
2023-06-26 17:01:58,368 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44831
2023-06-26 17:01:58,368 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44831
2023-06-26 17:01:58,368 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33757
2023-06-26 17:01:58,368 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,369 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,369 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,369 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-krrhvt3t
2023-06-26 17:01:58,369 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12be873e-77dd-4e9f-9ecf-8370ff771fc0
2023-06-26 17:01:58,380 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41845
2023-06-26 17:01:58,380 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41845
2023-06-26 17:01:58,380 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34839
2023-06-26 17:01:58,380 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,380 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,380 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,380 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,380 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vuvx430j
2023-06-26 17:01:58,381 - distributed.worker - INFO - Starting Worker plugin RMMSetup-69575b34-a0c0-4763-9335-3618df35aafc
2023-06-26 17:01:58,395 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37913
2023-06-26 17:01:58,395 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36727
2023-06-26 17:01:58,395 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37913
2023-06-26 17:01:58,395 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36727
2023-06-26 17:01:58,395 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42477
2023-06-26 17:01:58,395 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42601
2023-06-26 17:01:58,395 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,395 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,395 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,395 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,395 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,396 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,396 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cuidnnfj
2023-06-26 17:01:58,396 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9vtuc8k4
2023-06-26 17:01:58,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29cf3e85-2769-4773-a6e4-66b0dd3cdc82
2023-06-26 17:01:58,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-007f5899-919d-4765-bca0-4beb028b77b8
2023-06-26 17:01:58,413 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41723
2023-06-26 17:01:58,413 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41723
2023-06-26 17:01:58,414 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35761
2023-06-26 17:01:58,414 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,413 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38083
2023-06-26 17:01:58,414 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,414 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38083
2023-06-26 17:01:58,414 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,414 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40613
2023-06-26 17:01:58,414 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,414 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,414 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bjou3g06
2023-06-26 17:01:58,414 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,414 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h9j3fy86
2023-06-26 17:01:58,415 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f74d826d-094f-412e-a79a-0772cee05db7
2023-06-26 17:01:58,415 - distributed.worker - INFO - Starting Worker plugin PreImport-8b7fd802-4d09-4a96-813e-7f0f8801c6f8
2023-06-26 17:01:58,415 - distributed.worker - INFO - Starting Worker plugin RMMSetup-82edc099-d657-4541-a557-5b5794503259
2023-06-26 17:01:58,425 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43241
2023-06-26 17:01:58,425 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43241
2023-06-26 17:01:58,425 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38957
2023-06-26 17:01:58,425 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,425 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,425 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,426 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m46co0hb
2023-06-26 17:01:58,427 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57eda703-7490-4156-a4d1-0ec3d26a54fb
2023-06-26 17:01:58,427 - distributed.worker - INFO - Starting Worker plugin RMMSetup-908f4135-f2bc-44f1-b8a6-c16f3e4d840c
2023-06-26 17:01:58,429 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38477
2023-06-26 17:01:58,429 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38477
2023-06-26 17:01:58,429 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40165
2023-06-26 17:01:58,429 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,429 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,429 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,429 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,429 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pvfaj9px
2023-06-26 17:01:58,429 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40317
2023-06-26 17:01:58,430 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40317
2023-06-26 17:01:58,430 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38965
2023-06-26 17:01:58,430 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,430 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59c37dc3-52bb-45b5-b337-c66a2c56bd11
2023-06-26 17:01:58,430 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,430 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:01:58,430 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:01:58,430 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w24m39fm
2023-06-26 17:01:58,430 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d33c1b74-baaa-4f31-b29a-920fc4e2c42c
2023-06-26 17:01:58,431 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f777146-f0aa-4bf8-9daf-f2d52173e500
2023-06-26 17:01:58,491 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-278d21f0-ee42-4342-b9e8-e138f0591554
2023-06-26 17:01:58,492 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,502 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:01:58,502 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:01:58,506 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e48cbb03-61de-4723-98c6-1691878253e3
2023-06-26 17:02:00,769 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,769 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d48101ed-2b3e-45c3-ac2b-c1df9c2e3417
2023-06-26 17:02:00,770 - distributed.worker - INFO - Starting Worker plugin PreImport-e16ee76e-ba5c-4b5d-82a5-058c24d8c98a
2023-06-26 17:02:00,772 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,772 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22e61a9a-811f-4a4a-a03c-744f42aac944
2023-06-26 17:02:00,773 - distributed.worker - INFO - Starting Worker plugin PreImport-1f3a4b67-5aaf-4349-a3c8-9bb731deff77
2023-06-26 17:02:00,773 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,787 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,787 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,789 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,789 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,789 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,791 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,798 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,799 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,801 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-220f40b2-5eda-48d8-a2a3-26f591116b6f
2023-06-26 17:02:00,826 - distributed.worker - INFO - Starting Worker plugin PreImport-a3fc7334-0898-4d38-a830-092c64a1bca8
2023-06-26 17:02:00,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,836 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7bd016cc-b034-49da-8682-15ab4b27861a
2023-06-26 17:02:00,837 - distributed.worker - INFO - Starting Worker plugin PreImport-65847095-de6f-473a-adb0-96ddbb850c2d
2023-06-26 17:02:00,840 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-507d19fc-5aab-459d-8a2d-2ded64523f49
2023-06-26 17:02:00,844 - distributed.worker - INFO - Starting Worker plugin PreImport-b0887fb5-7023-4283-9f30-c5cc19f61b7d
2023-06-26 17:02:00,846 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,847 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,847 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,848 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,856 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78a330f9-85ac-4ab7-92a6-9d193890155a
2023-06-26 17:02:00,856 - distributed.worker - INFO - Starting Worker plugin PreImport-38d7cd41-3a31-4631-b73c-0300961d4178
2023-06-26 17:02:00,857 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,868 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,868 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,868 - distributed.worker - INFO - Starting Worker plugin PreImport-05aba046-56fd-430c-9cbc-5b6073ed2199
2023-06-26 17:02:00,869 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,869 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,870 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,870 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,870 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,871 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,872 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,873 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,875 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87289a68-2498-455f-afe2-35b9986519b4
2023-06-26 17:02:00,876 - distributed.worker - INFO - Starting Worker plugin PreImport-7af9a307-410f-4970-8885-1490759f5537
2023-06-26 17:02:00,877 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,883 - distributed.worker - INFO - Starting Worker plugin PreImport-1459a948-ade2-45ba-a0c9-80603ad31433
2023-06-26 17:02:00,885 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,891 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,891 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,893 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,894 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,894 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,896 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:00,903 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:02:00,903 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:02:00,905 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:02:10,197 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,301 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,380 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,519 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,550 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,571 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,598 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,621 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,637 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,646 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,711 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,780 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,780 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,806 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,809 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,819 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:02:10,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,833 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:02:10,841 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,841 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,841 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,841 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,841 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,841 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687798930.842545] [exp01:296907:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:42448) failed: Address already in use
2023-06-26 17:02:10,842 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:10,854 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:02:13,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:21,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:25,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:02:25,301 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,301 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,302 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:25,307 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:02:37,087 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,087 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,087 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:02:37,088 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41075. Reason: worker-close
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46047. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38083. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46229. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43241. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45705. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44831. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40011. Reason: worker-close
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40317. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38477. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41845. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36727. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34547. Reason: worker-close
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41175. Reason: worker-close
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41723. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,221 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37913. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:09:27,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43561'. Reason: nanny-close
2023-06-26 17:09:27,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45575'. Reason: nanny-close
2023-06-26 17:09:27,222 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:44212 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:09:27,222 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:44206 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:09:27,224 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,222 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:44178 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:09:27,222 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:44190 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:09:27,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36629'. Reason: nanny-close
2023-06-26 17:09:27,224 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38585'. Reason: nanny-close
2023-06-26 17:09:27,225 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44879'. Reason: nanny-close
2023-06-26 17:09:27,225 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35611'. Reason: nanny-close
2023-06-26 17:09:27,226 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44105'. Reason: nanny-close
2023-06-26 17:09:27,226 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45571'. Reason: nanny-close
2023-06-26 17:09:27,226 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46373'. Reason: nanny-close
2023-06-26 17:09:27,227 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43545'. Reason: nanny-close
2023-06-26 17:09:27,227 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43531'. Reason: nanny-close
2023-06-26 17:09:27,228 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37653'. Reason: nanny-close
2023-06-26 17:09:27,228 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35751'. Reason: nanny-close
2023-06-26 17:09:27,229 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,229 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38689'. Reason: nanny-close
2023-06-26 17:09:27,229 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,229 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35477'. Reason: nanny-close
2023-06-26 17:09:27,230 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,230 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37969'. Reason: nanny-close
2023-06-26 17:09:27,230 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:09:27,237 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36629 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45856 remote=tcp://10.120.104.11:36629>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36629 after 100 s
2023-06-26 17:09:27,242 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44879 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47686 remote=tcp://10.120.104.11:44879>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44879 after 100 s
2023-06-26 17:09:27,245 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38689 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:38238 remote=tcp://10.120.104.11:38689>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38689 after 100 s
2023-06-26 17:09:27,245 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38585 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37946 remote=tcp://10.120.104.11:38585>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38585 after 100 s
2023-06-26 17:09:27,246 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35611 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47138 remote=tcp://10.120.104.11:35611>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35611 after 100 s
2023-06-26 17:09:27,246 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45571 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:32842 remote=tcp://10.120.104.11:45571>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45571 after 100 s
2023-06-26 17:09:27,247 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46373 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50458 remote=tcp://10.120.104.11:46373>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46373 after 100 s
2023-06-26 17:09:27,247 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43545 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56316 remote=tcp://10.120.104.11:43545>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43545 after 100 s
2023-06-26 17:09:27,247 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43531 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55890 remote=tcp://10.120.104.11:43531>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43531 after 100 s
2023-06-26 17:09:27,248 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37969 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55898 remote=tcp://10.120.104.11:37969>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37969 after 100 s
2023-06-26 17:09:27,248 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44105 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55524 remote=tcp://10.120.104.11:44105>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44105 after 100 s
2023-06-26 17:09:27,249 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35751 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56340 remote=tcp://10.120.104.11:35751>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35751 after 100 s
2023-06-26 17:09:27,251 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35477 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52546 remote=tcp://10.120.104.11:35477>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35477 after 100 s
2023-06-26 17:09:27,252 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45575 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:53588 remote=tcp://10.120.104.11:45575>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45575 after 100 s
2023-06-26 17:09:27,257 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37653 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39234 remote=tcp://10.120.104.11:37653>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37653 after 100 s
2023-06-26 17:09:30,431 - distributed.nanny - WARNING - Worker process still alive after 3.1999954223632816 seconds, killing
2023-06-26 17:09:30,431 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:30,431 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 17:09:30,431 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:30,432 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:30,433 - distributed.nanny - WARNING - Worker process still alive after 3.19999984741211 seconds, killing
2023-06-26 17:09:30,434 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 17:09:30,435 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 17:09:30,435 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:30,436 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 17:09:30,436 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:30,437 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:30,437 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:30,438 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:30,438 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 17:09:30,440 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:09:31,223 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,225 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,226 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,226 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,227 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,227 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,227 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,228 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,228 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,229 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,229 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,230 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,230 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,231 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,231 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,232 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296925 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296922 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296919 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296916 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296913 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296910 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296907 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296904 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296901 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296898 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296895 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296892 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296848 parent=293815 started daemon>
2023-06-26 17:09:31,234 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296845 parent=293815 started daemon>
2023-06-26 17:09:31,235 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296828 parent=293815 started daemon>
2023-06-26 17:09:31,235 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=296824 parent=293815 started daemon>
2023-06-26 17:09:36,562 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 296901 exit status was already read will report exitcode 255
