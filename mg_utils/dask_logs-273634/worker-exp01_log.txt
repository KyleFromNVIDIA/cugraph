RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 16:45:09,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41289'
2023-06-26 16:45:09,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35639'
2023-06-26 16:45:09,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43457'
2023-06-26 16:45:09,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37049'
2023-06-26 16:45:09,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42717'
2023-06-26 16:45:09,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34853'
2023-06-26 16:45:09,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40361'
2023-06-26 16:45:09,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42355'
2023-06-26 16:45:09,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39929'
2023-06-26 16:45:09,193 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41381'
2023-06-26 16:45:09,194 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43563'
2023-06-26 16:45:09,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38965'
2023-06-26 16:45:09,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42515'
2023-06-26 16:45:09,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42373'
2023-06-26 16:45:09,203 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41667'
2023-06-26 16:45:09,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36015'
2023-06-26 16:45:10,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:10,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:10,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:10,924 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:10,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:10,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:10,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:10,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,016 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,016 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,018 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,018 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,023 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,023 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,031 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,031 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,034 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,034 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,034 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,034 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,037 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:45:11,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:45:11,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,102 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,102 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,194 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,200 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,202 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,213 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,214 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,214 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,215 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,215 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,216 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:11,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:45:17,552 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39393
2023-06-26 16:45:17,552 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39393
2023-06-26 16:45:17,552 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35019
2023-06-26 16:45:17,552 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,552 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,552 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,552 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p3qndegn
2023-06-26 16:45:17,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ba71360-14b9-41d8-912d-da77489addc7
2023-06-26 16:45:17,555 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ad4d88b-40d4-4c75-a5bf-a93f4820da1d
2023-06-26 16:45:17,558 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39513
2023-06-26 16:45:17,558 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39513
2023-06-26 16:45:17,558 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39027
2023-06-26 16:45:17,558 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,558 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,558 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,559 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,559 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-opo4e4n4
2023-06-26 16:45:17,559 - distributed.worker - INFO - Starting Worker plugin PreImport-c6d0aaf0-7e10-4749-86f5-8fc1470ee5a9
2023-06-26 16:45:17,559 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4b255c2-e199-4d59-8b31-a48fdbb8c625
2023-06-26 16:45:17,562 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33023
2023-06-26 16:45:17,562 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33023
2023-06-26 16:45:17,562 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34033
2023-06-26 16:45:17,562 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,562 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,563 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,563 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,563 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yrtflmdo
2023-06-26 16:45:17,563 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51baf0f2-8dc4-49ed-be7d-6a67a98ad177
2023-06-26 16:45:17,827 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41715
2023-06-26 16:45:17,827 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41715
2023-06-26 16:45:17,827 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46265
2023-06-26 16:45:17,827 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,827 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,827 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5lwhzshl
2023-06-26 16:45:17,828 - distributed.worker - INFO - Starting Worker plugin PreImport-59bcf036-747b-42ec-ad3d-57d81cc139c6
2023-06-26 16:45:17,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa393166-4066-48b1-9781-e49344248b73
2023-06-26 16:45:17,836 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37927
2023-06-26 16:45:17,836 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37927
2023-06-26 16:45:17,836 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33227
2023-06-26 16:45:17,836 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,836 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,836 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,836 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,836 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e72x3zhi
2023-06-26 16:45:17,837 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f36921f5-eb1f-4db1-818b-71ec0f28d331
2023-06-26 16:45:17,844 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40355
2023-06-26 16:45:17,845 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40355
2023-06-26 16:45:17,845 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45911
2023-06-26 16:45:17,845 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,845 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,845 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,845 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sdkpki97
2023-06-26 16:45:17,846 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad20e72c-c004-4915-976c-2e65c942ae7a
2023-06-26 16:45:17,846 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33591
2023-06-26 16:45:17,846 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33591
2023-06-26 16:45:17,846 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38243
2023-06-26 16:45:17,846 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,846 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,846 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,846 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,846 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0qse9ogr
2023-06-26 16:45:17,847 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8c60eb6-659e-409d-9d88-73f2db1e7bd9
2023-06-26 16:45:17,862 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35795
2023-06-26 16:45:17,862 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35795
2023-06-26 16:45:17,862 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35149
2023-06-26 16:45:17,862 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,862 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,863 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,863 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wtlhy5i3
2023-06-26 16:45:17,863 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29160965-630c-435d-bf52-beabe97dbe7d
2023-06-26 16:45:17,924 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38807
2023-06-26 16:45:17,924 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38807
2023-06-26 16:45:17,924 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35579
2023-06-26 16:45:17,924 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,924 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,924 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,924 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,924 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-be_zehdq
2023-06-26 16:45:17,925 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6bc3424-4bf6-43d1-afe8-b8d577882278
2023-06-26 16:45:17,930 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35073
2023-06-26 16:45:17,930 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35073
2023-06-26 16:45:17,930 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43381
2023-06-26 16:45:17,930 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,930 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,930 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,931 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,931 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-55rpir77
2023-06-26 16:45:17,932 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04c86323-df46-4964-a705-9fc21a8d0bab
2023-06-26 16:45:17,936 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41557
2023-06-26 16:45:17,936 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41557
2023-06-26 16:45:17,936 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41545
2023-06-26 16:45:17,936 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,936 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,936 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,936 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,936 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ba78ptj1
2023-06-26 16:45:17,937 - distributed.worker - INFO - Starting Worker plugin PreImport-345edb88-c2d2-4447-9615-19731091398e
2023-06-26 16:45:17,937 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6afb49b-324d-4746-895a-2d3796068e6a
2023-06-26 16:45:17,943 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38993
2023-06-26 16:45:17,943 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38993
2023-06-26 16:45:17,943 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43353
2023-06-26 16:45:17,943 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:17,943 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:17,943 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:17,943 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:17,943 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ys0pvco_
2023-06-26 16:45:17,945 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a9911fa-84bf-4d2e-a176-fb8359368450
2023-06-26 16:45:17,945 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1055f15-6143-4579-bb6c-016cf932f6a0
2023-06-26 16:45:18,127 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41367
2023-06-26 16:45:18,128 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41367
2023-06-26 16:45:18,128 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46245
2023-06-26 16:45:18,128 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:18,128 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:18,128 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:18,128 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:18,128 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ni8s0dvk
2023-06-26 16:45:18,129 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a894d1d-01b0-417b-b34c-8ff1aec133e2
2023-06-26 16:45:18,129 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84cdb93f-d60e-4b76-8ed6-8e93c153f7f9
2023-06-26 16:45:18,143 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33885
2023-06-26 16:45:18,143 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33885
2023-06-26 16:45:18,143 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38179
2023-06-26 16:45:18,143 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:18,143 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:18,143 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:18,144 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:18,144 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-svqkbx55
2023-06-26 16:45:18,144 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b1aa141-0650-46e0-8d91-0bfcd5bbac1c
2023-06-26 16:45:18,152 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41127
2023-06-26 16:45:18,152 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41127
2023-06-26 16:45:18,152 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42303
2023-06-26 16:45:18,152 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:18,153 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:18,153 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:18,153 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:18,153 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g5q7oe24
2023-06-26 16:45:18,153 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c9c1216-b32b-43b0-955a-fc3d02cafb38
2023-06-26 16:45:18,160 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41149
2023-06-26 16:45:18,160 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41149
2023-06-26 16:45:18,160 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33407
2023-06-26 16:45:18,160 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:45:18,160 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:18,160 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:45:18,161 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:45:18,161 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zpmwpgrt
2023-06-26 16:45:18,161 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e83b7e6-b168-4b7c-871b-1bc197b23d30
2023-06-26 16:45:21,545 - distributed.worker - INFO - Starting Worker plugin PreImport-0cabb95f-0799-4cd6-90b1-7d63a43cddc1
2023-06-26 16:45:21,546 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06858a31-527d-4c90-b55f-630cf59a512e
2023-06-26 16:45:21,570 - distributed.worker - INFO - Starting Worker plugin PreImport-f925d324-bbd5-4c4c-82c1-935c72a7743e
2023-06-26 16:45:21,570 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,578 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,578 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,579 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,599 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,599 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,600 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,679 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80bdde43-ac6b-4f12-bfa9-a3818c4a22c5
2023-06-26 16:45:21,679 - distributed.worker - INFO - Starting Worker plugin PreImport-a0c17f38-4e8b-47ee-841d-26983475f49e
2023-06-26 16:45:21,682 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9008f34-5501-475c-a2b0-6843fefd61f2
2023-06-26 16:45:21,682 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,685 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,690 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed0eda8b-5893-4a22-83ff-9c0925a71a6c
2023-06-26 16:45:21,692 - distributed.worker - INFO - Starting Worker plugin PreImport-e035af10-bf56-49f5-ba1e-08195deea347
2023-06-26 16:45:21,692 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,703 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,703 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,704 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,718 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,718 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,720 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,720 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,720 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,721 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,738 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5659302f-1655-42a8-8422-1cec8fc3f789
2023-06-26 16:45:21,743 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05e63d3a-454d-457c-bc8e-b9c7ad8bc0b8
2023-06-26 16:45:21,750 - distributed.worker - INFO - Starting Worker plugin PreImport-68c5f800-7078-4203-a913-d090251d624f
2023-06-26 16:45:21,753 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,777 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,777 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,779 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,782 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,782 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,784 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,795 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13dcf627-d132-43a6-821a-c00d4174897a
2023-06-26 16:45:21,795 - distributed.worker - INFO - Starting Worker plugin PreImport-719edf37-40b5-4512-bc76-405044d7612e
2023-06-26 16:45:21,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7e5bb2a-2df4-43e7-9a23-642d8ab98185
2023-06-26 16:45:21,799 - distributed.worker - INFO - Starting Worker plugin PreImport-26eee5d2-2ca5-4947-8143-5ff9f1b2db9e
2023-06-26 16:45:21,800 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,801 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,835 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,836 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,836 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,836 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,838 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,839 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,872 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b3649cc-afc8-461e-8953-555c0984ebb7
2023-06-26 16:45:21,873 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,891 - distributed.worker - INFO - Starting Worker plugin PreImport-c577a472-0161-43d9-9cce-f0247a4a00ff
2023-06-26 16:45:21,893 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,893 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,893 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,894 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,899 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-38f61c71-1fa4-454c-b631-16f491c5eba7
2023-06-26 16:45:21,900 - distributed.worker - INFO - Starting Worker plugin PreImport-e81511ce-90b5-4826-b75f-61b0c3cb11b1
2023-06-26 16:45:21,901 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,911 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,911 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,912 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,919 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,919 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,921 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,954 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20f31549-8aef-40eb-8078-21240d929ca9
2023-06-26 16:45:21,955 - distributed.worker - INFO - Starting Worker plugin PreImport-3dbaaa56-cb13-42a9-8c68-4e194b8350f5
2023-06-26 16:45:21,956 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,964 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56afbfd1-bda0-4d9d-8b5f-216a13e56285
2023-06-26 16:45:21,965 - distributed.worker - INFO - Starting Worker plugin PreImport-ef0d7312-e33e-4b67-8f88-aed2ad4bcfeb
2023-06-26 16:45:21,966 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,971 - distributed.worker - INFO - Starting Worker plugin PreImport-218d0396-f1de-406f-b8d3-42c2b03ab407
2023-06-26 16:45:21,972 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,975 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,975 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,976 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83549455-837a-479e-80ee-e5437205e230
2023-06-26 16:45:21,977 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,977 - distributed.worker - INFO - Starting Worker plugin PreImport-bbb1bf4d-f44b-4030-8655-4ef24e76a501
2023-06-26 16:45:21,978 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,992 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,992 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:21,995 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:21,999 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:21,999 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:22,002 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:45:22,002 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:45:22,002 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:22,005 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:45:30,494 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,494 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,494 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,494 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,494 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,495 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,495 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,496 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,496 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,496 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,497 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,497 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,499 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,500 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,500 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,504 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,513 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,514 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,514 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:30,514 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:45:31,214 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,214 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,214 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,214 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,214 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,214 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,214 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:31,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:45:34,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:46,791 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:46,884 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:46,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:46,894 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,072 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,287 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,287 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,296 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,307 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,398 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,405 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,406 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,408 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,410 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,468 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:47,498 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:45:54,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:45:54,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:32,731 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,731 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,732 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,733 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,736 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,737 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:32,740 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:46:35,900 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,908 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,908 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,908 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,908 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,908 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,908 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:35,908 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:46:36,275 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-73bce0343bfdf733f3b148ddafbba909-260d36ae63e430976ed351907b2d2e17', 1)
Function:  subgraph_callable-75dc0c8c-09fb-419c-a9c5-3b6ef612
args:      ("('__filter_batches-5e4b0443eac066eccbe24085cfe3a86f', 1)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:46:36,275 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-73bce0343bfdf733f3b148ddafbba909-260d36ae63e430976ed351907b2d2e17', 0)
Function:  subgraph_callable-75dc0c8c-09fb-419c-a9c5-3b6ef612
args:      ("('__filter_batches-5e4b0443eac066eccbe24085cfe3a86f', 0)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:46:36,276 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-73bce0343bfdf733f3b148ddafbba909-260d36ae63e430976ed351907b2d2e17', 3)
Function:  subgraph_callable-75dc0c8c-09fb-419c-a9c5-3b6ef612
args:      ("('__filter_batches-5e4b0443eac066eccbe24085cfe3a86f', 3)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:46:36,276 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-73bce0343bfdf733f3b148ddafbba909-260d36ae63e430976ed351907b2d2e17', 2)
Function:  subgraph_callable-75dc0c8c-09fb-419c-a9c5-3b6ef612
args:      ("('__filter_batches-5e4b0443eac066eccbe24085cfe3a86f', 2)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:46:36,282 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,288 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,289 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,289 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,289 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,289 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:36,289 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:46:40,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:46:40,772 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,774 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33023. Reason: scheduler-restart
2023-06-26 16:46:40,774 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,775 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,775 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,775 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33591. Reason: scheduler-restart
2023-06-26 16:46:40,776 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,776 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,776 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33885. Reason: scheduler-restart
2023-06-26 16:46:40,776 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35073. Reason: scheduler-restart
2023-06-26 16:46:40,777 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,777 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,777 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35795. Reason: scheduler-restart
2023-06-26 16:46:40,777 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,777 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37927. Reason: scheduler-restart
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,778 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,778 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,778 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38807. Reason: scheduler-restart
2023-06-26 16:46:40,778 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,778 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,779 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38993. Reason: scheduler-restart
2023-06-26 16:46:40,779 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,779 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,779 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,780 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,780 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39393. Reason: scheduler-restart
2023-06-26 16:46:40,780 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39513. Reason: scheduler-restart
2023-06-26 16:46:40,780 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,780 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,780 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,780 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,780 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40355. Reason: scheduler-restart
2023-06-26 16:46:40,780 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,781 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,781 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,781 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,781 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,781 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,781 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41127. Reason: scheduler-restart
2023-06-26 16:46:40,781 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,782 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,782 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,782 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,782 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:46:40,782 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,783 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,787 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,788 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41367. Reason: scheduler-restart
2023-06-26 16:46:40,788 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,788 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41149. Reason: scheduler-restart
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33591
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35073
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35795
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38807
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37927
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38993
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39393
2023-06-26 16:46:40,789 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39513
2023-06-26 16:46:40,790 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33591
2023-06-26 16:46:40,790 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 16:46:40,790 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35073
2023-06-26 16:46:40,790 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35795
2023-06-26 16:46:40,790 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38807
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37927
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38993
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33591
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39393
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39513
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35073
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35795
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38807
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37927
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40355
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38993
2023-06-26 16:46:40,791 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39393
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39513
2023-06-26 16:46:40,791 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40355
2023-06-26 16:46:40,797 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,800 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33023
2023-06-26 16:46:40,801 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33591
2023-06-26 16:46:40,801 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 16:46:40,801 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35073
2023-06-26 16:46:40,801 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35795
2023-06-26 16:46:40,801 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38807
2023-06-26 16:46:40,802 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37927
2023-06-26 16:46:40,802 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38993
2023-06-26 16:46:40,802 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39393
2023-06-26 16:46:40,802 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39513
2023-06-26 16:46:40,802 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41715. Reason: scheduler-restart
2023-06-26 16:46:40,802 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,802 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41557. Reason: scheduler-restart
2023-06-26 16:46:40,803 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40355
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41127
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33591
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41149
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33885
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35073
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35795
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38807
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37927
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38993
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39393
2023-06-26 16:46:40,804 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39513
2023-06-26 16:46:40,805 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41127
2023-06-26 16:46:40,805 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41149
2023-06-26 16:46:40,806 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41715
2023-06-26 16:46:40,806 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,807 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40355
2023-06-26 16:46:40,808 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,808 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,808 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,810 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,815 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41127
2023-06-26 16:46:40,815 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41149
2023-06-26 16:46:40,816 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41715
2023-06-26 16:46:40,816 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41367
2023-06-26 16:46:40,816 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:46:40,818 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:40,823 - distributed.nanny - INFO - Worker closed
2023-06-26 16:46:44,011 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:45,203 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:45,539 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:46,915 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:47,415 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:47,668 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,176 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:48,176 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:48,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:48,934 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:48,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:48,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:48,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:48,953 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,957 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,958 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,961 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,965 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,965 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,968 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:48,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:48,971 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:48,971 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:48,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:48,973 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:46:49,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:49,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:49,127 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:49,147 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:49,153 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:49,163 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:49,296 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:50,753 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,783 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,783 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,797 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,797 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45717
2023-06-26 16:46:50,797 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45717
2023-06-26 16:46:50,797 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43717
2023-06-26 16:46:50,797 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:50,797 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:50,797 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:50,798 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:50,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vzp0pyeg
2023-06-26 16:46:50,798 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25abcc95-c77b-4804-825a-25c878596771
2023-06-26 16:46:50,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,804 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,816 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,816 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:46:50,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:46:50,931 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:50,932 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:50,962 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:50,973 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:50,984 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:50,984 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:50,996 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:51,005 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:51,008 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:51,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:46:51,683 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39031
2023-06-26 16:46:51,683 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39031
2023-06-26 16:46:51,683 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46395
2023-06-26 16:46:51,683 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:51,683 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:51,683 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:51,683 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:51,683 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i91himwz
2023-06-26 16:46:51,684 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1daa4cad-c620-48da-b396-d1e789a00833
2023-06-26 16:46:51,688 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46411dc1-1a1e-4bd3-8ad7-2dc036e97938
2023-06-26 16:46:52,015 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40919
2023-06-26 16:46:52,015 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40919
2023-06-26 16:46:52,016 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37277
2023-06-26 16:46:52,016 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:52,016 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:52,016 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:52,016 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:52,016 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f272onlt
2023-06-26 16:46:52,016 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f05a9eff-bf2d-416d-8377-3e4f90f53236
2023-06-26 16:46:52,052 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34105
2023-06-26 16:46:52,052 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34105
2023-06-26 16:46:52,052 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35601
2023-06-26 16:46:52,052 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:52,052 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:52,052 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:52,052 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:52,052 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-elspdpcq
2023-06-26 16:46:52,053 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3b0cddb-ef45-42df-9282-7285a33fc9dc
2023-06-26 16:46:52,053 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7780b41b-5f9f-44c6-aefa-5becf399295e
2023-06-26 16:46:52,061 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40755
2023-06-26 16:46:52,062 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40755
2023-06-26 16:46:52,062 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38499
2023-06-26 16:46:52,062 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:52,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:52,062 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:52,062 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:52,062 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zpdfydih
2023-06-26 16:46:52,064 - distributed.worker - INFO - Starting Worker plugin PreImport-47f8a537-6530-4295-9bb5-0806a96e8d3b
2023-06-26 16:46:52,064 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7071280c-9928-424a-92f8-cbf986a85d6a
2023-06-26 16:46:52,097 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41427
2023-06-26 16:46:52,098 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41427
2023-06-26 16:46:52,098 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40169
2023-06-26 16:46:52,098 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:52,098 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:52,098 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:52,098 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:52,098 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7eh9sr5w
2023-06-26 16:46:52,099 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4736d57f-bb86-42e2-9df9-c51a14adc244
2023-06-26 16:46:53,285 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9034889c-12b0-4a3a-8a27-ee3bdcd18269
2023-06-26 16:46:53,285 - distributed.worker - INFO - Starting Worker plugin PreImport-73dda565-4c23-4173-bea2-4ed152485669
2023-06-26 16:46:53,286 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:53,298 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:46:53,298 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:53,300 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:46:54,144 - distributed.worker - INFO - Starting Worker plugin PreImport-826ce56c-ab0c-4203-94ed-b094f64fccf4
2023-06-26 16:46:54,146 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,164 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:46:54,164 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,167 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:46:54,352 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29e79778-f9e9-4fa4-a1e0-0895ce0b2b4a
2023-06-26 16:46:54,352 - distributed.worker - INFO - Starting Worker plugin PreImport-6bcf659e-d6b9-44b4-98bd-a93e5f0fdb6a
2023-06-26 16:46:54,353 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,362 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24b80f32-9f35-4883-b1a6-608f8decf479
2023-06-26 16:46:54,364 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,364 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:46:54,364 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,366 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:46:54,409 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:46:54,409 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,411 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:46:54,461 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-43003b79-4ef5-4e76-a7e8-068216b03755
2023-06-26 16:46:54,462 - distributed.worker - INFO - Starting Worker plugin PreImport-44668800-5888-41a3-8274-c7122cb61c83
2023-06-26 16:46:54,470 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,492 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:46:54,492 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,495 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:46:54,579 - distributed.worker - INFO - Starting Worker plugin PreImport-b3cee547-2d1f-4538-b1ea-6541d2ba6269
2023-06-26 16:46:54,580 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,598 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:46:54,599 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:54,600 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:46:57,944 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44191
2023-06-26 16:46:57,945 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44191
2023-06-26 16:46:57,945 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36391
2023-06-26 16:46:57,945 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:57,945 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:57,945 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:57,945 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:57,945 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ht1_zxvt
2023-06-26 16:46:57,945 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee853729-1865-45a6-b766-a19411e129f8
2023-06-26 16:46:57,973 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44053
2023-06-26 16:46:57,973 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44053
2023-06-26 16:46:57,973 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36559
2023-06-26 16:46:57,973 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:57,973 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:57,973 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:57,973 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:57,973 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ug0jg51o
2023-06-26 16:46:57,974 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e1f8687-4f23-459b-a485-bb2036197f76
2023-06-26 16:46:57,994 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42767
2023-06-26 16:46:57,995 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42767
2023-06-26 16:46:57,995 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39203
2023-06-26 16:46:57,995 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:57,995 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:57,995 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:57,995 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:57,995 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d9hbxbw8
2023-06-26 16:46:57,995 - distributed.worker - INFO - Starting Worker plugin PreImport-35949430-722e-4caa-b57c-45e1d0526e14
2023-06-26 16:46:57,996 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27cb3514-c08d-4354-9457-a5c028954a63
2023-06-26 16:46:58,026 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39975
2023-06-26 16:46:58,026 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39975
2023-06-26 16:46:58,026 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33939
2023-06-26 16:46:58,026 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:58,026 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:58,026 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:58,026 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:58,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-of4us6rp
2023-06-26 16:46:58,026 - distributed.worker - INFO - Starting Worker plugin PreImport-4ddfc289-dc71-443f-80ae-6e9eb7154e52
2023-06-26 16:46:58,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb352760-9b18-4ce6-9ba5-284a5193846c
2023-06-26 16:46:58,029 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39533
2023-06-26 16:46:58,029 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39533
2023-06-26 16:46:58,029 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41793
2023-06-26 16:46:58,030 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:58,030 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:58,030 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:58,030 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:58,030 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ejz94txr
2023-06-26 16:46:58,031 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59925823-5f20-4836-b186-669bb0e0700d
2023-06-26 16:46:58,041 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33795
2023-06-26 16:46:58,041 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33795
2023-06-26 16:46:58,041 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38987
2023-06-26 16:46:58,041 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:58,041 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:58,041 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:58,041 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:58,041 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-02attci8
2023-06-26 16:46:58,042 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d300dda7-3a48-423d-b8ca-5b2a0333a307
2023-06-26 16:46:58,056 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39883
2023-06-26 16:46:58,056 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39883
2023-06-26 16:46:58,056 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46317
2023-06-26 16:46:58,056 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:58,056 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:58,056 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:58,056 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:58,056 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cmevasz9
2023-06-26 16:46:58,057 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3fa99b2-4689-4fa6-86b2-54eb165a505b
2023-06-26 16:46:58,066 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33183
2023-06-26 16:46:58,066 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33183
2023-06-26 16:46:58,066 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40775
2023-06-26 16:46:58,066 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:58,066 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:58,066 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:58,066 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:58,066 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sk5sgo7_
2023-06-26 16:46:58,067 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32837
2023-06-26 16:46:58,067 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65f3d848-b6f5-49ca-9e53-a5ff4e9df12e
2023-06-26 16:46:58,067 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32837
2023-06-26 16:46:58,067 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45679
2023-06-26 16:46:58,067 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32799
2023-06-26 16:46:58,067 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:58,067 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:58,067 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32799
2023-06-26 16:46:58,067 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:58,067 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42283
2023-06-26 16:46:58,067 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:58,067 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:46:58,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2q3cs769
2023-06-26 16:46:58,067 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:46:58,067 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:46:58,067 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:46:58,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uzh787ga
2023-06-26 16:46:58,068 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02c0f3f5-6ef3-4498-8cc1-04c209fff857
2023-06-26 16:46:58,070 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-424fe05d-9afd-4452-ab43-bf6ec655eb7d
2023-06-26 16:46:58,070 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3cfb405-ae0d-4662-950e-a4ee8881370b
2023-06-26 16:47:00,406 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d03f76b0-f033-4323-99b9-3632c904f2bc
2023-06-26 16:47:00,406 - distributed.worker - INFO - Starting Worker plugin PreImport-cbbc627c-3349-4ff0-928f-5dd8cc1064e3
2023-06-26 16:47:00,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,423 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,423 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a83c4aaf-f587-4684-bf63-198cd714b29d
2023-06-26 16:47:00,425 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,426 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,439 - distributed.worker - INFO - Starting Worker plugin PreImport-ca2e0716-0ef4-4a1d-96cb-abc6f592e1f2
2023-06-26 16:47:00,440 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,446 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,446 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,449 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,455 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-585224a2-6611-4600-8188-024ec1e53bee
2023-06-26 16:47:00,456 - distributed.worker - INFO - Starting Worker plugin PreImport-e9d71a0f-a925-44dd-91df-56315462ea7a
2023-06-26 16:47:00,457 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,457 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,457 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,459 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,472 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c8e9605-a0bd-4853-85f9-a730e16edb20
2023-06-26 16:47:00,474 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,477 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,477 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,479 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-995f6e87-6d85-4042-9868-cc5ba7b5572a
2023-06-26 16:47:00,479 - distributed.worker - INFO - Starting Worker plugin PreImport-52a38e19-f678-4c90-bd70-48f82a1d47d1
2023-06-26 16:47:00,479 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,480 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,491 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,491 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,493 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,496 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,496 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,497 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,515 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff506684-08db-488b-b12c-b5452bdf65ac
2023-06-26 16:47:00,515 - distributed.worker - INFO - Starting Worker plugin PreImport-36d5c8af-a10e-4155-97df-0b0c4689696e
2023-06-26 16:47:00,516 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,524 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee05d5b7-54bb-4d18-9ca8-711c3627a130
2023-06-26 16:47:00,525 - distributed.worker - INFO - Starting Worker plugin PreImport-affba611-51d2-49a5-b13d-c668d39bf23f
2023-06-26 16:47:00,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,531 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,531 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,533 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,533 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6baf0173-7481-4c5c-a1b8-cb71ed63cc5e
2023-06-26 16:47:00,533 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64b6bff2-e9d8-473c-bb65-4279d0aa3790
2023-06-26 16:47:00,534 - distributed.worker - INFO - Starting Worker plugin PreImport-c3120976-ad1c-41a0-a3dd-9029625aaeff
2023-06-26 16:47:00,534 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,536 - distributed.worker - INFO - Starting Worker plugin PreImport-920ecb63-a0c4-44d8-b494-f07ef19c1ffe
2023-06-26 16:47:00,539 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,545 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,545 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,547 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,547 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,548 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,548 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:00,563 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:47:00,563 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:47:00,566 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:47:09,914 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:09,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,010 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,010 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,099 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,158 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,228 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,267 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,270 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,287 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,323 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,383 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,412 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,414 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,460 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,506 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,559 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:47:10,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:10,571 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,571 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,572 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:47:10,581 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,581 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,581 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,581 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,581 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,581 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,582 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:10,595 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:47:13,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:18,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:23,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:24,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:24,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:24,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,019 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,020 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,020 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,020 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,020 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,020 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:24,020 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,843 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,844 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:47:35,844 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41427. Reason: worker-close
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44053. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33183. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32837. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39031. Reason: worker-close
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40919. Reason: worker-close
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39975. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34105. Reason: worker-close
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40755. Reason: worker-close
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32799. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42767. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33795. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44191. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39883. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39533. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,064 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45717. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:49:46,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37049'. Reason: nanny-close
2023-06-26 16:49:46,066 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,065 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:40772 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:49:46,065 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:40750 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:49:46,065 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:40782 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:49:46,066 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39929'. Reason: nanny-close
2023-06-26 16:49:46,065 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:40758 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:49:46,065 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:40718 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:49:46,067 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,067 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41381'. Reason: nanny-close
2023-06-26 16:49:46,068 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43563'. Reason: nanny-close
2023-06-26 16:49:46,068 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41289'. Reason: nanny-close
2023-06-26 16:49:46,069 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35639'. Reason: nanny-close
2023-06-26 16:49:46,069 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43457'. Reason: nanny-close
2023-06-26 16:49:46,069 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42717'. Reason: nanny-close
2023-06-26 16:49:46,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34853'. Reason: nanny-close
2023-06-26 16:49:46,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40361'. Reason: nanny-close
2023-06-26 16:49:46,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42355'. Reason: nanny-close
2023-06-26 16:49:46,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38965'. Reason: nanny-close
2023-06-26 16:49:46,072 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42515'. Reason: nanny-close
2023-06-26 16:49:46,072 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42373'. Reason: nanny-close
2023-06-26 16:49:46,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41667'. Reason: nanny-close
2023-06-26 16:49:46,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36015'. Reason: nanny-close
2023-06-26 16:49:46,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:49:46,084 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39929 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48466 remote=tcp://10.120.104.11:39929>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39929 after 100 s
2023-06-26 16:49:46,086 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41289 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45194 remote=tcp://10.120.104.11:41289>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41289 after 100 s
2023-06-26 16:49:46,086 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34853 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55066 remote=tcp://10.120.104.11:34853>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34853 after 100 s
2023-06-26 16:49:46,088 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35639 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48526 remote=tcp://10.120.104.11:35639>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35639 after 100 s
2023-06-26 16:49:46,089 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43457 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:54472 remote=tcp://10.120.104.11:43457>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43457 after 100 s
2023-06-26 16:49:46,089 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42717 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41756 remote=tcp://10.120.104.11:42717>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42717 after 100 s
2023-06-26 16:49:46,091 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40361 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41850 remote=tcp://10.120.104.11:40361>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40361 after 100 s
2023-06-26 16:49:46,091 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38965 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55498 remote=tcp://10.120.104.11:38965>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38965 after 100 s
2023-06-26 16:49:46,092 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42515 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40882 remote=tcp://10.120.104.11:42515>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42515 after 100 s
2023-06-26 16:49:46,091 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42355 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46302 remote=tcp://10.120.104.11:42355>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42355 after 100 s
2023-06-26 16:49:46,091 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43563 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40028 remote=tcp://10.120.104.11:43563>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43563 after 100 s
2023-06-26 16:49:46,093 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41667 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60992 remote=tcp://10.120.104.11:41667>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41667 after 100 s
2023-06-26 16:49:46,093 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42373 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40170 remote=tcp://10.120.104.11:42373>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42373 after 100 s
2023-06-26 16:49:46,094 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36015 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:57436 remote=tcp://10.120.104.11:36015>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36015 after 100 s
2023-06-26 16:49:46,099 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41381 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:51558 remote=tcp://10.120.104.11:41381>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41381 after 100 s
2023-06-26 16:49:49,275 - distributed.nanny - WARNING - Worker process still alive after 3.199994659423828 seconds, killing
2023-06-26 16:49:49,275 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 16:49:49,275 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:49:49,275 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:49:49,276 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:49:49,277 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:49:49,279 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:49:49,280 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:49:49,280 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:49:49,281 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:49:49,282 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:49:49,282 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:49:49,283 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:49:49,283 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:49:49,283 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:49:49,284 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:49:50,066 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,068 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,068 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,069 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,069 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,069 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,070 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,071 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,071 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,071 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,072 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,072 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,072 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,074 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,074 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,075 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:49:50,076 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=277005 parent=273801 started daemon>
2023-06-26 16:49:50,076 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=277002 parent=273801 started daemon>
2023-06-26 16:49:50,076 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276999 parent=273801 started daemon>
2023-06-26 16:49:50,076 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276993 parent=273801 started daemon>
2023-06-26 16:49:50,076 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276989 parent=273801 started daemon>
2023-06-26 16:49:50,076 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276986 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276983 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276980 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276975 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276967 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276948 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276938 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276933 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276906 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276903 parent=273801 started daemon>
2023-06-26 16:49:50,077 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=276878 parent=273801 started daemon>
2023-06-26 16:49:54,940 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 276906 exit status was already read will report exitcode 255
2023-06-26 16:49:55,675 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 276999 exit status was already read will report exitcode 255
2023-06-26 16:49:57,179 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 276986 exit status was already read will report exitcode 255
