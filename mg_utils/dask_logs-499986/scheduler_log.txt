RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 20:38:28,652 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 20:38:29,170 - distributed.scheduler - INFO - State start
2023-06-26 20:38:29,171 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-esk6_7ua', purging
2023-06-26 20:38:29,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pz3rey5o', purging
2023-06-26 20:38:29,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-aruwrlmj', purging
2023-06-26 20:38:29,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ljoxb_co', purging
2023-06-26 20:38:29,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-9p900i2w', purging
2023-06-26 20:38:29,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-oef_1uce', purging
2023-06-26 20:38:29,173 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-z4zculaf', purging
2023-06-26 20:38:29,173 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0b0fhf6l', purging
2023-06-26 20:38:29,173 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-94j7v4l5', purging
2023-06-26 20:38:29,173 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o4xtlkme', purging
2023-06-26 20:38:29,173 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x84__cn3', purging
2023-06-26 20:38:29,174 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-srqrrlf8', purging
2023-06-26 20:38:29,174 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-z99vlz95', purging
2023-06-26 20:38:29,174 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8we79fbq', purging
2023-06-26 20:38:29,174 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rbkv7iu1', purging
2023-06-26 20:38:29,174 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jkleuo2h', purging
2023-06-26 20:38:29,187 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 20:38:29,187 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 20:38:29,188 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 20:38:44,204 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46625', status: init, memory: 0, processing: 0>
2023-06-26 20:38:44,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46625
2023-06-26 20:38:44,207 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44612
2023-06-26 20:38:48,382 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41435', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,383 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41435
2023-06-26 20:38:48,383 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44626
2023-06-26 20:38:48,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34591', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,478 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34591
2023-06-26 20:38:48,478 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44634
2023-06-26 20:38:48,514 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37465', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,514 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37465
2023-06-26 20:38:48,515 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44644
2023-06-26 20:38:48,519 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34615', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,519 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34615
2023-06-26 20:38:48,519 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44656
2023-06-26 20:38:48,528 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40725', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,528 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40725
2023-06-26 20:38:48,528 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44668
2023-06-26 20:38:48,544 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35485', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,544 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35485
2023-06-26 20:38:48,544 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44682
2023-06-26 20:38:48,587 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39169', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,587 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39169
2023-06-26 20:38:48,587 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44698
2023-06-26 20:38:48,588 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33431', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,588 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33431
2023-06-26 20:38:48,588 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44696
2023-06-26 20:38:48,605 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36659', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,606 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36659
2023-06-26 20:38:48,606 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44702
2023-06-26 20:38:48,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41737', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,607 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41737
2023-06-26 20:38:48,607 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44708
2023-06-26 20:38:48,614 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37005', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,615 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37005
2023-06-26 20:38:48,615 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44736
2023-06-26 20:38:48,615 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37793', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,615 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37793
2023-06-26 20:38:48,616 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44722
2023-06-26 20:38:48,616 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46443', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,616 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46443
2023-06-26 20:38:48,616 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44742
2023-06-26 20:38:48,622 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38391', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,622 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38391
2023-06-26 20:38:48,623 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44756
2023-06-26 20:38:48,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36245', status: init, memory: 0, processing: 0>
2023-06-26 20:38:48,626 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36245
2023-06-26 20:38:48,626 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:44772
2023-06-26 20:38:52,660 - distributed.scheduler - INFO - Receive client connection: Client-767030e2-1461-11ee-a285-5cff35c1a711
2023-06-26 20:38:52,661 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:42968
2023-06-26 20:38:53,357 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 20:39:45,129 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,880 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 20:40:37,882 - distributed.scheduler - INFO - Restarting workers and releasing all keys.
2023-06-26 20:40:37,906 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44696; closing.
2023-06-26 20:40:37,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33431', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,907 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,909 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44634; closing.
2023-06-26 20:40:37,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34591', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,909 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34591
2023-06-26 20:40:37,909 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44656; closing.
2023-06-26 20:40:37,910 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34615', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,910 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34615
2023-06-26 20:40:37,910 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44682; closing.
2023-06-26 20:40:37,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35485', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,911 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35485
2023-06-26 20:40:37,911 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44772; closing.
2023-06-26 20:40:37,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36245', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,911 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36245
2023-06-26 20:40:37,912 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44702; closing.
2023-06-26 20:40:37,912 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44736; closing.
2023-06-26 20:40:37,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36659', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,912 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36659
2023-06-26 20:40:37,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37005', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,912 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37005
2023-06-26 20:40:37,913 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44644; closing.
2023-06-26 20:40:37,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37465', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,913 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37465
2023-06-26 20:40:37,913 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44722; closing.
2023-06-26 20:40:37,914 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37793', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,914 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:40:37,917 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44756; closing.
2023-06-26 20:40:37,918 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38391', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38391
2023-06-26 20:40:37,921 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44698; closing.
2023-06-26 20:40:37,921 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39169', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39169
2023-06-26 20:40:37,929 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44668; closing.
2023-06-26 20:40:37,929 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40725', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,929 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40725
2023-06-26 20:40:37,930 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44742; closing.
2023-06-26 20:40:37,930 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46443', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,930 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46443
2023-06-26 20:40:37,931 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:44742>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:40:37,934 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44612; closing.
2023-06-26 20:40:37,934 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46625', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46625
2023-06-26 20:40:37,935 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44626; closing.
2023-06-26 20:40:37,935 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41435', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,935 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41435
2023-06-26 20:40:37,952 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:44708; closing.
2023-06-26 20:40:37,953 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41737', status: closing, memory: 0, processing: 0>
2023-06-26 20:40:37,953 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41737
2023-06-26 20:40:37,953 - distributed.scheduler - INFO - Lost all workers
2023-06-26 20:40:49,308 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39511', status: init, memory: 0, processing: 0>
2023-06-26 20:40:49,309 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39511
2023-06-26 20:40:49,309 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34608
2023-06-26 20:40:49,329 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37665', status: init, memory: 0, processing: 0>
2023-06-26 20:40:49,329 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37665
2023-06-26 20:40:49,329 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34618
2023-06-26 20:40:49,367 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35203', status: init, memory: 0, processing: 0>
2023-06-26 20:40:49,367 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35203
2023-06-26 20:40:49,367 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34626
2023-06-26 20:40:50,967 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40581', status: init, memory: 0, processing: 0>
2023-06-26 20:40:50,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40581
2023-06-26 20:40:50,967 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34670
2023-06-26 20:40:51,595 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39603', status: init, memory: 0, processing: 0>
2023-06-26 20:40:51,595 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39603
2023-06-26 20:40:51,595 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34678
2023-06-26 20:40:51,945 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41371', status: init, memory: 0, processing: 0>
2023-06-26 20:40:51,945 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41371
2023-06-26 20:40:51,945 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34692
2023-06-26 20:40:52,170 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34317', status: init, memory: 0, processing: 0>
2023-06-26 20:40:52,171 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34317
2023-06-26 20:40:52,171 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34722
2023-06-26 20:40:55,699 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38861', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,700 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38861
2023-06-26 20:40:55,700 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34768
2023-06-26 20:40:55,815 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44555', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,816 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44555
2023-06-26 20:40:55,816 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34776
2023-06-26 20:40:55,871 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36779', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,871 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36779
2023-06-26 20:40:55,871 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34786
2023-06-26 20:40:55,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33803', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33803
2023-06-26 20:40:55,892 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34796
2023-06-26 20:40:55,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41389', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41389
2023-06-26 20:40:55,894 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34800
2023-06-26 20:40:55,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41939', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,916 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41939
2023-06-26 20:40:55,917 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34812
2023-06-26 20:40:55,922 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38757', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,926 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38757
2023-06-26 20:40:55,926 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34820
2023-06-26 20:40:55,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43089', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,954 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43089
2023-06-26 20:40:55,954 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34840
2023-06-26 20:40:55,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36413', status: init, memory: 0, processing: 0>
2023-06-26 20:40:55,955 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36413
2023-06-26 20:40:55,955 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:34836
2023-06-26 20:40:56,019 - distributed.scheduler - INFO - Restarting finished.
2023-06-26 20:41:06,994 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 20:41:37,235 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 20:41:37,237 - distributed.scheduler - INFO - Remove client Client-767030e2-1461-11ee-a285-5cff35c1a711
2023-06-26 20:41:37,237 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:42968; closing.
2023-06-26 20:41:37,237 - distributed.scheduler - INFO - Remove client Client-767030e2-1461-11ee-a285-5cff35c1a711
2023-06-26 20:41:37,237 - distributed.scheduler - INFO - Close client connection: Client-767030e2-1461-11ee-a285-5cff35c1a711
2023-06-26 20:46:12,749 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 20:46:12,749 - distributed.core - INFO - Connection to tcp://10.120.104.11:34812 has been closed.
2023-06-26 20:46:12,750 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41939', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,750 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41939
2023-06-26 20:46:12,752 - distributed.core - INFO - Connection to tcp://10.120.104.11:34626 has been closed.
2023-06-26 20:46:12,752 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35203', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,752 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35203
2023-06-26 20:46:12,753 - distributed.core - INFO - Connection to tcp://10.120.104.11:34678 has been closed.
2023-06-26 20:46:12,753 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39603', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39603
2023-06-26 20:46:12,753 - distributed.core - INFO - Connection to tcp://10.120.104.11:34820 has been closed.
2023-06-26 20:46:12,754 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38757', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,754 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38757
2023-06-26 20:46:12,754 - distributed.core - INFO - Connection to tcp://10.120.104.11:34836 has been closed.
2023-06-26 20:46:12,754 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36413', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,754 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36413
2023-06-26 20:46:12,754 - distributed.core - INFO - Connection to tcp://10.120.104.11:34768 has been closed.
2023-06-26 20:46:12,754 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38861', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,755 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38861
2023-06-26 20:46:12,755 - distributed.core - INFO - Connection to tcp://10.120.104.11:34840 has been closed.
2023-06-26 20:46:12,755 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43089', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,755 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43089
2023-06-26 20:46:12,755 - distributed.core - INFO - Connection to tcp://10.120.104.11:34618 has been closed.
2023-06-26 20:46:12,755 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37665', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,755 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37665
2023-06-26 20:46:12,756 - distributed.core - INFO - Connection to tcp://10.120.104.11:34670 has been closed.
2023-06-26 20:46:12,756 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40581', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,756 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40581
2023-06-26 20:46:12,756 - distributed.core - INFO - Connection to tcp://10.120.104.11:34786 has been closed.
2023-06-26 20:46:12,756 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36779', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,756 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36779
2023-06-26 20:46:12,757 - distributed.core - INFO - Connection to tcp://10.120.104.11:34800 has been closed.
2023-06-26 20:46:12,757 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41389', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,757 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41389
2023-06-26 20:46:12,757 - distributed.core - INFO - Connection to tcp://10.120.104.11:34722 has been closed.
2023-06-26 20:46:12,757 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34317', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,757 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34317
2023-06-26 20:46:12,757 - distributed.core - INFO - Connection to tcp://10.120.104.11:34796 has been closed.
2023-06-26 20:46:12,757 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33803', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,757 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33803
2023-06-26 20:46:12,758 - distributed.core - INFO - Connection to tcp://10.120.104.11:34776 has been closed.
2023-06-26 20:46:12,758 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44555', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,758 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44555
2023-06-26 20:46:12,759 - distributed.core - INFO - Connection to tcp://10.120.104.11:34692 has been closed.
2023-06-26 20:46:12,759 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41371', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41371
2023-06-26 20:46:12,759 - distributed.core - INFO - Connection to tcp://10.120.104.11:34608 has been closed.
2023-06-26 20:46:12,759 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39511', status: running, memory: 0, processing: 0>
2023-06-26 20:46:12,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39511
2023-06-26 20:46:12,760 - distributed.scheduler - INFO - Lost all workers
2023-06-26 20:46:12,760 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 20:46:12,760 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34796>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,760 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34722>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,760 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34626>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,761 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34836>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,761 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,761 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34618>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,761 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34820>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,761 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34768>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,761 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34608>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34608>: Stream is closed
2023-06-26 20:46:12,762 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34678>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,762 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34670>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,762 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34692>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34692>: Stream is closed
2023-06-26 20:46:12,762 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34800>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,762 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34840>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,763 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:34776>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,764 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 20:46:12,768 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 20:46:12,769 - distributed.scheduler - INFO - End scheduler
