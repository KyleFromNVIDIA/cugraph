RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 20:38:35,743 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37285'
2023-06-26 20:38:35,746 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35717'
2023-06-26 20:38:35,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36135'
2023-06-26 20:38:35,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37083'
2023-06-26 20:38:35,752 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35875'
2023-06-26 20:38:35,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36709'
2023-06-26 20:38:35,756 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34221'
2023-06-26 20:38:35,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39911'
2023-06-26 20:38:35,760 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45091'
2023-06-26 20:38:35,763 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40145'
2023-06-26 20:38:35,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46591'
2023-06-26 20:38:35,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45691'
2023-06-26 20:38:35,773 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40515'
2023-06-26 20:38:35,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40537'
2023-06-26 20:38:35,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43499'
2023-06-26 20:38:35,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39549'
2023-06-26 20:38:37,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,442 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,442 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,488 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,496 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,496 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,498 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,498 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,499 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:38:37,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:38:37,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,587 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,621 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,635 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,658 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,668 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,678 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,692 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,694 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:37,695 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:38:43,841 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46625
2023-06-26 20:38:43,841 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46625
2023-06-26 20:38:43,841 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40861
2023-06-26 20:38:43,841 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:43,841 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:43,841 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:43,841 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:43,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qyh0nb_m
2023-06-26 20:38:43,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ff2ebd8-9d43-462e-8d92-3a90d2b0a4e7
2023-06-26 20:38:43,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a79c163e-c160-4c1e-877e-73e839334268
2023-06-26 20:38:44,196 - distributed.worker - INFO - Starting Worker plugin PreImport-1a5b88e5-b25e-40a1-bf98-0da4236b0f7f
2023-06-26 20:38:44,196 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,207 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,207 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,211 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:44,799 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46443
2023-06-26 20:38:44,799 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46443
2023-06-26 20:38:44,799 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38333
2023-06-26 20:38:44,800 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,800 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,800 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,800 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,800 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7aagwktv
2023-06-26 20:38:44,800 - distributed.worker - INFO - Starting Worker plugin RMMSetup-037cb9c1-ea76-47f1-890d-91fa745ca44d
2023-06-26 20:38:44,809 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39169
2023-06-26 20:38:44,810 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39169
2023-06-26 20:38:44,810 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37495
2023-06-26 20:38:44,810 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,810 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,810 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,810 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,810 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0eg17ir6
2023-06-26 20:38:44,810 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80fd854e-51f4-42eb-9081-dc1a54b81dae
2023-06-26 20:38:44,833 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37005
2023-06-26 20:38:44,833 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37005
2023-06-26 20:38:44,833 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39423
2023-06-26 20:38:44,833 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,833 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,834 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,834 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zpmpjggf
2023-06-26 20:38:44,834 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-add9213e-4de1-4a8e-9f64-b25cf9c5ecff
2023-06-26 20:38:44,835 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6067622-245d-4f39-aa91-24c1a9ee1828
2023-06-26 20:38:44,839 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34591
2023-06-26 20:38:44,839 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34591
2023-06-26 20:38:44,839 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39967
2023-06-26 20:38:44,839 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,839 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,839 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,839 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,839 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-62stetb3
2023-06-26 20:38:44,840 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f736a78a-8208-451f-a130-55200407996b
2023-06-26 20:38:44,853 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41737
2023-06-26 20:38:44,853 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41737
2023-06-26 20:38:44,853 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33357
2023-06-26 20:38:44,853 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,853 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,853 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,853 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,853 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dwz6bq_z
2023-06-26 20:38:44,854 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c5a4b4f-3dab-4410-84ea-be50e085e33a
2023-06-26 20:38:44,857 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33431
2023-06-26 20:38:44,857 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33431
2023-06-26 20:38:44,857 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40799
2023-06-26 20:38:44,858 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,858 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,858 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,858 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,858 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e_fn7m69
2023-06-26 20:38:44,858 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efff5eb0-be3a-4808-8ae0-44c744a35696
2023-06-26 20:38:44,876 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36659
2023-06-26 20:38:44,876 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36659
2023-06-26 20:38:44,876 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42141
2023-06-26 20:38:44,877 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,877 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,877 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,877 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,877 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ctrgp5bz
2023-06-26 20:38:44,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0666aa9c-031c-4673-821e-1404c5311f36
2023-06-26 20:38:44,881 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38391
2023-06-26 20:38:44,881 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38391
2023-06-26 20:38:44,881 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43483
2023-06-26 20:38:44,881 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,881 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,881 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,881 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,881 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ldt7a1ha
2023-06-26 20:38:44,882 - distributed.worker - INFO - Starting Worker plugin PreImport-aa4900cf-be67-430b-8224-1ee428b02853
2023-06-26 20:38:44,882 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37a26e6d-6632-4307-ba60-4bde1c0038b5
2023-06-26 20:38:44,894 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40725
2023-06-26 20:38:44,895 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40725
2023-06-26 20:38:44,895 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44049
2023-06-26 20:38:44,895 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,895 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,895 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,895 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,895 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2yh2xx6a
2023-06-26 20:38:44,895 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34615
2023-06-26 20:38:44,896 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34615
2023-06-26 20:38:44,896 - distributed.worker - INFO - Starting Worker plugin PreImport-1016fd45-4a9d-4d38-bdbd-19ee5ee0d786
2023-06-26 20:38:44,896 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45169
2023-06-26 20:38:44,896 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,896 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,896 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd94f91b-3a9e-4eb9-9d11-a71481f38dcd
2023-06-26 20:38:44,896 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,896 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,896 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v_ad2ed1
2023-06-26 20:38:44,897 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd7ae87c-bcb7-421d-ae4e-a32d6baf8a04
2023-06-26 20:38:44,898 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37793
2023-06-26 20:38:44,898 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37793
2023-06-26 20:38:44,898 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36651
2023-06-26 20:38:44,898 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,898 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,898 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,898 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4m74xoy7
2023-06-26 20:38:44,898 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35485
2023-06-26 20:38:44,898 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35485
2023-06-26 20:38:44,898 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41467
2023-06-26 20:38:44,898 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,898 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,898 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,898 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mtpap_a0
2023-06-26 20:38:44,898 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d57fcbe-42d8-4bfb-abef-acdb86bdb211
2023-06-26 20:38:44,899 - distributed.worker - INFO - Starting Worker plugin PreImport-9508ac8b-8fff-4bcf-aa26-599ba1e3fc50
2023-06-26 20:38:44,899 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e22abbc8-1bb0-474a-84df-b854c9ffeefe
2023-06-26 20:38:44,955 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37465
2023-06-26 20:38:44,955 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37465
2023-06-26 20:38:44,955 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36413
2023-06-26 20:38:44,955 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,955 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,955 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,955 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,955 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sh0t7kwa
2023-06-26 20:38:44,956 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fcaff3f4-3f1e-497e-9afe-44a4b6ea8ba4
2023-06-26 20:38:44,956 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77395cf0-f749-4e70-8a26-1ac96780b5a4
2023-06-26 20:38:44,959 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41435
2023-06-26 20:38:44,959 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41435
2023-06-26 20:38:44,959 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45389
2023-06-26 20:38:44,959 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,959 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,959 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,959 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,959 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xktutlyg
2023-06-26 20:38:44,959 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a22b7bd0-eee3-40af-bbab-ee0276624b6c
2023-06-26 20:38:44,961 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36245
2023-06-26 20:38:44,962 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36245
2023-06-26 20:38:44,962 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37173
2023-06-26 20:38:44,962 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:38:44,962 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:44,962 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:38:44,962 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:38:44,962 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sz6us6xv
2023-06-26 20:38:44,964 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e13e885-7593-46f6-b5b2-65dc26f2591a
2023-06-26 20:38:48,352 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4da7ba2c-af49-4212-b6cf-343a2fdc3996
2023-06-26 20:38:48,353 - distributed.worker - INFO - Starting Worker plugin PreImport-7f26e829-8bdc-474f-973b-a209fd4a15aa
2023-06-26 20:38:48,355 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,383 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,383 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,385 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,453 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d445e2a-6828-439d-9aae-f8cd17419fc4
2023-06-26 20:38:48,453 - distributed.worker - INFO - Starting Worker plugin PreImport-c722ab05-0c25-4984-bc06-484cde0f2c7c
2023-06-26 20:38:48,454 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,478 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,478 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,481 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,488 - distributed.worker - INFO - Starting Worker plugin PreImport-156a507e-20e7-4271-afda-2269679709ac
2023-06-26 20:38:48,490 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16ad6466-b9b4-451a-a3a6-516f56e6fe3f
2023-06-26 20:38:48,501 - distributed.worker - INFO - Starting Worker plugin PreImport-b316a832-7a9a-41b1-9ea4-91fdba7d576b
2023-06-26 20:38:48,502 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,505 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d990508-c34a-4cca-9e34-d06d4fa07f69
2023-06-26 20:38:48,506 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,515 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,515 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,517 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,519 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,519 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,522 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,524 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ac266f0-b182-48de-90d5-c66ab3bd6bda
2023-06-26 20:38:48,525 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,528 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,528 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,531 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,544 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,545 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,546 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5138e7f-41bb-4311-b807-c4c31ca40d4a
2023-06-26 20:38:48,570 - distributed.worker - INFO - Starting Worker plugin PreImport-8b0a43ca-a0e7-403d-97b9-42f2c7e62980
2023-06-26 20:38:48,571 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,572 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22e47b8b-3322-4b8c-9eaa-12f897e2e304
2023-06-26 20:38:48,572 - distributed.worker - INFO - Starting Worker plugin PreImport-ddb3084e-2bd6-44b1-8208-58a2ab588c5d
2023-06-26 20:38:48,573 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,587 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,587 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,589 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,589 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,589 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,589 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20989260-09cd-4d01-ad2b-9ecdb978f3e3
2023-06-26 20:38:48,590 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,590 - distributed.worker - INFO - Starting Worker plugin PreImport-edb13d2f-8727-43ce-9a65-57e2a532b14f
2023-06-26 20:38:48,591 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-988664da-2814-46ab-9dee-31f3ac445493
2023-06-26 20:38:48,593 - distributed.worker - INFO - Starting Worker plugin PreImport-7de4da1d-9ae6-4311-bcba-f368a2819bc9
2023-06-26 20:38:48,594 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bf5759d-9ffd-4574-ae93-b4d328b98010
2023-06-26 20:38:48,594 - distributed.worker - INFO - Starting Worker plugin PreImport-766cc648-5c1b-495b-9936-67414f6961ef
2023-06-26 20:38:48,595 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,595 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,599 - distributed.worker - INFO - Starting Worker plugin PreImport-c6b3b450-3122-4925-a2d3-7e82db8c3435
2023-06-26 20:38:48,600 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ceb0e52-d233-4f2c-bd3e-c15812269458
2023-06-26 20:38:48,603 - distributed.worker - INFO - Starting Worker plugin PreImport-4dd0b6df-9225-412c-a79d-f816ce8cf7f6
2023-06-26 20:38:48,604 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,606 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,606 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,607 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,607 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,607 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0adf46c7-549d-4ab0-aaff-fe5b4a4b3e89
2023-06-26 20:38:48,607 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,608 - distributed.worker - INFO - Starting Worker plugin PreImport-32dd6439-bdc2-4b8a-b830-880e7da6384e
2023-06-26 20:38:48,608 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,610 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c6f4607-6a3a-4b3c-89b5-93c5cc7449ec
2023-06-26 20:38:48,611 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,615 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,615 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,616 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,616 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,617 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,617 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,617 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,618 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,619 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,623 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,623 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,624 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:48,626 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:38:48,626 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:38:48,628 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:38:52,676 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,676 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,676 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,676 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,677 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,677 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,677 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,677 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,677 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,679 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,680 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,680 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,680 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,680 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,681 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,681 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,693 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,694 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,694 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,694 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,694 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,694 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,694 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:52,694 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:38:53,369 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,369 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,369 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:38:53,370 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:39:09,016 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,041 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,183 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,244 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,285 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,368 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,449 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,494 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,506 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,507 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,519 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,533 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,603 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,617 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,635 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:09,668 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:39:16,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:16,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:39:55,140 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,141 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,141 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,149 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,149 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,149 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,149 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,150 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,150 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,150 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,150 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,150 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,152 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,153 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,153 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:55,153 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:39:58,398 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,406 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,406 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,406 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,406 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,406 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,406 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:39:58,407 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:05,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:05,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:33,328 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,328 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,328 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,328 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,328 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,328 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,329 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:40:33,341 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,342 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:40:33,357 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,357 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:33,358 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:40:37,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:40:37,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,905 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33431. Reason: scheduler-restart
2023-06-26 20:40:37,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,906 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,906 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34591. Reason: scheduler-restart
2023-06-26 20:40:37,907 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34615. Reason: scheduler-restart
2023-06-26 20:40:37,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,908 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,908 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35485. Reason: scheduler-restart
2023-06-26 20:40:37,908 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,909 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,909 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,909 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36245. Reason: scheduler-restart
2023-06-26 20:40:37,909 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,909 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36659. Reason: scheduler-restart
2023-06-26 20:40:37,909 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 20:40:37,909 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37005. Reason: scheduler-restart
2023-06-26 20:40:37,909 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,910 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,910 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,910 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,910 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37465. Reason: scheduler-restart
2023-06-26 20:40:37,910 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,910 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,910 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,910 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,910 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37793. Reason: scheduler-restart
2023-06-26 20:40:37,911 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,911 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38391. Reason: scheduler-restart
2023-06-26 20:40:37,911 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,911 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,911 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,912 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,912 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,912 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,912 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,912 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,913 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,913 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:40:37,915 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41435. Reason: scheduler-restart
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34591
2023-06-26 20:40:37,918 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39169. Reason: scheduler-restart
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34615
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35485
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36245
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36659
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37005
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37465
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:40:37,918 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34591
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34615
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35485
2023-06-26 20:40:37,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36245
2023-06-26 20:40:37,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36659
2023-06-26 20:40:37,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37005
2023-06-26 20:40:37,919 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37465
2023-06-26 20:40:37,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:40:37,919 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,920 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40725. Reason: scheduler-restart
2023-06-26 20:40:37,920 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34591
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34615
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35485
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36245
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36659
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38391
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37005
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37465
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:40:37,921 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,921 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38391
2023-06-26 20:40:37,927 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46625. Reason: scheduler-restart
2023-06-26 20:40:37,928 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46443. Reason: scheduler-restart
2023-06-26 20:40:37,929 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,929 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39169
2023-06-26 20:40:37,929 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34591
2023-06-26 20:40:37,929 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34615
2023-06-26 20:40:37,929 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35485
2023-06-26 20:40:37,929 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36245
2023-06-26 20:40:37,930 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36659
2023-06-26 20:40:37,930 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37005
2023-06-26 20:40:37,930 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37465
2023-06-26 20:40:37,930 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:40:37,930 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38391
2023-06-26 20:40:37,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39169
2023-06-26 20:40:37,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34591
2023-06-26 20:40:37,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34615
2023-06-26 20:40:37,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35485
2023-06-26 20:40:37,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36245
2023-06-26 20:40:37,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36659
2023-06-26 20:40:37,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37005
2023-06-26 20:40:37,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37465
2023-06-26 20:40:37,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:40:37,933 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38391
2023-06-26 20:40:37,933 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34591
2023-06-26 20:40:37,933 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34615
2023-06-26 20:40:37,933 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35485
2023-06-26 20:40:37,933 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36245
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36659
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37005
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37465
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39169
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40725
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38391
2023-06-26 20:40:37,934 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39169
2023-06-26 20:40:37,935 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40725
2023-06-26 20:40:37,935 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,935 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,935 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,937 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,939 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,940 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41737. Reason: scheduler-restart
2023-06-26 20:40:37,941 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34591
2023-06-26 20:40:37,941 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34615
2023-06-26 20:40:37,941 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35485
2023-06-26 20:40:37,941 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36245
2023-06-26 20:40:37,941 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36659
2023-06-26 20:40:37,942 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37005
2023-06-26 20:40:37,942 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37465
2023-06-26 20:40:37,942 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:40:37,952 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,952 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38391
2023-06-26 20:40:37,953 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39169
2023-06-26 20:40:37,955 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40725
2023-06-26 20:40:37,955 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:40:37,957 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:37,963 - distributed.nanny - INFO - Worker closed
2023-06-26 20:40:39,830 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:40,292 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:41,794 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:41,794 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:41,813 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:41,968 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:43,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:43,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:43,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:43,710 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:43,710 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:43,713 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,105 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:44,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:44,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:44,338 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,339 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,359 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,360 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,362 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,364 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,369 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,370 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:44,376 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:40:45,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:45,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:45,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:45,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:45,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:45,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:45,480 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:45,495 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:45,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:45,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:45,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:45,890 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,033 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37665
2023-06-26 20:40:46,033 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37665
2023-06-26 20:40:46,033 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44263
2023-06-26 20:40:46,033 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:46,033 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:46,033 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:46,033 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:46,033 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9goer3sb
2023-06-26 20:40:46,034 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-307fc6fd-ca7c-4990-b82a-1261d92ab0a7
2023-06-26 20:40:46,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9471aa7-fc90-466e-848e-62e312c46c3d
2023-06-26 20:40:46,051 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39511
2023-06-26 20:40:46,052 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39511
2023-06-26 20:40:46,052 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44195
2023-06-26 20:40:46,052 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:46,052 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:46,052 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:46,052 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:46,052 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e5t0wzq0
2023-06-26 20:40:46,052 - distributed.worker - INFO - Starting Worker plugin PreImport-a488a847-8872-4fa0-aa6a-58cf3f9abe9f
2023-06-26 20:40:46,053 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce438785-7465-464e-aacb-a8bb53126e6d
2023-06-26 20:40:46,059 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35203
2023-06-26 20:40:46,059 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35203
2023-06-26 20:40:46,060 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45293
2023-06-26 20:40:46,060 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:46,060 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:46,060 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:46,060 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:46,060 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3mfw94fi
2023-06-26 20:40:46,060 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b1dc6a6-c1bd-4fd1-b879-b1881b330fe3
2023-06-26 20:40:46,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,181 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,181 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,210 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,210 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,240 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,240 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,242 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,251 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:40:46,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:40:46,258 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,359 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,389 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,416 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:46,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:40:48,027 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40581
2023-06-26 20:40:48,027 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40581
2023-06-26 20:40:48,028 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36929
2023-06-26 20:40:48,028 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:48,028 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:48,028 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:48,028 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:48,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ag2poqy
2023-06-26 20:40:48,028 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62adfe22-7400-4c4a-b9de-6afac3ed9431
2023-06-26 20:40:48,189 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39603
2023-06-26 20:40:48,189 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39603
2023-06-26 20:40:48,189 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33467
2023-06-26 20:40:48,189 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:48,189 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:48,189 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:48,189 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:48,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-59ymt6yl
2023-06-26 20:40:48,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e85381aa-fba8-498e-94fa-05b362b6f95d
2023-06-26 20:40:48,831 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41371
2023-06-26 20:40:48,831 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41371
2023-06-26 20:40:48,831 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40819
2023-06-26 20:40:48,831 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:48,831 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:48,831 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:48,832 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:48,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1y5lxc1b
2023-06-26 20:40:48,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3bf26d5a-d117-435b-8b1f-e79b4a3730af
2023-06-26 20:40:49,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd4e6fab-d476-4a4a-99b2-8201eff9d112
2023-06-26 20:40:49,294 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:49,309 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:49,309 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:49,310 - distributed.worker - INFO - Starting Worker plugin PreImport-21f7489a-edc3-4865-bca7-0dba4405fa9b
2023-06-26 20:40:49,311 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:49,312 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:49,330 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:49,330 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:49,332 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:49,355 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f95f958a-4e6d-4cdc-8cf8-d32b5a703578
2023-06-26 20:40:49,356 - distributed.worker - INFO - Starting Worker plugin PreImport-ef1fba98-5acc-4ab7-830d-b92ed39767b0
2023-06-26 20:40:49,356 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:49,367 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:49,367 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:49,371 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:50,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4fa8f11-275e-4a15-b4ca-dc7da1074a54
2023-06-26 20:40:50,933 - distributed.worker - INFO - Starting Worker plugin PreImport-3ce127f0-6380-438e-94b2-8e71334f5f99
2023-06-26 20:40:50,934 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:50,967 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:50,967 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:50,969 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:51,275 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34317
2023-06-26 20:40:51,275 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34317
2023-06-26 20:40:51,276 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40263
2023-06-26 20:40:51,276 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:51,276 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:51,276 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:51,276 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:51,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-igjdfwde
2023-06-26 20:40:51,277 - distributed.worker - INFO - Starting Worker plugin RMMSetup-16d3a6f1-6d34-4859-af45-c8de705392b2
2023-06-26 20:40:51,565 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e3c341c-94f9-4bde-8f32-c762b5b2b791
2023-06-26 20:40:51,566 - distributed.worker - INFO - Starting Worker plugin PreImport-a0009ede-b403-463f-b1a5-e6e50700a85b
2023-06-26 20:40:51,567 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:51,596 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:51,596 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:51,599 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:51,935 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf91b14c-9bc1-4bd3-8011-1cf7ff9d91bf
2023-06-26 20:40:51,935 - distributed.worker - INFO - Starting Worker plugin PreImport-bac01faa-291e-4447-a5ed-bc9b5267e41b
2023-06-26 20:40:51,936 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:51,945 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:51,945 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:51,947 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:52,158 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-674f4840-7aad-4914-a054-10f036fc4caa
2023-06-26 20:40:52,158 - distributed.worker - INFO - Starting Worker plugin PreImport-0444df16-a4ab-48fb-b053-e1ab200b3fb8
2023-06-26 20:40:52,159 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:52,171 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:52,171 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:52,179 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:53,608 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44555
2023-06-26 20:40:53,608 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44555
2023-06-26 20:40:53,608 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40273
2023-06-26 20:40:53,608 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,608 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,608 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,608 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,608 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0dm485e4
2023-06-26 20:40:53,609 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f640c805-9769-4083-94b7-8aabfb867df9
2023-06-26 20:40:53,612 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38861
2023-06-26 20:40:53,612 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38861
2023-06-26 20:40:53,612 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32919
2023-06-26 20:40:53,612 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,612 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,612 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,612 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iexb9a_p
2023-06-26 20:40:53,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54c51307-7083-47a7-a87e-a5b6e4410f16
2023-06-26 20:40:53,638 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33803
2023-06-26 20:40:53,638 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33803
2023-06-26 20:40:53,638 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34437
2023-06-26 20:40:53,639 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,639 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,639 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,639 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wmzk2yic
2023-06-26 20:40:53,640 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53bcde46-4164-4048-8db7-583ffc445cc1
2023-06-26 20:40:53,640 - distributed.worker - INFO - Starting Worker plugin PreImport-43a8277e-a78d-41f1-968d-63bb4efc2267
2023-06-26 20:40:53,640 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dde3be63-3cd4-4add-b182-3a417de95114
2023-06-26 20:40:53,655 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41389
2023-06-26 20:40:53,656 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41389
2023-06-26 20:40:53,656 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45283
2023-06-26 20:40:53,656 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,656 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,656 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,656 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,656 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5f8iu2gq
2023-06-26 20:40:53,656 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd1171ae-7ae4-47db-a24d-c827bd5fcb12
2023-06-26 20:40:53,665 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41939
2023-06-26 20:40:53,666 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41939
2023-06-26 20:40:53,666 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46867
2023-06-26 20:40:53,666 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,666 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,666 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,666 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3af2g0ib
2023-06-26 20:40:53,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49c08c2d-0c95-4327-ba9f-85f21c10ce42
2023-06-26 20:40:53,683 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38757
2023-06-26 20:40:53,684 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38757
2023-06-26 20:40:53,684 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33883
2023-06-26 20:40:53,684 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,684 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,684 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,684 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,684 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dstldu4w
2023-06-26 20:40:53,684 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4722db0-d04e-4a35-b5ca-e30fdde78451
2023-06-26 20:40:53,685 - distributed.worker - INFO - Starting Worker plugin PreImport-08b29193-d093-4364-9014-a5b73a54b383
2023-06-26 20:40:53,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-125f2aa4-fac2-4243-aa5b-7da2dc72f27c
2023-06-26 20:40:53,688 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36779
2023-06-26 20:40:53,688 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36779
2023-06-26 20:40:53,688 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40693
2023-06-26 20:40:53,688 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,689 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,689 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,689 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c3t59m1p
2023-06-26 20:40:53,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e5c6ab9f-e1ce-4b1c-b5f0-fc3cf12dbcdb
2023-06-26 20:40:53,689 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36413
2023-06-26 20:40:53,689 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36413
2023-06-26 20:40:53,689 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37247
2023-06-26 20:40:53,689 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,689 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,689 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,690 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,690 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cwmvfgvj
2023-06-26 20:40:53,691 - distributed.worker - INFO - Starting Worker plugin PreImport-3345f11d-bcc6-4475-9c29-136352290f33
2023-06-26 20:40:53,691 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cfa6d1f-b6ae-4a20-ac8c-224e1ed2c114
2023-06-26 20:40:53,694 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43089
2023-06-26 20:40:53,694 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43089
2023-06-26 20:40:53,695 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39309
2023-06-26 20:40:53,695 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:40:53,695 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:53,695 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:40:53,695 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:40:53,695 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6_qcszx_
2023-06-26 20:40:53,696 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-080adf9c-6d3e-44ca-afd3-6aeebaa0e7db
2023-06-26 20:40:53,697 - distributed.worker - INFO - Starting Worker plugin PreImport-35684d7a-37ae-4ab3-80b0-8ede8c3fd497
2023-06-26 20:40:53,697 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebbcb8e9-e983-4ef7-9940-39bf29046eb4
2023-06-26 20:40:55,677 - distributed.worker - INFO - Starting Worker plugin PreImport-27281c7f-972b-407f-88fb-756cf62f00fb
2023-06-26 20:40:55,678 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3417bb3e-e2ba-41cf-ba49-3c933d84c90c
2023-06-26 20:40:55,679 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,700 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,700 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,703 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:55,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4580e096-fd83-4ace-90a9-d7dbaae9b9ed
2023-06-26 20:40:55,797 - distributed.worker - INFO - Starting Worker plugin PreImport-1212439d-ef56-4e3b-ad56-ff8ce6e02583
2023-06-26 20:40:55,798 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,816 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,816 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,819 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:55,857 - distributed.worker - INFO - Starting Worker plugin PreImport-12d53528-f7da-4a05-9d64-44d46dad06dc
2023-06-26 20:40:55,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4270cc3-965b-4a8e-b5dc-0b9bdf3e8c84
2023-06-26 20:40:55,858 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,871 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,871 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,873 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:55,874 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,881 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c64bd6f-eb49-45de-ae22-d861b36f4878
2023-06-26 20:40:55,882 - distributed.worker - INFO - Starting Worker plugin PreImport-7f9121b8-7275-429a-b489-2a3038a551e7
2023-06-26 20:40:55,882 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,892 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,892 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,894 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,894 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,895 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:55,896 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:55,901 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ee6773d-6fe6-47cb-a9de-5950d96e9a51
2023-06-26 20:40:55,902 - distributed.worker - INFO - Starting Worker plugin PreImport-6a52a8c6-b16d-470b-9556-b3f3ab25ab39
2023-06-26 20:40:55,903 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,904 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,917 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,917 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,918 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:55,927 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,927 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,929 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:55,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fa14a41-5bc5-44cd-a923-5ddcb327c644
2023-06-26 20:40:55,934 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,938 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,955 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,955 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,955 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:40:55,955 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:40:55,956 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:40:55,957 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:41:05,381 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,398 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,421 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,489 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,575 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,654 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,665 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,727 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,761 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,846 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,864 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,871 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,888 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,921 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:05,956 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:05,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:06,968 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:41:06,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,980 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,981 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,981 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,981 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,981 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,981 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,990 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:41:06,991 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687812066.991057] [exp01:503250:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:40408) failed: Address already in use
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,001 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,002 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,002 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,002 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:07,002 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:41:10,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:10,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:14,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:21,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:24,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:41:24,996 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,996 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,996 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:24,997 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:41:36,801 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41371. Reason: worker-close
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41939. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35203. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39603. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40581. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43089. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38757. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36413. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38861. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39511. Reason: worker-close
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37665. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33803. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36779. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41389. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34317. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44555. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:46:12,748 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37285'. Reason: nanny-close
2023-06-26 20:46:12,749 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,750 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35717'. Reason: nanny-close
2023-06-26 20:46:12,749 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:34692 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,750 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36135'. Reason: nanny-close
2023-06-26 20:46:12,751 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,749 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:34608 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:46:12,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37083'. Reason: nanny-close
2023-06-26 20:46:12,751 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,752 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35875'. Reason: nanny-close
2023-06-26 20:46:12,752 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,752 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36709'. Reason: nanny-close
2023-06-26 20:46:12,752 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,753 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34221'. Reason: nanny-close
2023-06-26 20:46:12,753 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,753 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39911'. Reason: nanny-close
2023-06-26 20:46:12,753 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45091'. Reason: nanny-close
2023-06-26 20:46:12,754 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40145'. Reason: nanny-close
2023-06-26 20:46:12,754 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46591'. Reason: nanny-close
2023-06-26 20:46:12,757 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,757 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45691'. Reason: nanny-close
2023-06-26 20:46:12,757 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40515'. Reason: nanny-close
2023-06-26 20:46:12,758 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40537'. Reason: nanny-close
2023-06-26 20:46:12,758 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43499'. Reason: nanny-close
2023-06-26 20:46:12,759 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,759 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39549'. Reason: nanny-close
2023-06-26 20:46:12,759 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:46:12,766 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39911 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47398 remote=tcp://10.120.104.11:39911>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39911 after 100 s
2023-06-26 20:46:12,771 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40145 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:58108 remote=tcp://10.120.104.11:40145>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40145 after 100 s
2023-06-26 20:46:12,772 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35717 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41512 remote=tcp://10.120.104.11:35717>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35717 after 100 s
2023-06-26 20:46:12,772 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37083 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46392 remote=tcp://10.120.104.11:37083>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37083 after 100 s
2023-06-26 20:46:12,772 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35875 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:38590 remote=tcp://10.120.104.11:35875>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35875 after 100 s
2023-06-26 20:46:12,772 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34221 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59214 remote=tcp://10.120.104.11:34221>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34221 after 100 s
2023-06-26 20:46:12,773 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36709 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36648 remote=tcp://10.120.104.11:36709>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36709 after 100 s
2023-06-26 20:46:12,775 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45091 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35104 remote=tcp://10.120.104.11:45091>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45091 after 100 s
2023-06-26 20:46:12,777 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39549 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41488 remote=tcp://10.120.104.11:39549>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39549 after 100 s
2023-06-26 20:46:12,778 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40515 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:58994 remote=tcp://10.120.104.11:40515>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40515 after 100 s
2023-06-26 20:46:12,778 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40537 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37726 remote=tcp://10.120.104.11:40537>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40537 after 100 s
2023-06-26 20:46:12,779 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45691 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:49698 remote=tcp://10.120.104.11:45691>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45691 after 100 s
2023-06-26 20:46:12,779 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46591 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60382 remote=tcp://10.120.104.11:46591>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46591 after 100 s
2023-06-26 20:46:12,779 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43499 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40318 remote=tcp://10.120.104.11:43499>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43499 after 100 s
2023-06-26 20:46:12,783 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36135 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42626 remote=tcp://10.120.104.11:36135>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36135 after 100 s
2023-06-26 20:46:15,961 - distributed.nanny - WARNING - Worker process still alive after 3.1999960327148442 seconds, killing
2023-06-26 20:46:15,961 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,961 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,962 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:46:15,962 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,963 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,963 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,964 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,965 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,965 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,965 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:46:15,966 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,966 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 20:46:15,967 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,967 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:46:15,968 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:46:16,750 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,752 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,752 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,752 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,753 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,753 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,753 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,754 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,754 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,755 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,758 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,758 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,758 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,759 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,759 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,760 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:46:16,762 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503295 parent=500140 started daemon>
2023-06-26 20:46:16,762 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503292 parent=500140 started daemon>
2023-06-26 20:46:16,762 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503289 parent=500140 started daemon>
2023-06-26 20:46:16,762 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503286 parent=500140 started daemon>
2023-06-26 20:46:16,762 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503283 parent=500140 started daemon>
2023-06-26 20:46:16,762 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503280 parent=500140 started daemon>
2023-06-26 20:46:16,762 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503277 parent=500140 started daemon>
2023-06-26 20:46:16,762 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503274 parent=500140 started daemon>
2023-06-26 20:46:16,763 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503271 parent=500140 started daemon>
2023-06-26 20:46:16,763 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503265 parent=500140 started daemon>
2023-06-26 20:46:16,763 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503253 parent=500140 started daemon>
2023-06-26 20:46:16,763 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503250 parent=500140 started daemon>
2023-06-26 20:46:16,763 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503247 parent=500140 started daemon>
2023-06-26 20:46:16,763 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503220 parent=500140 started daemon>
2023-06-26 20:46:16,763 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503199 parent=500140 started daemon>
2023-06-26 20:46:16,763 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=503188 parent=500140 started daemon>
2023-06-26 20:46:20,984 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 503220 exit status was already read will report exitcode 255
2023-06-26 20:46:23,797 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 503289 exit status was already read will report exitcode 255
