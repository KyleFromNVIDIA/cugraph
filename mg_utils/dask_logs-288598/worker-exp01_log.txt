RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 16:55:36,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41437'
2023-06-26 16:55:36,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43955'
2023-06-26 16:55:36,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35809'
2023-06-26 16:55:36,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40777'
2023-06-26 16:55:36,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35375'
2023-06-26 16:55:36,562 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40509'
2023-06-26 16:55:36,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45979'
2023-06-26 16:55:36,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34917'
2023-06-26 16:55:36,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36769'
2023-06-26 16:55:36,571 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33359'
2023-06-26 16:55:36,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37623'
2023-06-26 16:55:36,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37671'
2023-06-26 16:55:36,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39749'
2023-06-26 16:55:36,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35821'
2023-06-26 16:55:36,582 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42791'
2023-06-26 16:55:36,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46059'
2023-06-26 16:55:38,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,314 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,325 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,328 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,328 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,332 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,332 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,346 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:55:38,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:55:38,422 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,425 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,439 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,473 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,492 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,506 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,506 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,514 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:38,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:55:44,879 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44283
2023-06-26 16:55:44,879 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44283
2023-06-26 16:55:44,879 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44355
2023-06-26 16:55:44,879 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:44,879 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:44,879 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:44,879 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:44,879 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dl92udqw
2023-06-26 16:55:44,880 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1b2ba29-0ba1-4998-a824-fe8ad508c603
2023-06-26 16:55:44,883 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42779
2023-06-26 16:55:44,883 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42779
2023-06-26 16:55:44,883 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37351
2023-06-26 16:55:44,883 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:44,883 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:44,883 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:44,883 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:44,884 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-86ectvnf
2023-06-26 16:55:44,884 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f07a1bb4-a99c-427e-b1b3-44b7e05ac081
2023-06-26 16:55:45,389 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37431
2023-06-26 16:55:45,390 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37431
2023-06-26 16:55:45,390 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38129
2023-06-26 16:55:45,390 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,390 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,390 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,390 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,389 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39191
2023-06-26 16:55:45,390 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-29qth88q
2023-06-26 16:55:45,390 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39191
2023-06-26 16:55:45,390 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44559
2023-06-26 16:55:45,390 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,391 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,391 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,391 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,391 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4aazs_9r
2023-06-26 16:55:45,391 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b51181e4-14f2-474e-8d82-985287cebc60
2023-06-26 16:55:45,391 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91969ab5-2e97-4f7b-a202-2467f94cb5ca
2023-06-26 16:55:45,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a4c8a39-b557-4d85-8356-67b1e9f3af01
2023-06-26 16:55:45,405 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35889
2023-06-26 16:55:45,405 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35889
2023-06-26 16:55:45,405 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42261
2023-06-26 16:55:45,405 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,405 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,405 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,405 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vqssnfao
2023-06-26 16:55:45,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54393db2-0f45-4d20-aa47-eb40a874c7fa
2023-06-26 16:55:45,413 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43061
2023-06-26 16:55:45,413 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43061
2023-06-26 16:55:45,413 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38169
2023-06-26 16:55:45,413 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45823
2023-06-26 16:55:45,413 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,413 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45823
2023-06-26 16:55:45,413 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,413 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46517
2023-06-26 16:55:45,413 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,413 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-byi19825
2023-06-26 16:55:45,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,413 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,413 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,413 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r951bczp
2023-06-26 16:55:45,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-797157ae-40bf-492e-91a8-5b09d3e1cd31
2023-06-26 16:55:45,414 - distributed.worker - INFO - Starting Worker plugin PreImport-a2bab7cb-ebe8-415c-a49e-93fe436fec08
2023-06-26 16:55:45,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f879128-8953-4965-ba13-add523227a5b
2023-06-26 16:55:45,420 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38439
2023-06-26 16:55:45,421 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38439
2023-06-26 16:55:45,421 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39015
2023-06-26 16:55:45,421 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,421 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,421 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,421 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,421 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j8tvvfio
2023-06-26 16:55:45,422 - distributed.worker - INFO - Starting Worker plugin PreImport-7d8a2a0b-711e-4ff4-8bf2-6261dcba517e
2023-06-26 16:55:45,423 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d87e8f84-dc64-40e9-b459-34ff418b8c06
2023-06-26 16:55:45,427 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42061
2023-06-26 16:55:45,428 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42061
2023-06-26 16:55:45,428 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34471
2023-06-26 16:55:45,428 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,428 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,428 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,428 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,428 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6gr3vzul
2023-06-26 16:55:45,429 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07bca47f-3520-4f4b-a5a9-ba2eb46fc7de
2023-06-26 16:55:45,431 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41327
2023-06-26 16:55:45,431 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41327
2023-06-26 16:55:45,431 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46559
2023-06-26 16:55:45,431 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,431 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,431 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,431 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,431 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wpsbv9vg
2023-06-26 16:55:45,432 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b27c05e-7ebd-43ac-807f-516fd3873cd8
2023-06-26 16:55:45,437 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37725
2023-06-26 16:55:45,437 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37725
2023-06-26 16:55:45,437 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43517
2023-06-26 16:55:45,437 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,437 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,437 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,438 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,438 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ex6_8l6r
2023-06-26 16:55:45,439 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d61b102-8039-4006-9b7d-49e681ff1bc1
2023-06-26 16:55:45,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2f6273b-42f1-4a3d-908f-875e3a96e352
2023-06-26 16:55:45,457 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37843
2023-06-26 16:55:45,457 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37843
2023-06-26 16:55:45,458 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41679
2023-06-26 16:55:45,458 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,458 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,458 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,458 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,458 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kjstj5n_
2023-06-26 16:55:45,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd669bc3-5a69-48b7-b480-f4164fdc719d
2023-06-26 16:55:45,486 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33005
2023-06-26 16:55:45,486 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33005
2023-06-26 16:55:45,486 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39043
2023-06-26 16:55:45,486 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,486 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,486 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,486 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rw_0bkyt
2023-06-26 16:55:45,487 - distributed.worker - INFO - Starting Worker plugin PreImport-4e26f1c9-31a0-4f1b-9440-e03be25e07df
2023-06-26 16:55:45,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c301fea1-06d5-490e-8c7e-197fd402e86c
2023-06-26 16:55:45,524 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42737
2023-06-26 16:55:45,524 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42737
2023-06-26 16:55:45,524 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45891
2023-06-26 16:55:45,524 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,524 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,524 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,525 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-no098nci
2023-06-26 16:55:45,524 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37195
2023-06-26 16:55:45,525 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37195
2023-06-26 16:55:45,525 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45433
2023-06-26 16:55:45,525 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,525 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fef469eb-effb-4faa-baa1-669c6068c59f
2023-06-26 16:55:45,525 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,525 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-imc423te
2023-06-26 16:55:45,526 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79848dcb-5a53-4402-beac-6bd91924582c
2023-06-26 16:55:45,533 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44737
2023-06-26 16:55:45,533 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44737
2023-06-26 16:55:45,533 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41617
2023-06-26 16:55:45,533 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:55:45,533 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:45,533 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:55:45,534 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:55:45,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7sze7p29
2023-06-26 16:55:45,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd201804-4f38-46eb-9c1c-8f35b4ce8890
2023-06-26 16:55:45,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e196ad2-54ba-4ca9-b4b1-96e6968797a9
2023-06-26 16:55:48,836 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1fde36d-99bc-46f2-807c-13a28d4b15ff
2023-06-26 16:55:48,836 - distributed.worker - INFO - Starting Worker plugin PreImport-0d39882b-91c8-491e-a4e9-b0d8fb9ec7f2
2023-06-26 16:55:48,837 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:48,861 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:48,861 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:48,862 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,007 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a622fef9-0676-473f-85be-1d7bebfe160d
2023-06-26 16:55:49,008 - distributed.worker - INFO - Starting Worker plugin PreImport-e6420a92-c369-44e9-9f48-02722cc177e3
2023-06-26 16:55:49,010 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,041 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,041 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,044 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b2bfc46-03c1-40a2-bcec-7a997d8a0d7f
2023-06-26 16:55:49,069 - distributed.worker - INFO - Starting Worker plugin PreImport-f2cec9fd-e238-47fb-bc77-dc906cf2c556
2023-06-26 16:55:49,070 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,080 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe98f4a9-f993-418a-a383-a64926c23558
2023-06-26 16:55:49,084 - distributed.worker - INFO - Starting Worker plugin PreImport-aa050693-f9d7-465a-803a-a43f306cb384
2023-06-26 16:55:49,087 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,089 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,089 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,090 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,117 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,117 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,119 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,155 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a7eb7c8-e90b-4a58-bf20-c663803f31b2
2023-06-26 16:55:49,156 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a0c08c4-ca4c-46bb-8e3e-addc559d197c
2023-06-26 16:55:49,172 - distributed.worker - INFO - Starting Worker plugin PreImport-c75227f9-2deb-471d-b8e0-aecaa0cb86eb
2023-06-26 16:55:49,172 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,172 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,173 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,174 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,183 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-939a9b79-dac4-445f-a9be-a4f61911fd6b
2023-06-26 16:55:49,183 - distributed.worker - INFO - Starting Worker plugin PreImport-5510ca00-32a5-405b-b4f2-30ba4987e27d
2023-06-26 16:55:49,185 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,190 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,190 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,191 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,194 - distributed.worker - INFO - Starting Worker plugin PreImport-3303a6c1-1660-4d1f-a6dd-a3f910598cbe
2023-06-26 16:55:49,196 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,199 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,199 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,201 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,216 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3ab01f6-4d34-46ad-890d-a9a7ed30fc7c
2023-06-26 16:55:49,216 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,216 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,217 - distributed.worker - INFO - Starting Worker plugin PreImport-04f2e11a-ea53-4b30-863f-a046228a7965
2023-06-26 16:55:49,218 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,218 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,232 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,232 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,234 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,244 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de010b65-50f0-4624-8b71-205c4c65fbc6
2023-06-26 16:55:49,245 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,246 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b909695d-29a0-4a02-a29e-e32a9c1eba15
2023-06-26 16:55:49,246 - distributed.worker - INFO - Starting Worker plugin PreImport-3e2de5ec-96ba-49db-8241-8a6686c6c4b6
2023-06-26 16:55:49,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,261 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,261 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,263 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,266 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,266 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,268 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,274 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef9c72de-7e3a-402e-92bb-ec7da139b384
2023-06-26 16:55:49,275 - distributed.worker - INFO - Starting Worker plugin PreImport-064b8f7b-989a-4c49-90b4-9aa2b4e9af0b
2023-06-26 16:55:49,275 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,276 - distributed.worker - INFO - Starting Worker plugin PreImport-0bc32e6a-05f0-494d-8917-92c0bfffba23
2023-06-26 16:55:49,276 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f928c00a-c81b-4f10-9499-fa8b6a698c56
2023-06-26 16:55:49,277 - distributed.worker - INFO - Starting Worker plugin PreImport-a0cc0fab-0ce2-4258-9fa8-07001caf2fa0
2023-06-26 16:55:49,277 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,278 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,286 - distributed.worker - INFO - Starting Worker plugin PreImport-6f33b880-d240-497a-bc2d-42bed8600cef
2023-06-26 16:55:49,287 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,287 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,287 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,289 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,289 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1b26957-a733-4892-9da2-a7c0d9c03d57
2023-06-26 16:55:49,290 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,294 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,294 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,297 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,298 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,298 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,301 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,301 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,301 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,303 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:55:49,305 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:55:49,305 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:55:49,307 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:57:04,075 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,078 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,079 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,080 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,080 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:57:04,089 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,089 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,089 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,089 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,089 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,089 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,089 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,090 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,791 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,791 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:04,791 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:57:07,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:20,301 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,488 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,641 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,767 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,909 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,911 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,919 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,943 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,978 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,979 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:20,986 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:21,031 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:21,051 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:21,070 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:21,089 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:21,219 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:57:27,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:57:27,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:06,430 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,430 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,430 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,432 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,435 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,435 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,435 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,435 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,435 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,436 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,436 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,436 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,436 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,438 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,439 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:06,447 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:58:09,624 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,631 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,631 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,631 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,631 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,631 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,631 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,631 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,632 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,632 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,632 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,632 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,632 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,632 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,633 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,633 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:58:09,987 - distributed.worker - WARNING - Compute Failed
Key:       ('_filter_batches-b03999dc6300d7df90c804a2446329fa', 0)
Function:  subgraph_callable-8450d6c6-82f3-41b5-8c4b-21ab7ab4
args:      ({'number': 0, 'division': None},            _START_  _BATCH_
0              602        0
1              684        0
2             1384        0
3             1525        0
4             2127        0
...            ...      ...
1546779  693089189     3021
1546780  693089300     3021
1546781  693089304     3021
1546782  693089465     3021
1546783  693089507     3021

[1546784 rows x 2 columns], '_BATCH_', 1169, 'rename-50b1cf554821098035f1b6ae3dd3b4f8')
kwargs:    {}
Exception: "ValueError('The columns in the computed data do not match the columns in the provided metadata\\nOrder of columns does not match')"

2023-06-26 16:58:10,020 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:10,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:58:14,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:58:14,461 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,463 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33005. Reason: scheduler-restart
2023-06-26 16:58:14,463 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,464 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,464 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35889. Reason: scheduler-restart
2023-06-26 16:58:14,464 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,464 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,464 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37195. Reason: scheduler-restart
2023-06-26 16:58:14,464 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,465 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,465 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37431. Reason: scheduler-restart
2023-06-26 16:58:14,465 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,465 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,465 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,465 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,465 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,466 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37725. Reason: scheduler-restart
2023-06-26 16:58:14,466 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33005
2023-06-26 16:58:14,466 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37843. Reason: scheduler-restart
2023-06-26 16:58:14,466 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,466 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,467 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,467 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38439. Reason: scheduler-restart
2023-06-26 16:58:14,467 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,468 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39191. Reason: scheduler-restart
2023-06-26 16:58:14,467 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,468 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,468 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,469 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,469 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,470 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,470 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,470 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,471 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,471 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41327. Reason: scheduler-restart
2023-06-26 16:58:14,471 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,472 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43061. Reason: scheduler-restart
2023-06-26 16:58:14,472 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:58:14,472 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,472 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,472 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,472 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,472 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,473 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,473 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,474 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,474 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,474 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,474 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,474 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,474 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42779. Reason: scheduler-restart
2023-06-26 16:58:14,474 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,474 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42061. Reason: scheduler-restart
2023-06-26 16:58:14,474 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,475 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,475 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,475 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,476 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,476 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42737. Reason: scheduler-restart
2023-06-26 16:58:14,476 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,476 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,476 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,476 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,476 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,476 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44283. Reason: scheduler-restart
2023-06-26 16:58:14,476 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,477 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,479 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,479 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,479 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,479 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,479 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,479 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,479 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,479 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,479 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,479 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,480 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43061
2023-06-26 16:58:14,480 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39191
2023-06-26 16:58:14,480 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41327
2023-06-26 16:58:14,480 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45823. Reason: scheduler-restart
2023-06-26 16:58:14,481 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,482 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44737. Reason: scheduler-restart
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43061
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39191
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41327
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42779
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37195
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37431
2023-06-26 16:58:14,482 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37725
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,482 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37843
2023-06-26 16:58:14,483 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,483 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38439
2023-06-26 16:58:14,483 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,484 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43061
2023-06-26 16:58:14,484 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39191
2023-06-26 16:58:14,484 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41327
2023-06-26 16:58:14,484 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42779
2023-06-26 16:58:14,485 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43061
2023-06-26 16:58:14,485 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39191
2023-06-26 16:58:14,485 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41327
2023-06-26 16:58:14,485 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42779
2023-06-26 16:58:14,487 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42061
2023-06-26 16:58:14,487 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42737
2023-06-26 16:58:14,487 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,487 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,487 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,487 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,509 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42061
2023-06-26 16:58:14,509 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42737
2023-06-26 16:58:14,510 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44283
2023-06-26 16:58:14,510 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44737
2023-06-26 16:58:14,514 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:58:14,515 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:14,519 - distributed.nanny - INFO - Worker closed
2023-06-26 16:58:18,097 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:19,085 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:19,116 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:19,124 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:19,587 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:19,878 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:21,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:21,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:21,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:22,223 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:22,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:22,497 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:22,497 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:22,497 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,497 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:22,498 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:22,511 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:22,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:22,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:22,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:22,522 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,523 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,527 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,528 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,530 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,532 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,536 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,541 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:58:22,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:22,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:22,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:22,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:22,698 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,088 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,263 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,354 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33437
2023-06-26 16:58:24,354 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33437
2023-06-26 16:58:24,354 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38171
2023-06-26 16:58:24,354 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:24,354 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:24,354 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:24,354 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:24,354 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g4sa3jgs
2023-06-26 16:58:24,354 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dcff355f-1a80-4c6c-969b-1bc6ad58e4e1
2023-06-26 16:58:24,356 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46de9502-821a-48f6-90f8-36cc3713d899
2023-06-26 16:58:24,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,397 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:58:24,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:58:24,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,577 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,578 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,606 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:24,662 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:58:25,439 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34149
2023-06-26 16:58:25,439 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34149
2023-06-26 16:58:25,439 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41813
2023-06-26 16:58:25,440 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:25,440 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:25,440 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:25,440 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:25,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hoj2rs3r
2023-06-26 16:58:25,440 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4293471-1be4-4636-9ae9-eee4f37a8257
2023-06-26 16:58:25,623 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34431
2023-06-26 16:58:25,624 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34431
2023-06-26 16:58:25,624 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35487
2023-06-26 16:58:25,624 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:25,624 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:25,624 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:25,624 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:25,624 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pj5af5bn
2023-06-26 16:58:25,625 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33dccd2a-e4d4-458c-b0f5-a60f10ce7b87
2023-06-26 16:58:25,625 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41861
2023-06-26 16:58:25,625 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41861
2023-06-26 16:58:25,625 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33115
2023-06-26 16:58:25,625 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:25,625 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:25,625 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:25,625 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:25,625 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ntankapx
2023-06-26 16:58:25,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e343d0e-b7c5-4dbc-ada2-e7fad63bcd4d
2023-06-26 16:58:25,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b8cb3a8-e25b-4f6c-b465-8781db586404
2023-06-26 16:58:25,629 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44865
2023-06-26 16:58:25,629 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44865
2023-06-26 16:58:25,630 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41839
2023-06-26 16:58:25,630 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:25,630 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:25,630 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:25,630 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:25,630 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-52p7r797
2023-06-26 16:58:25,630 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07587ca3-e1ef-45c4-880d-e4042d1f01eb
2023-06-26 16:58:25,653 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34843
2023-06-26 16:58:25,653 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34843
2023-06-26 16:58:25,654 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34307
2023-06-26 16:58:25,654 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:25,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:25,654 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:25,654 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:25,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ua44fqwl
2023-06-26 16:58:25,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-747b5fd3-842a-4ff4-b6fe-894bc67c0a62
2023-06-26 16:58:26,964 - distributed.worker - INFO - Starting Worker plugin PreImport-503d1502-534e-4daf-9175-af5aa6ebd360
2023-06-26 16:58:26,966 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:26,982 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:26,982 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:26,985 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:28,430 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01f73664-3a46-42b3-9150-9e57bd55eef9
2023-06-26 16:58:28,431 - distributed.worker - INFO - Starting Worker plugin PreImport-43cd3d1f-d182-4d72-9e51-e9d4e261e59d
2023-06-26 16:58:28,432 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,475 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7472a8c9-cb45-422a-85d2-ed0ac43e9c51
2023-06-26 16:58:28,475 - distributed.worker - INFO - Starting Worker plugin PreImport-1bc87737-bc64-401e-8d66-a481b98bf231
2023-06-26 16:58:28,480 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,480 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:28,480 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,482 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:28,499 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:28,499 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,503 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:28,576 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a18e5581-09e4-41a9-9037-368f030a62c1
2023-06-26 16:58:28,576 - distributed.worker - INFO - Starting Worker plugin PreImport-f6432cf8-2b82-48bd-8f80-27ba73a010c7
2023-06-26 16:58:28,577 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,601 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:28,601 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,602 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:28,630 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf0c5a42-61c4-4023-9a1b-eb1fc5df7ce0
2023-06-26 16:58:28,631 - distributed.worker - INFO - Starting Worker plugin PreImport-60c7d331-abad-43fc-b16f-12f0c8ca84d8
2023-06-26 16:58:28,632 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,652 - distributed.worker - INFO - Starting Worker plugin PreImport-e745db3b-8a94-45bd-9d0b-d7cde94be5b6
2023-06-26 16:58:28,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,655 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:28,655 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,657 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:28,682 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:28,682 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:28,684 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:30,135 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32923
2023-06-26 16:58:30,135 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32923
2023-06-26 16:58:30,135 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46677
2023-06-26 16:58:30,135 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:30,135 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:30,135 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:30,135 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:30,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vwvluxlb
2023-06-26 16:58:30,136 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3e7aa04-55bf-4222-b403-de67b8a6a9ba
2023-06-26 16:58:30,484 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f1649f6-8dee-45d7-afd6-f7d03ccd7be1
2023-06-26 16:58:30,485 - distributed.worker - INFO - Starting Worker plugin PreImport-60339124-54ff-43f7-8c72-280672805d72
2023-06-26 16:58:30,486 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:30,502 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:30,502 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:30,508 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:31,425 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36827
2023-06-26 16:58:31,425 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36827
2023-06-26 16:58:31,425 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42585
2023-06-26 16:58:31,425 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,425 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,425 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,425 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tnukvjuq
2023-06-26 16:58:31,426 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c18537c0-94aa-4066-8b22-8e946bbff15a
2023-06-26 16:58:31,428 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35395
2023-06-26 16:58:31,428 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35395
2023-06-26 16:58:31,428 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46197
2023-06-26 16:58:31,429 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,429 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,429 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,429 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,429 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pqlqzhq1
2023-06-26 16:58:31,429 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2c115bc-be3c-4d13-9cd7-c591fb85043f
2023-06-26 16:58:31,437 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43433
2023-06-26 16:58:31,438 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43433
2023-06-26 16:58:31,438 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35865
2023-06-26 16:58:31,438 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,438 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,438 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,438 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,438 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kkru9gr0
2023-06-26 16:58:31,438 - distributed.worker - INFO - Starting Worker plugin RMMSetup-276a637b-031f-4b34-b9b6-92591ef9ea6c
2023-06-26 16:58:31,460 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42193
2023-06-26 16:58:31,460 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42193
2023-06-26 16:58:31,460 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44481
2023-06-26 16:58:31,461 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,461 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,461 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,461 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,461 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g_8ji47b
2023-06-26 16:58:31,461 - distributed.worker - INFO - Starting Worker plugin PreImport-2bbbfbf5-c158-4808-9add-f19256c91fda
2023-06-26 16:58:31,461 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4c6b65c-f63a-4164-bdc1-4d3ad2ff1132
2023-06-26 16:58:31,480 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37737
2023-06-26 16:58:31,480 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37737
2023-06-26 16:58:31,480 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33067
2023-06-26 16:58:31,480 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,480 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,481 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,481 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,481 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4fzxsiwz
2023-06-26 16:58:31,481 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01524565-432a-4313-9b69-c36a34d98a0c
2023-06-26 16:58:31,487 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34323
2023-06-26 16:58:31,487 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34323
2023-06-26 16:58:31,487 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39089
2023-06-26 16:58:31,487 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,487 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,487 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,488 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,488 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-az7nemot
2023-06-26 16:58:31,488 - distributed.worker - INFO - Starting Worker plugin PreImport-27ad865d-47b3-4387-a0cd-b1aba0ed732e
2023-06-26 16:58:31,488 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4852460c-ca35-4bc3-a890-d5ca729b9fa7
2023-06-26 16:58:31,490 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37075
2023-06-26 16:58:31,490 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37075
2023-06-26 16:58:31,490 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34075
2023-06-26 16:58:31,490 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,490 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,490 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,490 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,490 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xlbh9b3o
2023-06-26 16:58:31,491 - distributed.worker - INFO - Starting Worker plugin PreImport-50dd1f3d-6eb4-4a2b-8e0d-e1c094937ba2
2023-06-26 16:58:31,491 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38c605ec-d41a-4f33-bbbe-ac4c4d26764c
2023-06-26 16:58:31,497 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37679
2023-06-26 16:58:31,498 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37679
2023-06-26 16:58:31,498 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38927
2023-06-26 16:58:31,498 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,498 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,498 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,498 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,498 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oeo7xb2_
2023-06-26 16:58:31,499 - distributed.worker - INFO - Starting Worker plugin RMMSetup-035b5eaf-3bb9-45c9-a9b8-287923d93264
2023-06-26 16:58:31,502 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41733
2023-06-26 16:58:31,503 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41733
2023-06-26 16:58:31,503 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40091
2023-06-26 16:58:31,503 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:58:31,503 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:31,503 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:58:31,503 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:58:31,503 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-49p1vdmx
2023-06-26 16:58:31,503 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b78881b4-ddf9-4304-8ab4-15430d9932af
2023-06-26 16:58:31,503 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79d68245-78f0-4211-af4c-9ba1adacf9d6
2023-06-26 16:58:33,564 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ebef216-792d-4cf7-ab31-a84d181fa551
2023-06-26 16:58:33,564 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,576 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,576 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,577 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:33,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d26589fd-da48-4b1e-9291-0b51cf6d5917
2023-06-26 16:58:33,689 - distributed.worker - INFO - Starting Worker plugin PreImport-b7fa080c-d846-42b4-8640-ffb8226e02f4
2023-06-26 16:58:33,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7bf1746-6b4e-4342-a014-aec8f693a323
2023-06-26 16:58:33,690 - distributed.worker - INFO - Starting Worker plugin PreImport-331211d1-179a-4286-8ccc-f415f47f0d9a
2023-06-26 16:58:33,690 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,692 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54cc063e-69aa-49da-a5ea-abf857158768
2023-06-26 16:58:33,696 - distributed.worker - INFO - Starting Worker plugin PreImport-d680e309-7abf-4111-92ac-b324af738ae5
2023-06-26 16:58:33,697 - distributed.worker - INFO - Starting Worker plugin PreImport-f0e8ff51-64eb-4352-9d88-f63786c91569
2023-06-26 16:58:33,698 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,699 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,710 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,710 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,711 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,711 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,712 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:33,713 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:33,718 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fb65efc-8ec0-4fa4-bf6f-139fe925e30f
2023-06-26 16:58:33,719 - distributed.worker - INFO - Starting Worker plugin PreImport-1b88905f-4742-4e20-9e44-797e340c1334
2023-06-26 16:58:33,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,722 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,722 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,723 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,723 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,724 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7ca7169-7280-4361-a1ec-dd6b1efc25da
2023-06-26 16:58:33,724 - distributed.worker - INFO - Starting Worker plugin PreImport-2d80b007-a198-4cf1-83fb-2473f2aad7eb
2023-06-26 16:58:33,724 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,725 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:33,725 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-817e64d0-f1f1-4434-a955-2c1559b1e407
2023-06-26 16:58:33,726 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:33,727 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,730 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,730 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,732 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:33,734 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ecee74d3-cc20-4d2a-9535-e75bb5b832b6
2023-06-26 16:58:33,734 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,737 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,737 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,738 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:33,744 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,744 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,747 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:58:33,747 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:58:33,747 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:58:33,748 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:59:20,138 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34431. Reason: worker-close
2023-06-26 16:59:20,138 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34149. Reason: worker-close
2023-06-26 16:59:20,138 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34843. Reason: worker-close
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43433. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41733. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37737. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42193. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36827. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37075. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37679. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,138 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41861. Reason: worker-close
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44865. Reason: worker-close
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35395. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33437. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34323. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41437'. Reason: nanny-close
2023-06-26 16:59:20,139 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32923. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:59:20,140 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,140 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38424 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:59:20,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43955'. Reason: nanny-close
2023-06-26 16:59:20,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,140 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38446 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:59:20,140 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38410 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:59:20,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35809'. Reason: nanny-close
2023-06-26 16:59:20,140 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38436 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:59:20,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40777'. Reason: nanny-close
2023-06-26 16:59:20,143 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35375'. Reason: nanny-close
2023-06-26 16:59:20,141 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38418 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:59:20,143 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40509'. Reason: nanny-close
2023-06-26 16:59:20,144 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45979'. Reason: nanny-close
2023-06-26 16:59:20,145 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36769'. Reason: nanny-close
2023-06-26 16:59:20,145 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,146 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33359'. Reason: nanny-close
2023-06-26 16:59:20,146 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,146 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37623'. Reason: nanny-close
2023-06-26 16:59:20,146 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,147 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37671'. Reason: nanny-close
2023-06-26 16:59:20,147 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,147 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39749'. Reason: nanny-close
2023-06-26 16:59:20,148 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,148 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34917'. Reason: nanny-close
2023-06-26 16:59:20,148 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,148 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35821'. Reason: nanny-close
2023-06-26 16:59:20,149 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42791'. Reason: nanny-close
2023-06-26 16:59:20,149 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46059'. Reason: nanny-close
2023-06-26 16:59:20,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:59:20,511 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35375 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:54420 remote=tcp://10.120.104.11:35375>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35375 after 100 s
2023-06-26 16:59:20,512 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40509 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56492 remote=tcp://10.120.104.11:40509>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40509 after 100 s
2023-06-26 16:59:20,513 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37671 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48488 remote=tcp://10.120.104.11:37671>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37671 after 100 s
2023-06-26 16:59:20,517 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36769 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:57250 remote=tcp://10.120.104.11:36769>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36769 after 100 s
2023-06-26 16:59:20,520 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46059 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:57818 remote=tcp://10.120.104.11:46059>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46059 after 100 s
2023-06-26 16:59:20,521 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35821 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45776 remote=tcp://10.120.104.11:35821>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35821 after 100 s
2023-06-26 16:59:20,521 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42791 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:38956 remote=tcp://10.120.104.11:42791>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42791 after 100 s
2023-06-26 16:59:20,521 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33359 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:51994 remote=tcp://10.120.104.11:33359>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33359 after 100 s
2023-06-26 16:59:20,522 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34917 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:38168 remote=tcp://10.120.104.11:34917>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34917 after 100 s
2023-06-26 16:59:20,527 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43955 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37720 remote=tcp://10.120.104.11:43955>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43955 after 100 s
2023-06-26 16:59:20,529 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39749 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:32784 remote=tcp://10.120.104.11:39749>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39749 after 100 s
2023-06-26 16:59:20,528 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35809 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34550 remote=tcp://10.120.104.11:35809>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35809 after 100 s
2023-06-26 16:59:20,530 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37623 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56658 remote=tcp://10.120.104.11:37623>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37623 after 100 s
2023-06-26 16:59:20,530 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45979 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47148 remote=tcp://10.120.104.11:45979>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45979 after 100 s
2023-06-26 16:59:20,533 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40777 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35526 remote=tcp://10.120.104.11:40777>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40777 after 100 s
2023-06-26 16:59:23,351 - distributed.nanny - WARNING - Worker process still alive after 3.1999971008300783 seconds, killing
2023-06-26 16:59:23,351 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 16:59:23,351 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 16:59:23,351 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:59:23,352 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:59:23,352 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:59:23,353 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:59:23,353 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:59:23,353 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:59:23,354 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:59:23,354 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:59:23,354 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:59:23,354 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:59:23,354 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:59:23,355 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:59:23,356 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:59:24,141 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,143 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,143 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,143 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,144 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,145 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,145 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,146 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,147 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,147 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,148 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,148 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,148 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,150 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,150 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,150 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292268 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292265 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292262 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292259 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292256 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292253 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292250 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292247 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292240 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292225 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292185 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292175 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292168 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292165 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292162 parent=288741 started daemon>
2023-06-26 16:59:24,152 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=292145 parent=288741 started daemon>
2023-06-26 16:59:27,428 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 292185 exit status was already read will report exitcode 255
2023-06-26 16:59:28,357 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 292175 exit status was already read will report exitcode 255
