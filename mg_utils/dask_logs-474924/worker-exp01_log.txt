RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 20:15:03,305 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45345'
2023-06-26 20:15:03,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39809'
2023-06-26 20:15:03,310 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38101'
2023-06-26 20:15:03,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39391'
2023-06-26 20:15:03,315 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42685'
2023-06-26 20:15:03,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43505'
2023-06-26 20:15:03,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44877'
2023-06-26 20:15:03,321 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33739'
2023-06-26 20:15:03,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34499'
2023-06-26 20:15:03,325 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34961'
2023-06-26 20:15:03,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37755'
2023-06-26 20:15:03,329 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42383'
2023-06-26 20:15:03,331 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42393'
2023-06-26 20:15:03,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45051'
2023-06-26 20:15:03,337 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43241'
2023-06-26 20:15:03,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46629'
2023-06-26 20:15:04,874 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:04,874 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:04,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:04,931 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:04,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:04,931 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:04,980 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:04,980 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,015 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,015 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,034 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,034 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,040 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,048 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,059 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:15:05,059 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:15:05,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,185 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,193 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,215 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,226 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,227 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,230 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:05,246 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:15:11,038 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33345
2023-06-26 20:15:11,038 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33345
2023-06-26 20:15:11,038 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38777
2023-06-26 20:15:11,038 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,038 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,038 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,038 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,038 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lyuc3o57
2023-06-26 20:15:11,039 - distributed.worker - INFO - Starting Worker plugin PreImport-61d5562a-0bd5-4b83-9333-78dabc9e5716
2023-06-26 20:15:11,040 - distributed.worker - INFO - Starting Worker plugin RMMSetup-927c70fa-8922-4a28-8b99-4c45469de69c
2023-06-26 20:15:11,169 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41327
2023-06-26 20:15:11,169 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41327
2023-06-26 20:15:11,169 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35779
2023-06-26 20:15:11,169 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,169 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,169 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,169 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k24_jg0f
2023-06-26 20:15:11,170 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2eebd7e1-c17d-49e6-8319-e072f504c348
2023-06-26 20:15:11,170 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42715
2023-06-26 20:15:11,170 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42715
2023-06-26 20:15:11,170 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44113
2023-06-26 20:15:11,170 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,170 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,170 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,170 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,170 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hzm4gpc0
2023-06-26 20:15:11,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9b7a0eb-1976-47f6-bdab-b321060caa33
2023-06-26 20:15:11,171 - distributed.worker - INFO - Starting Worker plugin RMMSetup-707c134c-957e-4ea8-a24a-e1fa1b0013aa
2023-06-26 20:15:11,274 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44243
2023-06-26 20:15:11,274 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44243
2023-06-26 20:15:11,275 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42817
2023-06-26 20:15:11,275 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,275 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,275 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,275 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,275 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-15pczx28
2023-06-26 20:15:11,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12ec5db7-6944-4c5b-969c-ffb4d6583d27
2023-06-26 20:15:11,294 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39077
2023-06-26 20:15:11,295 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39077
2023-06-26 20:15:11,295 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37297
2023-06-26 20:15:11,295 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,295 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,295 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,295 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,295 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yu5jtv_y
2023-06-26 20:15:11,296 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a16b7faa-dcaa-43af-8201-f74e53c04841
2023-06-26 20:15:11,384 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38347
2023-06-26 20:15:11,384 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38347
2023-06-26 20:15:11,385 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41831
2023-06-26 20:15:11,385 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,385 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,385 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,385 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2zx8wqyb
2023-06-26 20:15:11,386 - distributed.worker - INFO - Starting Worker plugin RMMSetup-923151dd-ba4f-4e25-a522-bc3cc64137a6
2023-06-26 20:15:11,427 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34005
2023-06-26 20:15:11,427 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34005
2023-06-26 20:15:11,427 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34093
2023-06-26 20:15:11,427 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,427 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,427 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,427 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,427 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ivlbtq1
2023-06-26 20:15:11,428 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f889f4c-486a-4922-b079-85ab6585d72e
2023-06-26 20:15:11,428 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1994649-9f68-4f0a-82f7-fcfc8b05e21c
2023-06-26 20:15:11,588 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35249
2023-06-26 20:15:11,588 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35249
2023-06-26 20:15:11,588 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34265
2023-06-26 20:15:11,588 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,588 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,589 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,589 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,589 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-annuq5i3
2023-06-26 20:15:11,589 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-451fadff-1677-4e9a-95e5-340c6e3fed49
2023-06-26 20:15:11,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52abb3ce-d1f7-4d60-ae93-8864a4e091aa
2023-06-26 20:15:11,635 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38723
2023-06-26 20:15:11,635 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38723
2023-06-26 20:15:11,635 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38547
2023-06-26 20:15:11,635 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,635 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,635 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,635 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ihowgzrr
2023-06-26 20:15:11,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dedca7e4-ca96-457a-bb8c-e7a7b10d2fb8
2023-06-26 20:15:11,691 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33257
2023-06-26 20:15:11,692 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33257
2023-06-26 20:15:11,692 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40405
2023-06-26 20:15:11,692 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,692 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,692 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,692 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,692 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6fpydynn
2023-06-26 20:15:11,692 - distributed.worker - INFO - Starting Worker plugin PreImport-3043beb7-8109-4f37-bee9-aa4eb7bb452b
2023-06-26 20:15:11,692 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9cdd817b-4344-42ce-8ba6-864f5c177683
2023-06-26 20:15:11,743 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42809
2023-06-26 20:15:11,743 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42809
2023-06-26 20:15:11,743 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35883
2023-06-26 20:15:11,743 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,743 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,743 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,743 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xrtpbt7w
2023-06-26 20:15:11,744 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79c6ceb6-8b09-4b73-b8fa-4515786a53c1
2023-06-26 20:15:11,828 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41637
2023-06-26 20:15:11,828 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41637
2023-06-26 20:15:11,828 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37473
2023-06-26 20:15:11,828 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,829 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,829 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,829 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,829 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uaaqf3rd
2023-06-26 20:15:11,830 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7fa9cd2f-1fd7-4268-9dd6-05706a40d4b8
2023-06-26 20:15:11,839 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36101
2023-06-26 20:15:11,840 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36101
2023-06-26 20:15:11,840 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37513
2023-06-26 20:15:11,840 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,840 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,840 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,840 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,840 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zip4r412
2023-06-26 20:15:11,841 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b926ffeb-dcbf-4e0e-984f-8248c433efab
2023-06-26 20:15:11,856 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39577
2023-06-26 20:15:11,856 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39577
2023-06-26 20:15:11,856 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33967
2023-06-26 20:15:11,856 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,856 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,856 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,856 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,856 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-08pq7b3c
2023-06-26 20:15:11,857 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80d5d4c2-67f0-4a77-a6a4-03642ad41566
2023-06-26 20:15:11,873 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43775
2023-06-26 20:15:11,874 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43775
2023-06-26 20:15:11,874 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37867
2023-06-26 20:15:11,874 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,874 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,874 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,874 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0aa5mhy0
2023-06-26 20:15:11,874 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1bf7a000-058f-45ec-994e-a03253b527e1
2023-06-26 20:15:11,882 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42335
2023-06-26 20:15:11,882 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42335
2023-06-26 20:15:11,882 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39103
2023-06-26 20:15:11,882 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:15:11,882 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:11,882 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:15:11,882 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:15:11,882 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d_r92d_f
2023-06-26 20:15:11,882 - distributed.worker - INFO - Starting Worker plugin PreImport-e4852b4e-14e1-4a7f-8ff9-59df57b4d0dd
2023-06-26 20:15:11,883 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ced66aa-422c-40df-9856-88d2fc9e0cb7
2023-06-26 20:15:15,001 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c5e6684-8cd4-4393-bdeb-d8af584a9d79
2023-06-26 20:15:15,003 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,026 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,027 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,028 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,033 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-941e8a9f-3f05-4ffc-b35a-980284d3b4c9
2023-06-26 20:15:15,033 - distributed.worker - INFO - Starting Worker plugin PreImport-e71d0877-3240-4c46-91ea-adebb8376620
2023-06-26 20:15:15,034 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,051 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,051 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,052 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,111 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4e865d5-dae1-4bf1-bc26-6f2cd2f487cc
2023-06-26 20:15:15,111 - distributed.worker - INFO - Starting Worker plugin PreImport-1231c34a-a551-446b-bede-de52ac40b3d1
2023-06-26 20:15:15,113 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,137 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,137 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,140 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,221 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd0315ad-13d2-40ad-a734-caff4d77d439
2023-06-26 20:15:15,222 - distributed.worker - INFO - Starting Worker plugin PreImport-e711ef68-beb5-4c23-9b60-802e715633eb
2023-06-26 20:15:15,223 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,246 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,246 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,247 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,259 - distributed.worker - INFO - Starting Worker plugin PreImport-a1e42e0b-3d76-43e7-b5c7-f19d481e116d
2023-06-26 20:15:15,260 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,286 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,286 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,288 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,325 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4da3313-ef86-421d-983e-fce4c5d0d036
2023-06-26 20:15:15,325 - distributed.worker - INFO - Starting Worker plugin PreImport-8b80a980-0b9c-4bf9-a8f6-8906b976f705
2023-06-26 20:15:15,327 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,335 - distributed.worker - INFO - Starting Worker plugin PreImport-388efb1f-a884-4906-bcbf-933e7c85b5d6
2023-06-26 20:15:15,336 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,346 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,346 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,349 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,352 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,352 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,353 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,390 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a8d3080-314a-44f5-ab7c-c82f7eac01d6
2023-06-26 20:15:15,391 - distributed.worker - INFO - Starting Worker plugin PreImport-3877f03c-5747-47b2-9559-38708c2aa9c5
2023-06-26 20:15:15,391 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,414 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,414 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,416 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,485 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-961ac72c-a4e9-431d-9f39-9314ba40ded3
2023-06-26 20:15:15,487 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,494 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7644c63f-024c-46c3-8394-b9f8a7fde400
2023-06-26 20:15:15,495 - distributed.worker - INFO - Starting Worker plugin PreImport-0f50424a-ce29-4473-b6b3-4af7a4a653fa
2023-06-26 20:15:15,496 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,508 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,508 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,511 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,514 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,514 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,515 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,523 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd6ef72f-4d60-4d81-8bdd-56329b899b7e
2023-06-26 20:15:15,524 - distributed.worker - INFO - Starting Worker plugin PreImport-035badea-4f90-4912-be33-c75eab652fb4
2023-06-26 20:15:15,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,526 - distributed.worker - INFO - Starting Worker plugin PreImport-e4837507-574f-44ea-ab5a-7db9bdbaf219
2023-06-26 20:15:15,528 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,537 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a75cfa93-6c49-406c-9d49-94663c82e45a
2023-06-26 20:15:15,538 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,538 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af979b45-0b31-458e-ae5e-9da2180e0fb9
2023-06-26 20:15:15,540 - distributed.worker - INFO - Starting Worker plugin PreImport-4b5e19ee-172d-4914-9fa8-43577417000a
2023-06-26 20:15:15,542 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,546 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,546 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,548 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,548 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,549 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,550 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,550 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,550 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,551 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,552 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-48330fdf-f2c7-4760-af61-d81880356168
2023-06-26 20:15:15,553 - distributed.worker - INFO - Starting Worker plugin PreImport-befa096a-255e-4c66-8c16-27eb1a0d4210
2023-06-26 20:15:15,554 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,555 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0df215e-7bb9-4c21-91e6-97b5fd933e82
2023-06-26 20:15:15,556 - distributed.worker - INFO - Starting Worker plugin PreImport-08d2aacf-f0e1-4fea-ae0e-2d918d753b9a
2023-06-26 20:15:15,557 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,561 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,561 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,563 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,571 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,571 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,573 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:15,574 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:15:15,574 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:15:15,576 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:15:35,311 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,311 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,311 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,311 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,313 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,314 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,316 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,316 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,317 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,319 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,319 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,320 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,320 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,320 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,320 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,328 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,339 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,340 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,340 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,340 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,340 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,340 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,340 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:35,340 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:15:36,044 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,044 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,044 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,044 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,044 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,044 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:36,045 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:15:39,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:51,080 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,160 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,307 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,444 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,452 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,505 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,552 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,564 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,569 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,575 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,639 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,728 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,728 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,754 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,818 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:51,856 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:15:58,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:15:58,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:38,270 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,271 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,275 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,277 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,278 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,278 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,278 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,278 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,279 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,279 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,279 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,279 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,279 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,279 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,281 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:38,282 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:16:41,556 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,561 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,561 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,561 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,562 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,562 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,562 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,562 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,562 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,563 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,563 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,563 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,564 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,564 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,565 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:41,565 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:16:51,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:16:51,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:51,866 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,866 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,866 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,866 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,866 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,866 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,866 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,867 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:17:51,882 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,882 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,882 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,882 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,882 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,882 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,882 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,882 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,883 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,883 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,883 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,883 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,883 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,883 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,883 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,883 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,895 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,896 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,896 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:51,896 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:17:56,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,474 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,476 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,476 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33257. Reason: scheduler-restart
2023-06-26 20:17:56,476 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,477 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33345. Reason: scheduler-restart
2023-06-26 20:17:56,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,477 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,477 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34005. Reason: scheduler-restart
2023-06-26 20:17:56,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,478 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35249. Reason: scheduler-restart
2023-06-26 20:17:56,478 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,478 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,478 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,478 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36101. Reason: scheduler-restart
2023-06-26 20:17:56,479 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,479 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,479 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38347. Reason: scheduler-restart
2023-06-26 20:17:56,479 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,480 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,480 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38723. Reason: scheduler-restart
2023-06-26 20:17:56,480 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39077. Reason: scheduler-restart
2023-06-26 20:17:56,480 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,481 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,480 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39577. Reason: scheduler-restart
2023-06-26 20:17:56,481 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,481 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41327. Reason: scheduler-restart
2023-06-26 20:17:56,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,481 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,481 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,481 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,482 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,482 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41637. Reason: scheduler-restart
2023-06-26 20:17:56,482 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,482 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42335. Reason: scheduler-restart
2023-06-26 20:17:56,482 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,483 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,483 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,483 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,483 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,483 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,484 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,484 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:17:56,487 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42715. Reason: scheduler-restart
2023-06-26 20:17:56,490 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,490 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33257
2023-06-26 20:17:56,490 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,490 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,491 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33257
2023-06-26 20:17:56,491 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42809. Reason: scheduler-restart
2023-06-26 20:17:56,497 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,498 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44243. Reason: scheduler-restart
2023-06-26 20:17:56,504 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33257
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33345
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34005
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35249
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36101
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38347
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38723
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39077
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39577
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41327
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41637
2023-06-26 20:17:56,505 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42335
2023-06-26 20:17:56,506 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42715
2023-06-26 20:17:56,506 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,508 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,512 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33257
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33345
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34005
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35249
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36101
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38347
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38723
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39077
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39577
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41327
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41637
2023-06-26 20:17:56,513 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42335
2023-06-26 20:17:56,514 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42715
2023-06-26 20:17:56,515 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,517 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,519 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44243
2023-06-26 20:17:56,522 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,528 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:56,530 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43775. Reason: scheduler-restart
2023-06-26 20:17:56,534 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33257
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33345
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34005
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35249
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36101
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38347
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38723
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39077
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39577
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41327
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41637
2023-06-26 20:17:56,535 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42335
2023-06-26 20:17:56,536 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42715
2023-06-26 20:17:56,536 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44243
2023-06-26 20:17:56,536 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42809
2023-06-26 20:17:56,536 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:17:56,538 - distributed.nanny - INFO - Worker closed
2023-06-26 20:17:59,841 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:00,234 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:01,625 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:01,626 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:01,629 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:01,631 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,106 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:03,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:03,280 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:03,786 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:03,786 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:03,794 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:03,794 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:03,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:03,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:03,799 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,800 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:03,807 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:03,807 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,808 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,808 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:03,808 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:03,810 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,812 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,814 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,820 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,821 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,822 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:18:03,974 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:03,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:04,040 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:04,050 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:04,060 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,480 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46697
2023-06-26 20:18:05,480 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46697
2023-06-26 20:18:05,481 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36631
2023-06-26 20:18:05,481 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:05,481 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:05,481 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:05,481 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:05,481 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kojakd46
2023-06-26 20:18:05,481 - distributed.worker - INFO - Starting Worker plugin PreImport-83590dad-f982-4d45-8619-dff6d14cc48f
2023-06-26 20:18:05,481 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9c212b4-c8fb-4b0b-b342-89fff48ea1c1
2023-06-26 20:18:05,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,660 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,669 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,669 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,676 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,708 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,708 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,748 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,751 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:18:05,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:18:05,839 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,856 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,900 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,926 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,926 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,939 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:05,960 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:18:06,586 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41131
2023-06-26 20:18:06,586 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41131
2023-06-26 20:18:06,586 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44071
2023-06-26 20:18:06,586 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:06,586 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:06,586 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:06,586 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:06,586 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1ztjfs5o
2023-06-26 20:18:06,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-696eac16-58ee-4887-aee0-7f7825fcb7e3
2023-06-26 20:18:06,590 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34673
2023-06-26 20:18:06,591 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34673
2023-06-26 20:18:06,591 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41095
2023-06-26 20:18:06,591 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:06,591 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:06,591 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:06,591 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:06,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3m6vs3pv
2023-06-26 20:18:06,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5bb2d43-b25c-44ba-ad12-6f516b5230fb
2023-06-26 20:18:06,817 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42399
2023-06-26 20:18:06,817 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42399
2023-06-26 20:18:06,817 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36907
2023-06-26 20:18:06,817 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:06,817 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:06,817 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:06,817 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:06,817 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z86w3tud
2023-06-26 20:18:06,818 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32aaa7ec-629c-4eed-bedf-5d5055b36e1a
2023-06-26 20:18:06,818 - distributed.worker - INFO - Starting Worker plugin RMMSetup-03616b18-74ce-476b-8dbe-4d2daec7ada0
2023-06-26 20:18:06,818 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39663
2023-06-26 20:18:06,819 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39663
2023-06-26 20:18:06,819 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43831
2023-06-26 20:18:06,819 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:06,819 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:06,819 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:06,819 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:06,819 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dam3fn3_
2023-06-26 20:18:06,818 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34963
2023-06-26 20:18:06,819 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34963
2023-06-26 20:18:06,819 - distributed.worker - INFO - Starting Worker plugin RMMSetup-438e72e9-4804-4540-9d5d-0c8541fb9091
2023-06-26 20:18:06,819 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33801
2023-06-26 20:18:06,819 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:06,819 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:06,819 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:06,819 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:06,819 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qxs6e255
2023-06-26 20:18:06,821 - distributed.worker - INFO - Starting Worker plugin RMMSetup-881fa601-0c75-49a1-975e-f442f157d24a
2023-06-26 20:18:07,737 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-824575bf-9a37-4b0b-8cdb-6075b3f86854
2023-06-26 20:18:07,741 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:07,761 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:07,761 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:07,764 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:09,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f017902-7a27-4ceb-8638-831249bc0c84
2023-06-26 20:18:09,091 - distributed.worker - INFO - Starting Worker plugin PreImport-c41eeef7-a8d8-485a-be41-b285be9b3a70
2023-06-26 20:18:09,092 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,101 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5e037ca-6628-4485-a524-8ade5914754e
2023-06-26 20:18:09,102 - distributed.worker - INFO - Starting Worker plugin PreImport-2469d08e-96a8-414a-9b96-a87e8e139370
2023-06-26 20:18:09,104 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,105 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00cd0e08-7352-4412-96c9-8d538bc595e2
2023-06-26 20:18:09,105 - distributed.worker - INFO - Starting Worker plugin PreImport-91d27434-6613-4e40-a91e-9d706c6a436a
2023-06-26 20:18:09,106 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:09,106 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,106 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,108 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:09,117 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:09,117 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,119 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:09,123 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:09,123 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,126 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:09,146 - distributed.worker - INFO - Starting Worker plugin PreImport-c68ae18b-489c-427d-8671-a59b81c68719
2023-06-26 20:18:09,147 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,160 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:09,160 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,162 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:09,187 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7269dac-8700-4e90-833d-39e48e01c893
2023-06-26 20:18:09,187 - distributed.worker - INFO - Starting Worker plugin PreImport-12369a24-90bb-432c-8ee6-aea5c951b064
2023-06-26 20:18:09,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,197 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:09,197 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:09,199 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:12,291 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39451
2023-06-26 20:18:12,292 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39451
2023-06-26 20:18:12,292 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36825
2023-06-26 20:18:12,292 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,292 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,292 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,292 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,292 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mb1rjgbq
2023-06-26 20:18:12,294 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb617202-e39b-4db8-977b-fb91701f25bd
2023-06-26 20:18:12,304 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39395
2023-06-26 20:18:12,304 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39395
2023-06-26 20:18:12,304 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45561
2023-06-26 20:18:12,304 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,304 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,305 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,305 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,305 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ha0iqpyn
2023-06-26 20:18:12,305 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6d54d77-5694-45a5-9f58-d3a7f57be0fe
2023-06-26 20:18:12,305 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4f280bb-bab3-4639-8da2-7d80f6c481a5
2023-06-26 20:18:12,306 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39963
2023-06-26 20:18:12,306 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39963
2023-06-26 20:18:12,306 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44475
2023-06-26 20:18:12,306 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,306 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,306 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,306 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,306 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zqz84jm6
2023-06-26 20:18:12,307 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e8524025-be0d-430e-ac94-c7e5f8781a53
2023-06-26 20:18:12,308 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72d46b2d-9877-4af7-97df-dacba86f1d55
2023-06-26 20:18:12,396 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45247
2023-06-26 20:18:12,396 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45247
2023-06-26 20:18:12,396 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45391
2023-06-26 20:18:12,396 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,396 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,396 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,396 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rm0dmyvz
2023-06-26 20:18:12,397 - distributed.worker - INFO - Starting Worker plugin RMMSetup-043e8a52-3d72-4da5-ac9b-eb33c094ad40
2023-06-26 20:18:12,417 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39957
2023-06-26 20:18:12,417 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39957
2023-06-26 20:18:12,417 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46211
2023-06-26 20:18:12,417 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,417 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,417 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s3ny85x4
2023-06-26 20:18:12,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7023714d-0a50-4fd4-88b9-da0377b35943
2023-06-26 20:18:12,462 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46115
2023-06-26 20:18:12,462 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46115
2023-06-26 20:18:12,463 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37207
2023-06-26 20:18:12,463 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,463 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,463 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,463 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,463 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6sm5xqai
2023-06-26 20:18:12,463 - distributed.worker - INFO - Starting Worker plugin PreImport-158a0746-77f0-4c48-bd73-343fc505f231
2023-06-26 20:18:12,463 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c27fc5ae-aa7c-4458-8f41-80fdf41eb8df
2023-06-26 20:18:12,487 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36903
2023-06-26 20:18:12,488 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36903
2023-06-26 20:18:12,488 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41947
2023-06-26 20:18:12,488 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,488 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,488 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,488 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,488 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hb9e6flw
2023-06-26 20:18:12,488 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c4b946b-89f6-41f1-98a7-476a19ee92c8
2023-06-26 20:18:12,553 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40767
2023-06-26 20:18:12,554 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40767
2023-06-26 20:18:12,554 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43575
2023-06-26 20:18:12,554 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,554 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,554 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,554 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,554 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x1c7cefs
2023-06-26 20:18:12,556 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25d6b428-28e6-4703-84f2-1867c41119d5
2023-06-26 20:18:12,556 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39509
2023-06-26 20:18:12,556 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39509
2023-06-26 20:18:12,556 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46591
2023-06-26 20:18:12,557 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,557 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,557 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,557 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,557 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pxg02qrj
2023-06-26 20:18:12,558 - distributed.worker - INFO - Starting Worker plugin PreImport-74418682-1484-428c-97a6-154ab1c07bc0
2023-06-26 20:18:12,558 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b3992b2-a5a7-4244-a637-6a83e283252d
2023-06-26 20:18:12,567 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39157
2023-06-26 20:18:12,567 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39157
2023-06-26 20:18:12,567 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35129
2023-06-26 20:18:12,567 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:18:12,567 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:12,567 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:18:12,567 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:18:12,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jpy_3wae
2023-06-26 20:18:12,568 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56c21736-d40a-462a-8b05-6a0f2e56c682
2023-06-26 20:18:14,750 - distributed.worker - INFO - Starting Worker plugin PreImport-ab1f5d8f-5996-43f2-b8b6-c266688e79e2
2023-06-26 20:18:14,753 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,780 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:14,780 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,782 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:14,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31bc7386-1711-4dd3-9cb4-7beb0866c681
2023-06-26 20:18:14,859 - distributed.worker - INFO - Starting Worker plugin PreImport-33031acd-345f-4684-86ad-6edbc32552d9
2023-06-26 20:18:14,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,876 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:14,876 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,877 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:14,897 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a0d17c2-6033-4964-b50c-8c04d13e56aa
2023-06-26 20:18:14,897 - distributed.worker - INFO - Starting Worker plugin PreImport-95115ae8-e93f-4104-aa8c-c28a13a65c99
2023-06-26 20:18:14,899 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,926 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:14,926 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,929 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:14,959 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1979e33a-45a9-4223-b766-8f2f4afdf59a
2023-06-26 20:18:14,960 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,975 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:14,975 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,976 - distributed.worker - INFO - Starting Worker plugin PreImport-3049cf39-d309-4268-8800-2ca3b722d57f
2023-06-26 20:18:14,977 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:14,979 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,987 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-caa56a9e-4794-4153-aa40-dbd324562fc5
2023-06-26 20:18:14,987 - distributed.worker - INFO - Starting Worker plugin PreImport-73d9c59e-b3f8-412d-9486-005661a9a30b
2023-06-26 20:18:14,987 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e97c1a0-0da8-4376-9597-b0c1d36c21fb
2023-06-26 20:18:14,988 - distributed.worker - INFO - Starting Worker plugin PreImport-69dadc4f-c316-46da-8bdb-1230ff959d06
2023-06-26 20:18:14,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81bb1b58-318b-4fa9-94c8-0ce50f0e190a
2023-06-26 20:18:14,989 - distributed.worker - INFO - Starting Worker plugin PreImport-a60aa7bc-2a7c-4486-963e-e519b4281999
2023-06-26 20:18:14,989 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,989 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,989 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6319ab6b-143d-4d7f-bd84-5842256c5125
2023-06-26 20:18:14,991 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:14,991 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:15,000 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56b4a5ec-0d49-415a-bcc5-e84e2839cd70
2023-06-26 20:18:15,001 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:15,001 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:15,002 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:15,002 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:15,003 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:15,003 - distributed.worker - INFO - Starting Worker plugin PreImport-89d6e3be-fcc5-41fe-a1e6-99d7d712009b
2023-06-26 20:18:15,004 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:15,005 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:15,012 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:15,012 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:15,013 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:15,013 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:15,013 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:15,014 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:15,014 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:15,015 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:15,016 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:15,018 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:18:15,018 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:18:15,020 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:18:24,412 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,422 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,435 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,486 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,566 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,676 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,747 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,758 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,779 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,793 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,817 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,834 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,837 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,903 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:24,931 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:24,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:27,204 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:18:27,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:27,215 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,215 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,215 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,215 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,215 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,215 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,215 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,215 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,216 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,216 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,216 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,216 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,216 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,216 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,216 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,216 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:18:27,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,225 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,226 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687810707.226088] [exp01:478923:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:32908) failed: Address already in use
2023-06-26 20:18:27,226 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,226 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,226 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,226 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,226 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,226 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:18:27,226 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687810707.226479] [exp01:478895:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:52145) failed: Address already in use
2023-06-26 20:18:27,236 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,236 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:27,237 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:18:30,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:36,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:36,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,723 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,724 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,724 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,724 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,724 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,724 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:37,724 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,520 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,521 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,521 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,521 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,521 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,521 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,521 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:18:49,542 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:19:58,978 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45247. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,978 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42399. Reason: worker-close
2023-06-26 20:19:58,978 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46697. Reason: worker-close
2023-06-26 20:19:58,978 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36903. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,978 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40767. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,978 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39663. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,978 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39509. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39963. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39395. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34673. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34963. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39451. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39957. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41131. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39157. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:19:58,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45345'. Reason: nanny-close
2023-06-26 20:19:58,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,980 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:51694 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,980 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:43644 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,982 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39809'. Reason: nanny-close
2023-06-26 20:19:58,984 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,984 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38101'. Reason: nanny-close
2023-06-26 20:19:58,984 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39391'. Reason: nanny-close
2023-06-26 20:19:58,985 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42685'. Reason: nanny-close
2023-06-26 20:19:58,985 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43505'. Reason: nanny-close
2023-06-26 20:19:58,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44877'. Reason: nanny-close
2023-06-26 20:19:58,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33739'. Reason: nanny-close
2023-06-26 20:19:58,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34499'. Reason: nanny-close
2023-06-26 20:19:58,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34961'. Reason: nanny-close
2023-06-26 20:19:58,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37755'. Reason: nanny-close
2023-06-26 20:19:58,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42383'. Reason: nanny-close
2023-06-26 20:19:58,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42393'. Reason: nanny-close
2023-06-26 20:19:58,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45051'. Reason: nanny-close
2023-06-26 20:19:58,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43241'. Reason: nanny-close
2023-06-26 20:19:58,991 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:19:58,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46629'. Reason: nanny-close
2023-06-26 20:19:58,991 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
Process Dask Worker process (from Nanny):
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 202, in _run
    target(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 999, in _run
    asyncio.run(run())
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 47, in run
    _cancel_all_tasks(loop)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 63, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 685, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 919, in _run
    val = self.callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/system_monitor.py", line 199, in update
    result["num_fds"] = self.proc.num_fds()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/psutil/__init__.py", line 760, in num_fds
    return self._proc.num_fds()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/psutil/_pslinux.py", line 1653, in wrapper
    return fun(self, *args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/psutil/_pslinux.py", line 2249, in num_fds
    return len(os.listdir("%s/%s/fd" % (self._procfs_path, self.pid)))
KeyboardInterrupt
2023-06-26 20:19:59,002 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45345 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41592 remote=tcp://10.120.104.11:45345>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45345 after 100 s
2023-06-26 20:19:59,004 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39809 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:51508 remote=tcp://10.120.104.11:39809>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39809 after 100 s
2023-06-26 20:19:59,005 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38101 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45554 remote=tcp://10.120.104.11:38101>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38101 after 100 s
2023-06-26 20:19:59,005 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43505 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46112 remote=tcp://10.120.104.11:43505>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43505 after 100 s
2023-06-26 20:19:59,006 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42685 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:58718 remote=tcp://10.120.104.11:42685>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42685 after 100 s
2023-06-26 20:19:59,006 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39391 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:57260 remote=tcp://10.120.104.11:39391>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39391 after 100 s
2023-06-26 20:19:59,007 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42393 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48192 remote=tcp://10.120.104.11:42393>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42393 after 100 s
2023-06-26 20:19:59,007 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46629 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36644 remote=tcp://10.120.104.11:46629>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46629 after 100 s
2023-06-26 20:19:59,008 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44877 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:38902 remote=tcp://10.120.104.11:44877>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44877 after 100 s
2023-06-26 20:19:59,008 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42383 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46984 remote=tcp://10.120.104.11:42383>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42383 after 100 s
2023-06-26 20:19:59,008 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33739 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55920 remote=tcp://10.120.104.11:33739>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33739 after 100 s
2023-06-26 20:19:59,009 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34961 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33688 remote=tcp://10.120.104.11:34961>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34961 after 100 s
2023-06-26 20:19:59,010 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37755 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34882 remote=tcp://10.120.104.11:37755>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37755 after 100 s
2023-06-26 20:19:59,010 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34499 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41350 remote=tcp://10.120.104.11:34499>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34499 after 100 s
2023-06-26 20:19:59,011 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45051 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:57680 remote=tcp://10.120.104.11:45051>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45051 after 100 s
2023-06-26 20:19:59,486 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46115. Reason: worker-close
2023-06-26 20:19:59,487 - distributed.core - ERROR - Event loop is closed
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 955, in run
    await worker.finished()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1534, in close
    self.status = Status.closing
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1011, in status
    self._send_worker_status_change(stimulus_id)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1022, in _send_worker_status_change
    self.batched_send(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1167, in batched_send
    self.batched_stream.send(msg)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 162, in send
    self.waker.set()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/locks.py", line 222, in set
    fut.set_result(None)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 753, in call_soon
    self._check_closed()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Exception ignored in: <coroutine object WorkerProcess._run.<locals>.run at 0x7fc92f5d07b0>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 939, in run
    async with worker:
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 644, in __aexit__
    await self.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1534, in close
    self.status = Status.closing
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1011, in status
    self._send_worker_status_change(stimulus_id)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1022, in _send_worker_status_change
    self.batched_send(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1167, in batched_send
    self.batched_stream.send(msg)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 162, in send
    self.waker.set()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/locks.py", line 222, in set
    fut.set_result(None)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 753, in call_soon
    self._check_closed()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Task was destroyed but it is pending!
task: <Task pending name='Task-11' coro=<Worker.handle_scheduler() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py:209> wait_for=<Future cancelled> cb=[IOLoop.add_future.<locals>.<lambda>() at /opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py:685, gather.<locals>._done_callback() at /opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py:720]>
2023-06-26 20:19:59,489 - distributed.worker - ERROR - <asyncio.locks.Event object at 0x7fc92f605e40 [unset]> is bound to a different event loop
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1304, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1025, in handle_stream
    await comm.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 210, in wrapper
    future = _create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 147, in _create_future
    future = Future()  # type: Future
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 656, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'MainThread'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1510, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 592, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 211, in wait
    fut = self._get_loop().create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/mixins.py", line 30, in _get_loop
    raise RuntimeError(f'{self!r} is bound to a different event loop')
RuntimeError: <asyncio.locks.Event object at 0x7fc92f605e40 [unset]> is bound to a different event loop
2023-06-26 20:19:59,490 - distributed.worker - CRITICAL - Error trying close worker in response to broken internal state. Forcibly exiting worker NOW
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1304, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1025, in handle_stream
    await comm.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 210, in wrapper
    future = _create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 147, in _create_future
    future = Future()  # type: Future
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 656, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'MainThread'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 209, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1306, in handle_scheduler
    await self.close(reason="worker-handle-scheduler-connection-broken")
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1510, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 592, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 211, in wait
    fut = self._get_loop().create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/mixins.py", line 30, in _get_loop
    raise RuntimeError(f'{self!r} is bound to a different event loop')
RuntimeError: <asyncio.locks.Event object at 0x7fc92f605e40 [unset]> is bound to a different event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 241, in _force_close
    await wait_for(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 405, in wait_for
    loop = events.get_running_loop()
RuntimeError: no running event loop
2023-06-26 20:20:00,035 - distributed.nanny - INFO - Worker process 478905 exited with status 1
2023-06-26 20:20:02,196 - distributed.nanny - WARNING - Worker process still alive after 3.199997711181641 seconds, killing
2023-06-26 20:20:02,196 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing
2023-06-26 20:20:02,197 - distributed.nanny - WARNING - Worker process still alive after 3.1999981689453127 seconds, killing
2023-06-26 20:20:02,197 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,197 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,199 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,200 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,201 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 20:20:02,201 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,202 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,202 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,203 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:20:02,203 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,204 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:20:02,204 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:20:02,982 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,985 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,985 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,985 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,986 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,987 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,987 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,988 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,988 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,988 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,990 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,990 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,990 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,990 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,992 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478923 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478920 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478918 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478914 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478911 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478908 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478901 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478895 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478892 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478852 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478849 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478846 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478844 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478817 parent=475049 started daemon>
2023-06-26 20:20:02,994 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=478814 parent=475049 started daemon>
2023-06-26 20:20:06,705 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 478849 exit status was already read will report exitcode 255
2023-06-26 20:20:07,468 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 478895 exit status was already read will report exitcode 255
2023-06-26 20:20:08,423 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 478920 exit status was already read will report exitcode 255
2023-06-26 20:20:08,696 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 478814 exit status was already read will report exitcode 255
2023-06-26 20:20:09,464 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 478901 exit status was already read will report exitcode 255
