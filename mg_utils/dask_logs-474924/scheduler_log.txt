RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 20:14:56,211 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 20:14:56,719 - distributed.scheduler - INFO - State start
2023-06-26 20:14:56,720 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_0d_a2dh', purging
2023-06-26 20:14:56,721 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-98iyj2mu', purging
2023-06-26 20:14:56,721 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qpmoavlm', purging
2023-06-26 20:14:56,721 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-z5jdakj7', purging
2023-06-26 20:14:56,721 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-f3ov14a6', purging
2023-06-26 20:14:56,721 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8b6211af', purging
2023-06-26 20:14:56,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2v0wj2ru', purging
2023-06-26 20:14:56,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7_ozrorx', purging
2023-06-26 20:14:56,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-e1sfn63g', purging
2023-06-26 20:14:56,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-l3ffyt29', purging
2023-06-26 20:14:56,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4ks9c8j5', purging
2023-06-26 20:14:56,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-i6axznvj', purging
2023-06-26 20:14:56,723 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ecc90bif', purging
2023-06-26 20:14:56,723 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-h2gthn6b', purging
2023-06-26 20:14:56,723 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6ma5m6at', purging
2023-06-26 20:14:56,723 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-wh7r6hi5', purging
2023-06-26 20:14:56,735 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 20:14:56,736 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 20:14:56,736 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 20:15:15,023 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33345', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33345
2023-06-26 20:15:15,026 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33774
2023-06-26 20:15:15,050 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44243', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,051 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44243
2023-06-26 20:15:15,051 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33782
2023-06-26 20:15:15,136 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41327', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,137 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41327
2023-06-26 20:15:15,137 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33796
2023-06-26 20:15:15,245 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39077', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,245 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39077
2023-06-26 20:15:15,246 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33798
2023-06-26 20:15:15,285 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42715', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,285 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42715
2023-06-26 20:15:15,285 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33800
2023-06-26 20:15:15,345 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38347', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,346 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38347
2023-06-26 20:15:15,346 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33814
2023-06-26 20:15:15,351 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34005', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,351 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34005
2023-06-26 20:15:15,351 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33826
2023-06-26 20:15:15,414 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38723', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,414 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38723
2023-06-26 20:15:15,414 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33840
2023-06-26 20:15:15,507 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42335', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,508 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42335
2023-06-26 20:15:15,508 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33844
2023-06-26 20:15:15,513 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36101', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,513 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36101
2023-06-26 20:15:15,513 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33854
2023-06-26 20:15:15,545 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42809', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,546 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42809
2023-06-26 20:15:15,546 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33858
2023-06-26 20:15:15,547 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35249', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,547 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35249
2023-06-26 20:15:15,548 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33866
2023-06-26 20:15:15,549 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33257', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33257
2023-06-26 20:15:15,550 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33872
2023-06-26 20:15:15,560 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43775', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,560 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43775
2023-06-26 20:15:15,560 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33884
2023-06-26 20:15:15,570 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41637', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,571 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41637
2023-06-26 20:15:15,571 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33894
2023-06-26 20:15:15,573 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39577', status: init, memory: 0, processing: 0>
2023-06-26 20:15:15,573 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39577
2023-06-26 20:15:15,573 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33888
2023-06-26 20:15:35,295 - distributed.scheduler - INFO - Receive client connection: Client-358b1e1a-145e-11ee-81ce-5cff35c1a711
2023-06-26 20:15:35,295 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58034
2023-06-26 20:15:36,029 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 20:16:28,081 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:17:56,439 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 20:17:56,440 - distributed.scheduler - INFO - Restarting workers and releasing all keys.
2023-06-26 20:17:56,482 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33872; closing.
2023-06-26 20:17:56,483 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33257', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,483 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33257
2023-06-26 20:17:56,484 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33774; closing.
2023-06-26 20:17:56,484 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33826; closing.
2023-06-26 20:17:56,484 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33866; closing.
2023-06-26 20:17:56,484 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33854; closing.
2023-06-26 20:17:56,484 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33814; closing.
2023-06-26 20:17:56,485 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33840; closing.
2023-06-26 20:17:56,485 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33798; closing.
2023-06-26 20:17:56,485 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33345', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33345
2023-06-26 20:17:56,486 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34005', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34005
2023-06-26 20:17:56,486 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35249', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35249
2023-06-26 20:17:56,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36101', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,487 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36101
2023-06-26 20:17:56,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38347', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,487 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38347
2023-06-26 20:17:56,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38723', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,488 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38723
2023-06-26 20:17:56,488 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39077', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,488 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39077
2023-06-26 20:17:56,488 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33888; closing.
2023-06-26 20:17:56,488 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33796; closing.
2023-06-26 20:17:56,489 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39577', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,489 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39577
2023-06-26 20:17:56,489 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41327', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,490 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41327
2023-06-26 20:17:56,490 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33894; closing.
2023-06-26 20:17:56,490 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33844; closing.
2023-06-26 20:17:56,490 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:33774>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:17:56,492 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:33826>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:17:56,492 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:33866>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:17:56,492 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:33854>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:17:56,492 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:33814>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:17:56,492 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:33840>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:17:56,492 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:33798>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:17:56,493 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41637', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,493 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41637
2023-06-26 20:17:56,493 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42335', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,493 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42335
2023-06-26 20:17:56,494 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33800; closing.
2023-06-26 20:17:56,494 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42715', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,494 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42715
2023-06-26 20:17:56,495 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:33800>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:17:56,506 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33782; closing.
2023-06-26 20:17:56,506 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44243', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,506 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44243
2023-06-26 20:17:56,514 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33858; closing.
2023-06-26 20:17:56,514 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42809', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,514 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42809
2023-06-26 20:17:56,536 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33884; closing.
2023-06-26 20:17:56,536 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43775', status: closing, memory: 0, processing: 0>
2023-06-26 20:17:56,536 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43775
2023-06-26 20:17:56,536 - distributed.scheduler - INFO - Lost all workers
2023-06-26 20:18:07,760 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46697', status: init, memory: 0, processing: 0>
2023-06-26 20:18:07,761 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46697
2023-06-26 20:18:07,761 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:43644
2023-06-26 20:18:09,105 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34673', status: init, memory: 0, processing: 0>
2023-06-26 20:18:09,106 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34673
2023-06-26 20:18:09,106 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51658
2023-06-26 20:18:09,117 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34963', status: init, memory: 0, processing: 0>
2023-06-26 20:18:09,117 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34963
2023-06-26 20:18:09,117 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51682
2023-06-26 20:18:09,122 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41131', status: init, memory: 0, processing: 0>
2023-06-26 20:18:09,122 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41131
2023-06-26 20:18:09,123 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51674
2023-06-26 20:18:09,159 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42399', status: init, memory: 0, processing: 0>
2023-06-26 20:18:09,160 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42399
2023-06-26 20:18:09,160 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51694
2023-06-26 20:18:09,197 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39663', status: init, memory: 0, processing: 0>
2023-06-26 20:18:09,197 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39663
2023-06-26 20:18:09,197 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51708
2023-06-26 20:18:14,779 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39395', status: init, memory: 0, processing: 0>
2023-06-26 20:18:14,779 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39395
2023-06-26 20:18:14,780 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51772
2023-06-26 20:18:14,875 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:45247', status: init, memory: 0, processing: 0>
2023-06-26 20:18:14,875 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:45247
2023-06-26 20:18:14,876 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51786
2023-06-26 20:18:14,926 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39451', status: init, memory: 0, processing: 0>
2023-06-26 20:18:14,926 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39451
2023-06-26 20:18:14,926 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51788
2023-06-26 20:18:14,975 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46115', status: init, memory: 0, processing: 0>
2023-06-26 20:18:14,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46115
2023-06-26 20:18:14,975 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51800
2023-06-26 20:18:15,001 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36903', status: init, memory: 0, processing: 0>
2023-06-26 20:18:15,001 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36903
2023-06-26 20:18:15,001 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51810
2023-06-26 20:18:15,002 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39963', status: init, memory: 0, processing: 0>
2023-06-26 20:18:15,002 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39963
2023-06-26 20:18:15,002 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51804
2023-06-26 20:18:15,011 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40767', status: init, memory: 0, processing: 0>
2023-06-26 20:18:15,011 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40767
2023-06-26 20:18:15,012 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51812
2023-06-26 20:18:15,012 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39509', status: init, memory: 0, processing: 0>
2023-06-26 20:18:15,012 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39509
2023-06-26 20:18:15,012 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51822
2023-06-26 20:18:15,013 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39957', status: init, memory: 0, processing: 0>
2023-06-26 20:18:15,013 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39957
2023-06-26 20:18:15,013 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51830
2023-06-26 20:18:15,017 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39157', status: init, memory: 0, processing: 0>
2023-06-26 20:18:15,018 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39157
2023-06-26 20:18:15,018 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:51844
2023-06-26 20:18:15,049 - distributed.scheduler - INFO - Restarting finished.
2023-06-26 20:18:27,229 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 20:18:49,959 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 20:18:49,960 - distributed.scheduler - INFO - Remove client Client-358b1e1a-145e-11ee-81ce-5cff35c1a711
2023-06-26 20:18:49,961 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:58034; closing.
2023-06-26 20:18:49,961 - distributed.scheduler - INFO - Remove client Client-358b1e1a-145e-11ee-81ce-5cff35c1a711
2023-06-26 20:18:49,961 - distributed.scheduler - INFO - Close client connection: Client-358b1e1a-145e-11ee-81ce-5cff35c1a711
2023-06-26 20:19:58,979 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 20:19:58,980 - distributed.core - INFO - Connection to tcp://10.120.104.11:51786 has been closed.
2023-06-26 20:19:58,980 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:45247', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45247
2023-06-26 20:19:58,981 - distributed.core - INFO - Connection to tcp://10.120.104.11:51812 has been closed.
2023-06-26 20:19:58,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40767', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,981 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40767
2023-06-26 20:19:58,981 - distributed.core - INFO - Connection to tcp://10.120.104.11:51810 has been closed.
2023-06-26 20:19:58,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36903', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,981 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36903
2023-06-26 20:19:58,981 - distributed.core - INFO - Connection to tcp://10.120.104.11:51804 has been closed.
2023-06-26 20:19:58,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39963', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,981 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39963
2023-06-26 20:19:58,982 - distributed.core - INFO - Connection to tcp://10.120.104.11:51772 has been closed.
2023-06-26 20:19:58,982 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39395', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,982 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39395
2023-06-26 20:19:58,982 - distributed.core - INFO - Connection to tcp://10.120.104.11:51822 has been closed.
2023-06-26 20:19:58,982 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39509', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,982 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39509
2023-06-26 20:19:58,982 - distributed.core - INFO - Connection to tcp://10.120.104.11:51708 has been closed.
2023-06-26 20:19:58,982 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39663', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,982 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39663
2023-06-26 20:19:58,984 - distributed.core - INFO - Connection to tcp://10.120.104.11:51674 has been closed.
2023-06-26 20:19:58,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41131', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,984 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41131
2023-06-26 20:19:58,984 - distributed.core - INFO - Connection to tcp://10.120.104.11:51682 has been closed.
2023-06-26 20:19:58,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34963', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,984 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34963
2023-06-26 20:19:58,984 - distributed.core - INFO - Connection to tcp://10.120.104.11:51830 has been closed.
2023-06-26 20:19:58,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39957', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,984 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39957
2023-06-26 20:19:58,985 - distributed.core - INFO - Connection to tcp://10.120.104.11:51788 has been closed.
2023-06-26 20:19:58,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39451', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,985 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39451
2023-06-26 20:19:58,985 - distributed.core - INFO - Connection to tcp://10.120.104.11:51658 has been closed.
2023-06-26 20:19:58,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34673', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,985 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34673
2023-06-26 20:19:58,985 - distributed.core - INFO - Connection to tcp://10.120.104.11:51844 has been closed.
2023-06-26 20:19:58,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39157', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,985 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39157
2023-06-26 20:19:58,985 - distributed.core - INFO - Connection to tcp://10.120.104.11:51694 has been closed.
2023-06-26 20:19:58,986 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42399', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,986 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42399
2023-06-26 20:19:58,986 - distributed.core - INFO - Connection to tcp://10.120.104.11:43644 has been closed.
2023-06-26 20:19:58,986 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46697', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,986 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46697
2023-06-26 20:19:58,986 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 20:19:58,986 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:51658>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,987 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:51682>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,987 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:51844>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,987 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:51788>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,987 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:51830>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,987 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:51674>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,987 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:51694>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,987 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:43644>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:19:58,988 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 20:19:58,989 - distributed.core - INFO - Connection to tcp://10.120.104.11:51800 has been closed.
2023-06-26 20:19:58,989 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46115', status: running, memory: 0, processing: 0>
2023-06-26 20:19:58,989 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46115
2023-06-26 20:19:58,989 - distributed.scheduler - INFO - Lost all workers
2023-06-26 20:19:58,995 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 20:19:58,996 - distributed.scheduler - INFO - End scheduler
