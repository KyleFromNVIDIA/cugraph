RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 23:06:41,340 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 23:06:41,835 - distributed.scheduler - INFO - State start
2023-06-22 23:06:41,837 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-n7yry5q1', purging
2023-06-22 23:06:41,837 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ehe9nd_o', purging
2023-06-22 23:06:41,837 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-87m91774', purging
2023-06-22 23:06:41,837 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ah2unmk4', purging
2023-06-22 23:06:41,838 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-338l1c19', purging
2023-06-22 23:06:41,838 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8_ebzxjh', purging
2023-06-22 23:06:41,838 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qqcs8fn1', purging
2023-06-22 23:06:41,838 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-d70km_ug', purging
2023-06-22 23:06:41,848 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 23:06:41,849 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 23:06:41,849 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 23:06:52,935 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:44207', status: init, memory: 0, processing: 0>
2023-06-22 23:06:53,192 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:44207
2023-06-22 23:06:53,192 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:54896
2023-06-22 23:06:53,193 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:46091', status: init, memory: 0, processing: 0>
2023-06-22 23:06:53,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:46091
2023-06-22 23:06:53,194 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:54898
2023-06-22 23:06:53,202 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:39563', status: init, memory: 0, processing: 0>
2023-06-22 23:06:53,203 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:39563
2023-06-22 23:06:53,203 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:54940
2023-06-22 23:06:53,204 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:33651', status: init, memory: 0, processing: 0>
2023-06-22 23:06:53,204 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:33651
2023-06-22 23:06:53,204 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:54942
2023-06-22 23:06:53,205 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:35587', status: init, memory: 0, processing: 0>
2023-06-22 23:06:53,205 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:35587
2023-06-22 23:06:53,205 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:54920
2023-06-22 23:06:53,206 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:43217', status: init, memory: 0, processing: 0>
2023-06-22 23:06:53,206 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:43217
2023-06-22 23:06:53,206 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:54930
2023-06-22 23:06:53,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:41229', status: init, memory: 0, processing: 0>
2023-06-22 23:06:53,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:41229
2023-06-22 23:06:53,207 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:54908
2023-06-22 23:06:53,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:43165', status: init, memory: 0, processing: 0>
2023-06-22 23:06:53,208 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:43165
2023-06-22 23:06:53,208 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:54924
2023-06-22 23:06:59,640 - distributed.scheduler - INFO - Receive client connection: Client-7dd6f55d-1151-11ee-a238-d8c49778ced7
2023-06-22 23:06:59,640 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:39874
2023-06-22 23:06:59,747 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 23:07:50,440 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:07:52,694 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 23:07:56,640 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:11:55,444 - distributed.scheduler - INFO - Remove client Client-7dd6f55d-1151-11ee-a238-d8c49778ced7
2023-06-22 23:11:55,449 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:39874; closing.
2023-06-22 23:11:55,449 - distributed.scheduler - INFO - Remove client Client-7dd6f55d-1151-11ee-a238-d8c49778ced7
2023-06-22 23:11:55,451 - distributed.scheduler - INFO - Close client connection: Client-7dd6f55d-1151-11ee-a238-d8c49778ced7
2023-06-22 23:12:08,944 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 23:12:08,945 - distributed.core - INFO - Connection to tcp://10.33.227.169:54896 has been closed.
2023-06-22 23:12:08,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:44207', status: running, memory: 0, processing: 0>
2023-06-22 23:12:08,946 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:44207
2023-06-22 23:12:08,946 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 23:12:08,948 - distributed.core - INFO - Connection to tcp://10.33.227.169:54898 has been closed.
2023-06-22 23:12:08,949 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:46091', status: running, memory: 0, processing: 0>
2023-06-22 23:12:08,949 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:46091
2023-06-22 23:12:08,949 - distributed.core - INFO - Connection to tcp://10.33.227.169:54924 has been closed.
2023-06-22 23:12:08,949 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:43165', status: running, memory: 0, processing: 0>
2023-06-22 23:12:08,949 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:43165
2023-06-22 23:12:08,950 - distributed.core - INFO - Connection to tcp://10.33.227.169:54908 has been closed.
2023-06-22 23:12:08,950 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:41229', status: running, memory: 0, processing: 0>
2023-06-22 23:12:08,950 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:41229
2023-06-22 23:12:08,950 - distributed.core - INFO - Connection to tcp://10.33.227.169:54940 has been closed.
2023-06-22 23:12:08,950 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:39563', status: running, memory: 0, processing: 0>
2023-06-22 23:12:08,950 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:39563
2023-06-22 23:12:08,951 - distributed.core - INFO - Connection to tcp://10.33.227.169:54930 has been closed.
2023-06-22 23:12:08,951 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:43217', status: running, memory: 0, processing: 0>
2023-06-22 23:12:08,951 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:43217
2023-06-22 23:12:08,951 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54940>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54940>: Stream is closed
2023-06-22 23:12:08,953 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54908>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54908>: Stream is closed
2023-06-22 23:12:08,953 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54924>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54924>: Stream is closed
2023-06-22 23:12:08,953 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54930>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54930>: Stream is closed
2023-06-22 23:12:08,953 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54898>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:54898>: Stream is closed
2023-06-22 23:12:08,954 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 23:12:08,955 - distributed.core - INFO - Connection to tcp://10.33.227.169:54942 has been closed.
2023-06-22 23:12:08,955 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:33651', status: running, memory: 0, processing: 0>
2023-06-22 23:12:08,955 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:33651
2023-06-22 23:12:08,956 - distributed.core - INFO - Connection to tcp://10.33.227.169:54920 has been closed.
2023-06-22 23:12:08,956 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:35587', status: running, memory: 0, processing: 0>
2023-06-22 23:12:08,956 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:35587
2023-06-22 23:12:08,956 - distributed.scheduler - INFO - Lost all workers
2023-06-22 23:12:09,760 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 23:12:09,760 - distributed.scheduler - INFO - End scheduler
