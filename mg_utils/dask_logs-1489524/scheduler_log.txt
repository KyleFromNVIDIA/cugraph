RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 22:55:14,950 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 22:55:15,430 - distributed.scheduler - INFO - State start
2023-06-22 22:55:15,431 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-f8agvcgk', purging
2023-06-22 22:55:15,431 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-y33c_v2_', purging
2023-06-22 22:55:15,431 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0gxq_p5n', purging
2023-06-22 22:55:15,432 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ijz72abh', purging
2023-06-22 22:55:15,432 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qnlhlqz_', purging
2023-06-22 22:55:15,432 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xo19b72h', purging
2023-06-22 22:55:15,432 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1zh7iz8q', purging
2023-06-22 22:55:15,441 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 22:55:15,442 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 22:55:15,442 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 22:55:26,587 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:44275', status: init, memory: 0, processing: 0>
2023-06-22 22:55:26,848 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:44275
2023-06-22 22:55:26,848 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:38768
2023-06-22 22:55:26,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:45867', status: init, memory: 0, processing: 0>
2023-06-22 22:55:26,857 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:45867
2023-06-22 22:55:26,857 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:38798
2023-06-22 22:55:26,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:34495', status: init, memory: 0, processing: 0>
2023-06-22 22:55:26,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:34495
2023-06-22 22:55:26,859 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:38804
2023-06-22 22:55:26,859 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:43069', status: init, memory: 0, processing: 0>
2023-06-22 22:55:26,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:43069
2023-06-22 22:55:26,860 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:38782
2023-06-22 22:55:26,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:35141', status: init, memory: 0, processing: 0>
2023-06-22 22:55:26,861 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:35141
2023-06-22 22:55:26,861 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:38816
2023-06-22 22:55:26,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:34325', status: init, memory: 0, processing: 0>
2023-06-22 22:55:26,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:34325
2023-06-22 22:55:26,862 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:38808
2023-06-22 22:55:26,862 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:34837', status: init, memory: 0, processing: 0>
2023-06-22 22:55:26,863 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:34837
2023-06-22 22:55:26,863 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:38820
2023-06-22 22:55:26,865 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:36755', status: init, memory: 0, processing: 0>
2023-06-22 22:55:26,866 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:36755
2023-06-22 22:55:26,866 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:38814
2023-06-22 22:55:34,034 - distributed.scheduler - INFO - Receive client connection: Client-e52fd5f8-114f-11ee-bb93-d8c49778ced7
2023-06-22 22:55:34,035 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59326
2023-06-22 22:55:34,137 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 22:56:22,666 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:56:24,882 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 22:56:28,930 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:57:42,725 - distributed.scheduler - INFO - Remove client Client-e52fd5f8-114f-11ee-bb93-d8c49778ced7
2023-06-22 22:57:42,730 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:59326; closing.
2023-06-22 22:57:42,731 - distributed.scheduler - INFO - Remove client Client-e52fd5f8-114f-11ee-bb93-d8c49778ced7
2023-06-22 22:57:42,732 - distributed.scheduler - INFO - Close client connection: Client-e52fd5f8-114f-11ee-bb93-d8c49778ced7
2023-06-22 22:57:45,286 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 22:57:45,287 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 22:57:45,287 - distributed.core - INFO - Connection to tcp://10.33.227.169:38782 has been closed.
2023-06-22 22:57:45,287 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:43069', status: running, memory: 0, processing: 0>
2023-06-22 22:57:45,288 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:43069
2023-06-22 22:57:45,289 - distributed.core - INFO - Connection to tcp://10.33.227.169:38768 has been closed.
2023-06-22 22:57:45,289 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:44275', status: running, memory: 0, processing: 0>
2023-06-22 22:57:45,290 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:44275
2023-06-22 22:57:45,290 - distributed.core - INFO - Connection to tcp://10.33.227.169:38816 has been closed.
2023-06-22 22:57:45,290 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:35141', status: running, memory: 0, processing: 0>
2023-06-22 22:57:45,290 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:35141
2023-06-22 22:57:45,290 - distributed.core - INFO - Connection to tcp://10.33.227.169:38798 has been closed.
2023-06-22 22:57:45,290 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:45867', status: running, memory: 0, processing: 0>
2023-06-22 22:57:45,290 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:45867
2023-06-22 22:57:45,291 - distributed.core - INFO - Connection to tcp://10.33.227.169:38820 has been closed.
2023-06-22 22:57:45,292 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:34837', status: running, memory: 0, processing: 0>
2023-06-22 22:57:45,292 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:34837
2023-06-22 22:57:45,292 - distributed.core - INFO - Connection to tcp://10.33.227.169:38814 has been closed.
2023-06-22 22:57:45,292 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:36755', status: running, memory: 0, processing: 0>
2023-06-22 22:57:45,292 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:36755
2023-06-22 22:57:45,293 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 22:57:45,293 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:38820>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:38820>: Stream is closed
2023-06-22 22:57:45,295 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:38816>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 22:57:45,295 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:38814>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:38814>: Stream is closed
2023-06-22 22:57:45,295 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:38768>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 22:57:45,296 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:38798>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 22:57:45,297 - distributed.core - INFO - Connection to tcp://10.33.227.169:38808 has been closed.
2023-06-22 22:57:45,297 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:34325', status: running, memory: 0, processing: 0>
2023-06-22 22:57:45,297 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:34325
2023-06-22 22:57:45,297 - distributed.core - INFO - Connection to tcp://10.33.227.169:38804 has been closed.
2023-06-22 22:57:45,297 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:34495', status: running, memory: 0, processing: 0>
2023-06-22 22:57:45,297 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:34495
2023-06-22 22:57:45,297 - distributed.scheduler - INFO - Lost all workers
2023-06-22 22:57:46,149 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 22:57:46,150 - distributed.scheduler - INFO - End scheduler
