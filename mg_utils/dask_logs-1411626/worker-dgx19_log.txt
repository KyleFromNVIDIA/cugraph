RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=12G
             --local-directory=/tmp/
             --scheduler-file=/root/work/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-22 21:11:47,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:38045'
2023-06-22 21:11:47,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:46735'
2023-06-22 21:11:47,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:41587'
2023-06-22 21:11:47,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:41159'
2023-06-22 21:11:47,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:44213'
2023-06-22 21:11:47,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:38551'
2023-06-22 21:11:47,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:37059'
2023-06-22 21:11:47,612 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:44289'
2023-06-22 21:11:49,042 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-y21e2qyl', purging
2023-06-22 21:11:49,043 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-sx6mh9kz', purging
2023-06-22 21:11:49,043 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-aetzqj6q', purging
2023-06-22 21:11:49,043 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yv7835q1', purging
2023-06-22 21:11:49,043 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nwhdplpa', purging
2023-06-22 21:11:49,043 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gakepndj', purging
2023-06-22 21:11:49,044 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ypxprco1', purging
2023-06-22 21:11:49,044 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-g2xc7367', purging
2023-06-22 21:11:49,054 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:11:49,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:11:49,089 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:11:49,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:11:49,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:11:49,129 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:11:49,133 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:11:49,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:11:49,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:11:49,134 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:11:49,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:11:49,134 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:11:49,135 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:11:49,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:11:49,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:11:49,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:11:49,475 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:11:49,512 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:11:49,559 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:11:49,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:11:49,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:11:49,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:11:49,583 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:11:49,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:11:51,034 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:43881
2023-06-22 21:11:51,035 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:43881
2023-06-22 21:11:51,035 - distributed.worker - INFO -          dashboard at:        10.33.227.169:43637
2023-06-22 21:11:51,035 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,035 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,035 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:11:51,035 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:11:51,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j8j3rqxj
2023-06-22 21:11:51,035 - distributed.worker - INFO - Starting Worker plugin PreImport-39325a0f-a81d-4db5-939d-182e16019322
2023-06-22 21:11:51,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5372ff73-71ed-40a2-81b9-64c35080bcf5
2023-06-22 21:11:51,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1a97b52-6ed5-4bfa-945d-7ef43ec9cffe
2023-06-22 21:11:51,280 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,473 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,473 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,476 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:11:51,784 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:41507
2023-06-22 21:11:51,785 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:41507
2023-06-22 21:11:51,785 - distributed.worker - INFO -          dashboard at:        10.33.227.169:38727
2023-06-22 21:11:51,785 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,785 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,785 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:11:51,785 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:11:51,785 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o2nik00d
2023-06-22 21:11:51,786 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21b7ad39-9301-4e2a-861e-34c4451c6d4e
2023-06-22 21:11:51,905 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:37123
2023-06-22 21:11:51,905 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:37123
2023-06-22 21:11:51,906 - distributed.worker - INFO -          dashboard at:        10.33.227.169:43865
2023-06-22 21:11:51,906 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,906 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,906 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:11:51,906 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:11:51,906 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3esxu5t3
2023-06-22 21:11:51,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8358a74b-e75a-49d4-abea-9640e3cb364d
2023-06-22 21:11:51,920 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-638aad54-8f9f-40fe-8da3-b22cd156c823
2023-06-22 21:11:51,920 - distributed.worker - INFO - Starting Worker plugin PreImport-b625ed92-7d1a-4a4b-83ec-606648f81733
2023-06-22 21:11:51,921 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,928 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:40817
2023-06-22 21:11:51,928 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:40817
2023-06-22 21:11:51,928 - distributed.worker - INFO -          dashboard at:        10.33.227.169:33631
2023-06-22 21:11:51,928 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,929 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,929 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:11:51,929 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:11:51,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8qh4h70y
2023-06-22 21:11:51,929 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b801fd5-0c6f-4ac4-bc1e-6656d6685153
2023-06-22 21:11:51,929 - distributed.worker - INFO - Starting Worker plugin PreImport-5fb11315-549f-45fe-9b14-9894578449a8
2023-06-22 21:11:51,929 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88ea22be-af08-4917-b45e-6439cd60e380
2023-06-22 21:11:51,929 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:32837
2023-06-22 21:11:51,930 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:32837
2023-06-22 21:11:51,930 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:37629
2023-06-22 21:11:51,930 - distributed.worker - INFO -          dashboard at:        10.33.227.169:45229
2023-06-22 21:11:51,930 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:37629
2023-06-22 21:11:51,930 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,930 - distributed.worker - INFO -          dashboard at:        10.33.227.169:34137
2023-06-22 21:11:51,930 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,930 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,930 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:11:51,930 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,930 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:11:51,930 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:11:51,930 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:11:51,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m1uvw2fz
2023-06-22 21:11:51,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0ckj2otm
2023-06-22 21:11:51,931 - distributed.worker - INFO - Starting Worker plugin PreImport-5e45093d-91e6-4b5c-94c1-140ea655451c
2023-06-22 21:11:51,931 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3690569e-de23-45dc-8e4d-fe10fd3e6420
2023-06-22 21:11:51,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f967128-4395-43da-9fa8-97ca4d574e6b
2023-06-22 21:11:51,931 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-395b625c-5965-4961-b814-332b6d9a48bd
2023-06-22 21:11:51,931 - distributed.worker - INFO - Starting Worker plugin PreImport-1267a361-fc62-43e2-ab3c-e76f13791705
2023-06-22 21:11:51,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-263405be-610d-4e7b-aaa2-2da7187fe6ff
2023-06-22 21:11:51,934 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,934 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,937 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:11:51,948 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:38285
2023-06-22 21:11:51,948 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:38285
2023-06-22 21:11:51,948 - distributed.worker - INFO -          dashboard at:        10.33.227.169:35611
2023-06-22 21:11:51,948 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,948 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,949 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:11:51,949 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:11:51,949 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z94umjmd
2023-06-22 21:11:51,949 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8442d748-cb93-4ddd-bf99-cff249232031
2023-06-22 21:11:51,949 - distributed.worker - INFO - Starting Worker plugin PreImport-aeac3531-e1e1-430d-b1d4-ab6415a5346e
2023-06-22 21:11:51,950 - distributed.worker - INFO - Starting Worker plugin RMMSetup-524b5021-f4bb-4537-9b5e-9c662e222b0d
2023-06-22 21:11:51,951 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:43643
2023-06-22 21:11:51,951 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:43643
2023-06-22 21:11:51,951 - distributed.worker - INFO -          dashboard at:        10.33.227.169:42101
2023-06-22 21:11:51,951 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:11:51,952 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:51,952 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:11:51,952 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:11:51,952 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h9t89j5d
2023-06-22 21:11:51,952 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5644168f-782e-4527-a517-b88cc1ab072c
2023-06-22 21:11:52,117 - distributed.worker - INFO - Starting Worker plugin PreImport-88c2d019-0cca-4a26-9d40-57870c15cc5b
2023-06-22 21:11:52,117 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-029950d2-6f8c-41d0-a57a-853caf0650ae
2023-06-22 21:11:52,118 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,118 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,118 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,119 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,128 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:11:52,128 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,129 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:11:52,129 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,129 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:11:52,130 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:11:52,131 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:11:52,131 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,133 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:11:52,133 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,134 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:11:52,136 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:11:52,162 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0f0412a-f83d-450a-a757-e774cf601095
2023-06-22 21:11:52,162 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,163 - distributed.worker - INFO - Starting Worker plugin PreImport-d5ce6451-9d9d-43cb-b456-76838bbf3b9f
2023-06-22 21:11:52,164 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,171 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:11:52,172 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,173 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:11:52,178 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:11:52,178 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:11:52,181 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:11:54,027 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:11:54,027 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:11:54,027 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:11:54,028 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:11:54,028 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:11:54,028 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:11:54,028 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:11:54,028 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:11:54,124 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:11:54,124 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:11:54,124 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:11:54,124 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:11:54,124 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:11:54,124 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:11:54,125 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:11:54,125 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:12:05,119 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:12:05,256 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:12:05,330 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:12:05,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:12:05,374 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:12:05,435 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:12:05,482 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:12:05,581 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:12:11,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:12:11,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:12:11,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:12:11,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:12:11,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:12:11,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:12:11,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:12:11,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:12:44,959 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:12:44,959 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:12:44,963 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:12:44,963 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:12:44,964 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:12:44,964 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:12:44,964 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:12:44,964 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:12:49,192 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,192 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,193 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,193 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,193 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,193 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,193 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,193 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,785 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,790 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,790 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,790 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,790 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,790 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,790 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,790 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:12:49,868 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-499cb5d6-2c46-4608-b4ab-39560f1c5f17
Function:  _call_plc_uniform_neighbor_sample
args:      (b',\x1b^\x8d\x0b\xccG\xce\xa8\xc1\x92m\xbd\xdb\xb9\xd5', <pylibcugraph.graphs.MGGraph object at 0x7fec8980f7f0>, < could not convert arg to str >, True, 8, 10, 19, array([10, 25], dtype=int32), False)
kwargs:    {'weight_t': 'float32', 'with_edge_properties': True, 'random_state': 2850353539077388950, 'return_offsets': False}
Exception: "RuntimeError('non-success value returned from cugraph_uniform_neighbor_sample_with_edge_properties: CUGRAPH_UNKNOWN_ERROR for_each: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-06-22 21:12:49,874 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-95f06f52-ecb6-4b6f-b924-c203d624ed26
Function:  _call_plc_uniform_neighbor_sample
args:      (b',\x1b^\x8d\x0b\xccG\xce\xa8\xc1\x92m\xbd\xdb\xb9\xd5', <pylibcugraph.graphs.MGGraph object at 0x7fec467e5b90>, < could not convert arg to str >, True, 8, 10, 19, array([10, 25], dtype=int32), False)
kwargs:    {'weight_t': 'float32', 'with_edge_properties': True, 'random_state': -5364899678283231885, 'return_offsets': False}
Exception: "RuntimeError('non-success value returned from cugraph_uniform_neighbor_sample_with_edge_properties: CUGRAPH_UNKNOWN_ERROR for_each: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-06-22 21:18:40,210 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:37629. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:18:40,211 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:43643. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:18:40,211 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:38045'. Reason: nanny-close
2023-06-22 21:18:40,212 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:18:40,213 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:46735'. Reason: nanny-close
2023-06-22 21:18:40,213 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:18:40,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:41587'. Reason: nanny-close
2023-06-22 21:18:40,214 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:18:40,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:41159'. Reason: nanny-close
2023-06-22 21:18:40,215 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:18:40,215 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:44213'. Reason: nanny-close
2023-06-22 21:18:40,215 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:18:40,217 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:38551'. Reason: nanny-close
2023-06-22 21:18:40,218 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:18:40,218 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:37059'. Reason: nanny-close
2023-06-22 21:18:40,218 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:18:40,219 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:44289'. Reason: nanny-close
2023-06-22 21:18:40,219 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:18:40,233 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:41159 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:59644 remote=tcp://10.33.227.169:41159>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:41159 after 100 s
2023-06-22 21:18:40,239 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:44213 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:43712 remote=tcp://10.33.227.169:44213>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:44213 after 100 s
2023-06-22 21:18:43,420 - distributed.nanny - WARNING - Worker process still alive after 3.199982604980469 seconds, killing
2023-06-22 21:18:43,421 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 21:18:43,421 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 21:18:43,422 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:18:43,423 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 21:18:43,423 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:18:43,423 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 21:18:43,425 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:18:44,106 - distributed.nanny - INFO - Worker process 1411846 was killed by signal 9
2023-06-22 21:18:44,213 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:18:44,215 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:18:44,215 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:18:44,216 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:18:44,218 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:18:44,219 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:18:44,219 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:18:44,221 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1411864 parent=1411779 started daemon>
2023-06-22 21:18:44,221 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1411861 parent=1411779 started daemon>
2023-06-22 21:18:44,221 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1411858 parent=1411779 started daemon>
2023-06-22 21:18:44,221 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1411855 parent=1411779 started daemon>
2023-06-22 21:18:44,221 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1411852 parent=1411779 started daemon>
2023-06-22 21:18:44,221 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1411849 parent=1411779 started daemon>
2023-06-22 21:18:44,221 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1411843 parent=1411779 started daemon>
2023-06-22 21:18:44,587 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1411855 exit status was already read will report exitcode 255
2023-06-22 21:18:44,805 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1411849 exit status was already read will report exitcode 255
2023-06-22 21:18:44,918 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1411861 exit status was already read will report exitcode 255
