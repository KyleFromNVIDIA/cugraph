RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 18:24:56,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34889'
2023-06-26 18:24:56,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43989'
2023-06-26 18:24:56,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37395'
2023-06-26 18:24:56,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41427'
2023-06-26 18:24:56,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35547'
2023-06-26 18:24:56,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42859'
2023-06-26 18:24:56,266 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35285'
2023-06-26 18:24:56,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40253'
2023-06-26 18:24:56,270 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39625'
2023-06-26 18:24:56,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37931'
2023-06-26 18:24:56,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44225'
2023-06-26 18:24:56,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33125'
2023-06-26 18:24:56,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45701'
2023-06-26 18:24:56,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34397'
2023-06-26 18:24:56,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:32881'
2023-06-26 18:24:56,291 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35555'
2023-06-26 18:24:57,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:57,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:57,942 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:57,943 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:57,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:57,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:57,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:57,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:57,983 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:57,983 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:57,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:57,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:57,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:57,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:57,997 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:57,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:58,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:58,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:58,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,006 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,012 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:58,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,014 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:58,014 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,016 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:58,016 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,022 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:58,022 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,025 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:24:58,025 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:24:58,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,162 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,163 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,173 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,176 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,180 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,192 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,194 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:24:58,202 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:25:04,237 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41103
2023-06-26 18:25:04,237 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41103
2023-06-26 18:25:04,237 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46173
2023-06-26 18:25:04,237 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:04,237 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:04,237 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:04,237 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:04,237 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tncz4wic
2023-06-26 18:25:04,238 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ed3b64e-53d3-44c1-9ec9-ccf56524e04c
2023-06-26 18:25:04,363 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33579
2023-06-26 18:25:04,363 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33579
2023-06-26 18:25:04,363 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39961
2023-06-26 18:25:04,363 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:04,363 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:04,363 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:04,363 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:04,363 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sap716ns
2023-06-26 18:25:04,364 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08d848f1-5482-4f79-9141-332f715d2696
2023-06-26 18:25:04,406 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45131
2023-06-26 18:25:04,406 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45131
2023-06-26 18:25:04,406 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36813
2023-06-26 18:25:04,406 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:04,406 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:04,406 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:04,406 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:04,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_f3qzlyd
2023-06-26 18:25:04,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1cb2d59d-7873-42b7-be74-95b3ad996960
2023-06-26 18:25:04,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a467ee79-e669-42f7-bd28-e182c2f18540
2023-06-26 18:25:04,926 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37517
2023-06-26 18:25:04,927 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37517
2023-06-26 18:25:04,927 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36987
2023-06-26 18:25:04,927 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:04,927 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:04,927 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:04,927 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:04,927 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mwz2yej0
2023-06-26 18:25:04,927 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6da82ddd-ad24-4c66-afa7-340f6b13544f
2023-06-26 18:25:05,225 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35357
2023-06-26 18:25:05,225 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35357
2023-06-26 18:25:05,225 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43017
2023-06-26 18:25:05,225 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,225 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,225 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,225 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,225 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y_qwmul8
2023-06-26 18:25:05,226 - distributed.worker - INFO - Starting Worker plugin RMMSetup-357377f5-e6dd-4b90-b57d-7278a6bc4d78
2023-06-26 18:25:05,247 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39719
2023-06-26 18:25:05,247 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39719
2023-06-26 18:25:05,247 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38333
2023-06-26 18:25:05,247 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,247 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,248 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-my29735w
2023-06-26 18:25:05,248 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1075c2a5-1e93-480f-931f-ec56877cf444
2023-06-26 18:25:05,275 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46633
2023-06-26 18:25:05,276 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46633
2023-06-26 18:25:05,276 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37247
2023-06-26 18:25:05,276 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,276 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,276 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,276 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ih40cedy
2023-06-26 18:25:05,276 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f14586f7-c546-4824-959a-6ba97f1f848c
2023-06-26 18:25:05,281 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40523
2023-06-26 18:25:05,282 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40523
2023-06-26 18:25:05,282 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43375
2023-06-26 18:25:05,282 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,282 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,282 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33431
2023-06-26 18:25:05,282 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,282 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33431
2023-06-26 18:25:05,282 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,282 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8cvaf8pz
2023-06-26 18:25:05,282 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41073
2023-06-26 18:25:05,282 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,282 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,282 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,282 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,282 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8cpqtl8i
2023-06-26 18:25:05,282 - distributed.worker - INFO - Starting Worker plugin PreImport-574b5c70-2d3d-4022-aaeb-8be2bff48998
2023-06-26 18:25:05,282 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e155725-4329-4016-a84a-cf678e512373
2023-06-26 18:25:05,282 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43281c6e-3c33-4ab0-857d-0edd56d1f0fc
2023-06-26 18:25:05,285 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35323
2023-06-26 18:25:05,285 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35323
2023-06-26 18:25:05,286 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44219
2023-06-26 18:25:05,286 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,286 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,286 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,286 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-57kv23ek
2023-06-26 18:25:05,286 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40097fc5-adcd-47ec-abd8-93af79944b3e
2023-06-26 18:25:05,287 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57552b96-e10a-46d7-aaa8-b82ca05d06e8
2023-06-26 18:25:05,304 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44457
2023-06-26 18:25:05,304 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44457
2023-06-26 18:25:05,304 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42865
2023-06-26 18:25:05,304 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,304 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,304 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,304 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,304 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1gbva0k7
2023-06-26 18:25:05,305 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40159
2023-06-26 18:25:05,305 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40159
2023-06-26 18:25:05,305 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36147
2023-06-26 18:25:05,305 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,305 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,305 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f3ec0dad-d3e5-469e-9a91-79e1b3f1843b
2023-06-26 18:25:05,305 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,305 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,305 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c7kqjsu1
2023-06-26 18:25:05,305 - distributed.worker - INFO - Starting Worker plugin RMMSetup-208edd2b-b699-4ee7-b29f-b690ce4817e1
2023-06-26 18:25:05,309 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40533
2023-06-26 18:25:05,309 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40533
2023-06-26 18:25:05,309 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39173
2023-06-26 18:25:05,309 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,309 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,309 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,310 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kdjpjw4u
2023-06-26 18:25:05,311 - distributed.worker - INFO - Starting Worker plugin PreImport-d6124044-b7cc-460c-bdba-6361a84449e6
2023-06-26 18:25:05,311 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd712567-3865-469c-9470-6255c60eb379
2023-06-26 18:25:05,311 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44331
2023-06-26 18:25:05,311 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44331
2023-06-26 18:25:05,311 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42711
2023-06-26 18:25:05,311 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,311 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,311 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,311 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3t0g3rf0
2023-06-26 18:25:05,312 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60528d9c-d372-465e-92f1-7c0447e88ddc
2023-06-26 18:25:05,313 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35009
2023-06-26 18:25:05,313 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35009
2023-06-26 18:25:05,313 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42051
2023-06-26 18:25:05,313 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,313 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,313 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,313 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,313 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-km1eda2_
2023-06-26 18:25:05,314 - distributed.worker - INFO - Starting Worker plugin PreImport-d0c0bfd2-8a49-4648-9f5d-774b667c4aa1
2023-06-26 18:25:05,314 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d916f482-0128-4f8b-8a16-1ee62032ad53
2023-06-26 18:25:05,316 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42449
2023-06-26 18:25:05,316 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42449
2023-06-26 18:25:05,316 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33833
2023-06-26 18:25:05,316 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:25:05,317 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:05,317 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:25:05,317 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:25:05,317 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f2yhwh97
2023-06-26 18:25:05,317 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40115f85-cfc6-4018-b1cf-1c1c62f5a774
2023-06-26 18:25:05,317 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97ce5840-d2a4-4e0c-8947-7d458bd20ea8
2023-06-26 18:25:06,139 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85124fd7-0d16-4ed8-8a56-187c37552838
2023-06-26 18:25:06,140 - distributed.worker - INFO - Starting Worker plugin PreImport-0290f117-97e2-4245-8bde-06ff695b2f59
2023-06-26 18:25:06,141 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:06,162 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:06,162 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:06,163 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:07,564 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-659cfaab-3cab-49c9-808d-769f885b479b
2023-06-26 18:25:07,564 - distributed.worker - INFO - Starting Worker plugin PreImport-136d7ca8-fd74-44a9-9c0f-d64e161c9c03
2023-06-26 18:25:07,565 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:07,589 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:07,589 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:07,590 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:07,762 - distributed.worker - INFO - Starting Worker plugin PreImport-2ea95fac-6a5f-42dc-843d-d0ae61bfe074
2023-06-26 18:25:07,763 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:07,782 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:07,782 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:07,785 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83e51dcc-b6ea-4e23-acaa-145f424bc473
2023-06-26 18:25:08,622 - distributed.worker - INFO - Starting Worker plugin PreImport-0e70acc0-02fe-481d-9350-34f1f407d37e
2023-06-26 18:25:08,623 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,642 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,642 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,643 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,715 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb56bc93-3fbd-4a5d-b02c-a4d504409120
2023-06-26 18:25:08,715 - distributed.worker - INFO - Starting Worker plugin PreImport-a7a46521-50d1-4d63-838e-497c3c354ae0
2023-06-26 18:25:08,716 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,732 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,732 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,733 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3976591b-54a6-4d0e-8b94-2fce422d495a
2023-06-26 18:25:08,733 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,733 - distributed.worker - INFO - Starting Worker plugin PreImport-dea09edd-3772-4853-8c6a-f82193d349ee
2023-06-26 18:25:08,734 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,755 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,755 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,757 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,790 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-983ee1de-5aa7-4428-9783-8f09f46479d0
2023-06-26 18:25:08,791 - distributed.worker - INFO - Starting Worker plugin PreImport-81334dfa-2040-46f2-94ce-373f9c7609e5
2023-06-26 18:25:08,792 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,807 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebdd5ce0-9df6-4ade-a0b5-a0a0ad4319bf
2023-06-26 18:25:08,809 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,812 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,813 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,815 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,818 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-037a6689-25b3-463f-8c59-27ad2d0f0d7f
2023-06-26 18:25:08,819 - distributed.worker - INFO - Starting Worker plugin PreImport-ef374da8-e5bf-4fe8-9220-caea08454b5a
2023-06-26 18:25:08,821 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-995d681a-7d4b-4894-95e0-fb64cc41897d
2023-06-26 18:25:08,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,825 - distributed.worker - INFO - Starting Worker plugin PreImport-b69b5837-d14a-4967-801b-2dc6ff77dd17
2023-06-26 18:25:08,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,831 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,831 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,833 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,841 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,841 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,844 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,844 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,844 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,845 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,848 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b07c3e40-09f3-4806-9381-ab1e4aefa5fc
2023-06-26 18:25:08,849 - distributed.worker - INFO - Starting Worker plugin PreImport-7cdaf403-3c0b-42ea-b96b-a3170dfc840c
2023-06-26 18:25:08,849 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,851 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,851 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,853 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,854 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1381ef20-69cb-4955-98ee-49c5d77e969b
2023-06-26 18:25:08,854 - distributed.worker - INFO - Starting Worker plugin PreImport-ec6db1e4-a1ac-493d-8904-7ffad1b694a3
2023-06-26 18:25:08,855 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,860 - distributed.worker - INFO - Starting Worker plugin PreImport-d6ab52fa-de7d-442b-b531-391ff0fd26f5
2023-06-26 18:25:08,861 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,864 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,864 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,866 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,868 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,868 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,868 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b5c15fa-1062-4458-b4e8-20b220d72729
2023-06-26 18:25:08,869 - distributed.worker - INFO - Starting Worker plugin PreImport-743d81c5-d5af-4338-ab60-b6d8bd60dabc
2023-06-26 18:25:08,869 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,870 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,874 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,874 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,875 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b1e64857-f1aa-46ac-9603-8c186c567d6d
2023-06-26 18:25:08,887 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,889 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,889 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,892 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:08,900 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:25:08,900 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:25:08,902 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:25:17,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,832 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,833 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,833 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,833 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,833 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,834 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,835 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,836 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,836 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,837 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,837 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,837 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,837 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,839 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:17,850 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:25:18,534 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,534 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,534 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:18,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:25:21,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:33,351 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,383 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,498 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,568 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,572 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,623 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,623 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,748 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,808 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,818 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,834 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,868 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,872 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,964 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:33,985 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:34,306 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:25:40,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:25:40,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:19,836 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,836 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,837 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,839 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,839 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,841 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,841 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,842 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,842 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,842 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,842 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,842 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,842 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,843 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,847 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:19,847 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:26:23,081 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,087 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,088 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,088 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,088 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:23,088 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:30,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,842 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,843 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,843 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,843 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,845 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,857 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,858 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:26:59,871 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,871 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,871 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:26:59,872 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:27:03,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:03,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:04,311 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,314 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33431. Reason: scheduler-restart
2023-06-26 18:27:04,314 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,314 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,314 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,315 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33579. Reason: scheduler-restart
2023-06-26 18:27:04,315 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,315 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,315 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35009. Reason: scheduler-restart
2023-06-26 18:27:04,316 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,316 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35323. Reason: scheduler-restart
2023-06-26 18:27:04,316 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,316 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,317 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,317 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35357. Reason: scheduler-restart
2023-06-26 18:27:04,317 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37517. Reason: scheduler-restart
2023-06-26 18:27:04,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33431
2023-06-26 18:27:04,317 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,317 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39719. Reason: scheduler-restart
2023-06-26 18:27:04,317 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,317 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,318 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40159. Reason: scheduler-restart
2023-06-26 18:27:04,318 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,318 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,318 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40523. Reason: scheduler-restart
2023-06-26 18:27:04,318 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,318 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,318 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,319 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,319 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,320 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,320 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,320 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,321 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,321 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,321 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,321 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,322 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,322 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,323 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,323 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,324 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:27:04,327 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,329 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42449. Reason: scheduler-restart
2023-06-26 18:27:04,329 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41103. Reason: scheduler-restart
2023-06-26 18:27:04,329 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35009
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35323
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39719
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37517
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35357
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40159
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35009
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35323
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39719
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37517
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35357
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40159
2023-06-26 18:27:04,330 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 18:27:04,335 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42449
2023-06-26 18:27:04,336 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,337 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44457. Reason: scheduler-restart
2023-06-26 18:27:04,337 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45131. Reason: scheduler-restart
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35009
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35323
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39719
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37517
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35009
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35357
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35323
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40159
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39719
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 18:27:04,338 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44331. Reason: scheduler-restart
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37517
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35357
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40159
2023-06-26 18:27:04,338 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 18:27:04,339 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42449
2023-06-26 18:27:04,339 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46633. Reason: scheduler-restart
2023-06-26 18:27:04,339 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42449
2023-06-26 18:27:04,339 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41103
2023-06-26 18:27:04,339 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,339 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,339 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41103
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35009
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35323
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39719
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37517
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35357
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40159
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 18:27:04,340 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35009
2023-06-26 18:27:04,341 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35323
2023-06-26 18:27:04,341 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39719
2023-06-26 18:27:04,341 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37517
2023-06-26 18:27:04,341 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35357
2023-06-26 18:27:04,341 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42449
2023-06-26 18:27:04,341 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40159
2023-06-26 18:27:04,341 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 18:27:04,341 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41103
2023-06-26 18:27:04,341 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40533. Reason: scheduler-restart
2023-06-26 18:27:04,341 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,341 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42449
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35009
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35323
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39719
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37517
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35357
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40159
2023-06-26 18:27:04,342 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 18:27:04,343 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,343 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42449
2023-06-26 18:27:04,343 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,344 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,345 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,345 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41103
2023-06-26 18:27:04,345 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45131
2023-06-26 18:27:04,346 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44457
2023-06-26 18:27:04,346 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46633
2023-06-26 18:27:04,346 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41103
2023-06-26 18:27:04,346 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,347 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45131
2023-06-26 18:27:04,347 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44457
2023-06-26 18:27:04,347 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46633
2023-06-26 18:27:04,348 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:27:04,348 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,349 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,359 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:04,379 - distributed.nanny - INFO - Worker closed
2023-06-26 18:27:06,325 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:07,340 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:08,231 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:08,232 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:08,516 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:08,719 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:08,743 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:08,743 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:08,921 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:10,381 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,382 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,387 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,601 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:10,601 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:10,633 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,634 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,634 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:10,634 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:10,643 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:10,643 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:10,643 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:10,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:10,646 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,647 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,652 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,654 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,660 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:27:10,662 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:10,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:10,783 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:10,826 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:10,847 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:10,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:10,879 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,210 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,210 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,246 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37663
2023-06-26 18:27:12,247 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37663
2023-06-26 18:27:12,247 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35719
2023-06-26 18:27:12,247 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:12,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:12,247 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:12,247 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:12,247 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-21mp0oxi
2023-06-26 18:27:12,248 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73749e60-2264-4ce5-91b2-7763705eb326
2023-06-26 18:27:12,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,376 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,403 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,425 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,425 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,491 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,492 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:27:12,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:27:12,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,580 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,602 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,650 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,667 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,668 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:12,694 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:27:13,491 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38171
2023-06-26 18:27:13,491 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38171
2023-06-26 18:27:13,491 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39791
2023-06-26 18:27:13,491 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:13,491 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:13,491 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:13,491 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:13,491 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o1whew_9
2023-06-26 18:27:13,492 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78c13fbd-3748-44e1-a9d7-9aceb3af3246
2023-06-26 18:27:13,587 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33821
2023-06-26 18:27:13,588 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33821
2023-06-26 18:27:13,588 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44373
2023-06-26 18:27:13,588 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:13,588 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:13,588 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:13,588 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:13,588 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4vvkx31h
2023-06-26 18:27:13,588 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f2b212b-12fe-4d8d-9b60-d2b161a72213
2023-06-26 18:27:13,643 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38245
2023-06-26 18:27:13,643 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38245
2023-06-26 18:27:13,643 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34083
2023-06-26 18:27:13,643 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:13,643 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:13,643 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:13,643 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:13,643 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2a0y5i5y
2023-06-26 18:27:13,645 - distributed.worker - INFO - Starting Worker plugin PreImport-cf76c311-61e5-4421-a96e-4dfb242b6c77
2023-06-26 18:27:13,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28ae53b4-d308-4f5d-8209-71cde5d68194
2023-06-26 18:27:13,751 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44703
2023-06-26 18:27:13,751 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44703
2023-06-26 18:27:13,751 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38049
2023-06-26 18:27:13,751 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:13,751 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:13,751 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:13,751 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:13,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a9dnk7uw
2023-06-26 18:27:13,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39b316cb-662f-4072-881c-93bac0f75b60
2023-06-26 18:27:13,885 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41113
2023-06-26 18:27:13,886 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41113
2023-06-26 18:27:13,886 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42547
2023-06-26 18:27:13,886 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:13,886 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:13,886 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:13,886 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:13,886 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u3m0an4_
2023-06-26 18:27:13,887 - distributed.worker - INFO - Starting Worker plugin PreImport-39bf5a2a-d05e-41f4-b385-ce1c441e5c64
2023-06-26 18:27:13,887 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aedf4a13-c42f-4d54-b492-616d77005810
2023-06-26 18:27:15,072 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95fd5065-12b4-4d23-93db-1984e50289a8
2023-06-26 18:27:15,073 - distributed.worker - INFO - Starting Worker plugin PreImport-97b97f0b-688b-4a45-adeb-68d71b171622
2023-06-26 18:27:15,074 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:15,089 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:15,089 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:15,090 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:15,910 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d25c506-0be3-4a40-8a48-700374d2d57f
2023-06-26 18:27:15,910 - distributed.worker - INFO - Starting Worker plugin PreImport-cc7f1b84-2dbc-49c1-9ddb-a0f2d1066995
2023-06-26 18:27:15,911 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:15,922 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:15,922 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:15,924 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:16,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7714e489-9f74-4a3d-8fe7-140549bc8283
2023-06-26 18:27:16,010 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:16,037 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:16,037 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:16,040 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:16,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b7ef53db-39a7-481d-b6ed-35522123c76b
2023-06-26 18:27:16,041 - distributed.worker - INFO - Starting Worker plugin PreImport-39815397-03b5-40ba-9767-6847c337cf3d
2023-06-26 18:27:16,043 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:16,046 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ca32e3d-e518-4e06-8593-8f81d009a046
2023-06-26 18:27:16,046 - distributed.worker - INFO - Starting Worker plugin PreImport-e4d8915f-3e35-4e79-b2df-b417e24e3d58
2023-06-26 18:27:16,047 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:16,060 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:16,060 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:16,061 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:16,061 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:16,061 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:16,063 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:16,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9752272a-3a6c-4abd-bca2-66f79f861bae
2023-06-26 18:27:16,211 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:16,240 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:16,241 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:16,242 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:19,018 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44115
2023-06-26 18:27:19,018 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44115
2023-06-26 18:27:19,018 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33243
2023-06-26 18:27:19,018 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,018 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,018 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,018 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,018 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h2bb24jt
2023-06-26 18:27:19,019 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a4322d7-6f82-4d16-be84-b7f38e4b789f
2023-06-26 18:27:19,574 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39235
2023-06-26 18:27:19,575 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39235
2023-06-26 18:27:19,575 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34995
2023-06-26 18:27:19,575 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,575 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,575 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,575 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,575 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tozm6wub
2023-06-26 18:27:19,575 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e172a1fc-cdc4-4ed3-b428-c296d43011da
2023-06-26 18:27:19,587 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42273
2023-06-26 18:27:19,587 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42273
2023-06-26 18:27:19,587 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41879
2023-06-26 18:27:19,587 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,587 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,587 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,587 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yn7gfpu9
2023-06-26 18:27:19,588 - distributed.worker - INFO - Starting Worker plugin PreImport-7051e4e4-e896-4caf-8a99-aee905753db8
2023-06-26 18:27:19,588 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e39eab99-756f-47af-8509-90934d9f87cd
2023-06-26 18:27:19,594 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44897
2023-06-26 18:27:19,594 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44897
2023-06-26 18:27:19,594 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41029
2023-06-26 18:27:19,594 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,594 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,594 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,594 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,594 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3pk6dijv
2023-06-26 18:27:19,595 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0048f1dd-7423-4f7f-8bf6-20aa8d830793
2023-06-26 18:27:19,595 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4131b94e-be07-4718-aa5e-e18dcca00844
2023-06-26 18:27:19,601 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42889
2023-06-26 18:27:19,601 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42889
2023-06-26 18:27:19,602 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32905
2023-06-26 18:27:19,602 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,602 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,602 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,602 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j2v8ty_m
2023-06-26 18:27:19,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7d5a805-7804-462f-99e8-7120f1e05ae7
2023-06-26 18:27:19,602 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0ed2308-7ba6-4f69-ac7a-b2512955d881
2023-06-26 18:27:19,617 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33697
2023-06-26 18:27:19,617 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33697
2023-06-26 18:27:19,617 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37197
2023-06-26 18:27:19,617 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,617 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,617 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,617 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,617 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w60u5scy
2023-06-26 18:27:19,619 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1060c51f-a9ec-4458-8169-1045468f9dc8
2023-06-26 18:27:19,619 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40373
2023-06-26 18:27:19,619 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40373
2023-06-26 18:27:19,620 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43623
2023-06-26 18:27:19,620 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,620 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,620 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,620 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tidjinwd
2023-06-26 18:27:19,620 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee03ac15-dcd8-4a2d-97cf-5d7d6217c9ff
2023-06-26 18:27:19,623 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46689
2023-06-26 18:27:19,624 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46689
2023-06-26 18:27:19,624 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41941
2023-06-26 18:27:19,624 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,624 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,624 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,624 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,624 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sx1u4tt_
2023-06-26 18:27:19,624 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c523277-2f73-4986-b5b4-be9e15ec2e20
2023-06-26 18:27:19,654 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46691
2023-06-26 18:27:19,654 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46691
2023-06-26 18:27:19,654 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35301
2023-06-26 18:27:19,654 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,654 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,654 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rqtn0x_j
2023-06-26 18:27:19,655 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79ff6848-1fa3-4157-83d5-900aa72baba3
2023-06-26 18:27:19,656 - distributed.worker - INFO - Starting Worker plugin RMMSetup-18ab3d17-5909-4974-8f3c-410d5082280c
2023-06-26 18:27:19,657 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35991
2023-06-26 18:27:19,657 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35991
2023-06-26 18:27:19,658 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41581
2023-06-26 18:27:19,658 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:27:19,658 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:19,658 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:27:19,658 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:27:19,658 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zt8u4d0t
2023-06-26 18:27:19,659 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8069211-83f6-445c-b8df-da8ef672d728
2023-06-26 18:27:21,412 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-714ca877-6081-4b63-9b19-2fadb5bfc056
2023-06-26 18:27:21,412 - distributed.worker - INFO - Starting Worker plugin PreImport-c523dae0-6c91-4a45-89c4-159218b41c2a
2023-06-26 18:27:21,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:21,437 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:21,437 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:21,440 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:21,990 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c82bcf44-9fb8-4bd9-abd2-64f7a4423007
2023-06-26 18:27:21,990 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:21,994 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-775fbf25-e48a-4cb9-bfa8-de9e24f65777
2023-06-26 18:27:21,994 - distributed.worker - INFO - Starting Worker plugin PreImport-5fcebb96-72b2-4ee7-875a-b40fb52aba47
2023-06-26 18:27:21,996 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,005 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,005 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,006 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:22,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49110c91-82c3-444c-9eed-71a77192634a
2023-06-26 18:27:22,013 - distributed.worker - INFO - Starting Worker plugin PreImport-79570b8f-0eac-4124-b0d0-967ace31b288
2023-06-26 18:27:22,014 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,018 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,018 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,021 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:22,027 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,027 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,029 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:22,055 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08dc5c23-1a54-43bf-a55d-a80af60488be
2023-06-26 18:27:22,056 - distributed.worker - INFO - Starting Worker plugin PreImport-5e3a449a-1ef3-4eff-944c-34f26c54d5cb
2023-06-26 18:27:22,067 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,070 - distributed.worker - INFO - Starting Worker plugin PreImport-0f8fe24e-2700-424d-a00f-19f2c86894a7
2023-06-26 18:27:22,070 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,074 - distributed.worker - INFO - Starting Worker plugin PreImport-c9d1db94-0eb9-4a10-8a57-de85485a273c
2023-06-26 18:27:22,077 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,078 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e577177-25e2-47a7-89f5-0c57441dff60
2023-06-26 18:27:22,078 - distributed.worker - INFO - Starting Worker plugin PreImport-4380090e-ecb8-4169-9314-b672f1817c6a
2023-06-26 18:27:22,079 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d94dce89-fb1d-454a-904b-be08f7effd3b
2023-06-26 18:27:22,082 - distributed.worker - INFO - Starting Worker plugin PreImport-0efa53ec-f377-4149-bbab-566deaf3c5ae
2023-06-26 18:27:22,082 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,082 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,083 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,084 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:22,095 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,095 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,097 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:22,098 - distributed.worker - INFO - Starting Worker plugin PreImport-143a62b6-7ba6-4e06-980d-040bbc0955fa
2023-06-26 18:27:22,099 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,110 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,110 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,111 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,111 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,112 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,112 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,113 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:22,113 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:22,115 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:22,130 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:27:22,130 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:27:22,132 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:27:31,461 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:31,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:31,759 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:31,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:31,783 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:31,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:31,790 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:31,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:31,823 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:31,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:31,883 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:31,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:31,933 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:31,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:31,970 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:31,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,010 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:32,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,021 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:32,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,056 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:32,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,067 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:32,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,118 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:32,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,134 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:32,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,188 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:32,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,233 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:27:32,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:32,246 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,246 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,246 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,246 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,246 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,246 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,247 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,257 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,258 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,258 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,258 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,258 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,258 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,270 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,271 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,271 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,271 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:32,271 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:27:35,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:40,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:40,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:40,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:40,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:40,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:45,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:49,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:27:49,533 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,533 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:27:49,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:28:01,390 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,390 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,390 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,390 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:28:01,391 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46691. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38245. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44703. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33821. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44897. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41113. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42889. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44115. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35991. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38171. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33697. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46689. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42273. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39235. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40373. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:35:47,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37663. Reason: worker-close
2023-06-26 18:35:47,651 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34889'. Reason: nanny-close
2023-06-26 18:35:47,652 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,652 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43989'. Reason: nanny-close
2023-06-26 18:35:47,653 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,653 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37395'. Reason: nanny-close
2023-06-26 18:35:47,654 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,654 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42859'. Reason: nanny-close
2023-06-26 18:35:47,654 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,654 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35285'. Reason: nanny-close
2023-06-26 18:35:47,655 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,655 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40253'. Reason: nanny-close
2023-06-26 18:35:47,655 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,655 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39625'. Reason: nanny-close
2023-06-26 18:35:47,655 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,653 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:47828 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 740, in _run_callback
    from tornado import gen
  File "<frozen importlib._bootstrap>", line 1053, in _handle_fromlist
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:35:47,656 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37931'. Reason: nanny-close
2023-06-26 18:35:47,656 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,656 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44225'. Reason: nanny-close
2023-06-26 18:35:47,656 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,657 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33125'. Reason: nanny-close
2023-06-26 18:35:47,657 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,657 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45701'. Reason: nanny-close
2023-06-26 18:35:47,657 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,658 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34397'. Reason: nanny-close
2023-06-26 18:35:47,658 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,658 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41427'. Reason: nanny-close
2023-06-26 18:35:47,658 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35547'. Reason: nanny-close
2023-06-26 18:35:47,659 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:32881'. Reason: nanny-close
2023-06-26 18:35:47,659 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35555'. Reason: nanny-close
2023-06-26 18:35:47,660 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:35:47,670 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43989 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56002 remote=tcp://10.120.104.11:43989>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43989 after 100 s
2023-06-26 18:35:47,673 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37395 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34088 remote=tcp://10.120.104.11:37395>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37395 after 100 s
2023-06-26 18:35:47,674 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34397 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37796 remote=tcp://10.120.104.11:34397>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34397 after 100 s
2023-06-26 18:35:47,675 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39625 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55390 remote=tcp://10.120.104.11:39625>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39625 after 100 s
2023-06-26 18:35:47,677 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40253 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42408 remote=tcp://10.120.104.11:40253>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40253 after 100 s
2023-06-26 18:35:47,677 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33125 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59584 remote=tcp://10.120.104.11:33125>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33125 after 100 s
2023-06-26 18:35:47,677 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45701 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35074 remote=tcp://10.120.104.11:45701>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45701 after 100 s
2023-06-26 18:35:47,678 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41427 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56200 remote=tcp://10.120.104.11:41427>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41427 after 100 s
2023-06-26 18:35:47,678 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35555 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35596 remote=tcp://10.120.104.11:35555>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35555 after 100 s
2023-06-26 18:35:47,679 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:32881 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46026 remote=tcp://10.120.104.11:32881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:32881 after 100 s
2023-06-26 18:35:47,680 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37931 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33944 remote=tcp://10.120.104.11:37931>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37931 after 100 s
2023-06-26 18:35:47,681 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35547 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48706 remote=tcp://10.120.104.11:35547>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35547 after 100 s
2023-06-26 18:35:47,685 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42859 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41476 remote=tcp://10.120.104.11:42859>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42859 after 100 s
2023-06-26 18:35:47,687 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44225 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:43514 remote=tcp://10.120.104.11:44225>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44225 after 100 s
2023-06-26 18:35:50,860 - distributed.nanny - WARNING - Worker process still alive after 3.1999966430664064 seconds, killing
2023-06-26 18:35:50,861 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:35:50,861 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:35:50,862 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:35:50,863 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:35:50,863 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:35:50,864 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:35:50,864 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:35:50,864 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:35:50,865 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:35:50,865 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:35:50,866 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:35:50,866 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:35:50,867 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:35:50,867 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:35:50,868 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:35:51,653 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,655 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,655 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,655 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,656 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,656 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,656 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,657 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,657 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,658 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,658 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,659 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,659 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,659 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,660 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,660 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371775 parent=368424 started daemon>
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371771 parent=368424 started daemon>
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371768 parent=368424 started daemon>
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371765 parent=368424 started daemon>
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371762 parent=368424 started daemon>
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371755 parent=368424 started daemon>
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371751 parent=368424 started daemon>
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371738 parent=368424 started daemon>
2023-06-26 18:35:51,662 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371735 parent=368424 started daemon>
2023-06-26 18:35:51,663 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371732 parent=368424 started daemon>
2023-06-26 18:35:51,663 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371707 parent=368424 started daemon>
2023-06-26 18:35:51,663 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371702 parent=368424 started daemon>
2023-06-26 18:35:51,663 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371692 parent=368424 started daemon>
2023-06-26 18:35:51,663 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371689 parent=368424 started daemon>
2023-06-26 18:35:51,663 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371671 parent=368424 started daemon>
2023-06-26 18:35:51,663 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=371660 parent=368424 started daemon>
2023-06-26 18:35:56,113 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 371732 exit status was already read will report exitcode 255
2023-06-26 18:35:57,872 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 371689 exit status was already read will report exitcode 255
2023-06-26 18:35:58,625 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 371738 exit status was already read will report exitcode 255
