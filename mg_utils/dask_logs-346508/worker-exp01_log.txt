RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 18:04:06,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43375'
2023-06-26 18:04:06,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38935'
2023-06-26 18:04:06,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36139'
2023-06-26 18:04:06,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33625'
2023-06-26 18:04:06,599 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43629'
2023-06-26 18:04:06,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44815'
2023-06-26 18:04:06,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33235'
2023-06-26 18:04:06,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43577'
2023-06-26 18:04:06,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35839'
2023-06-26 18:04:06,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39017'
2023-06-26 18:04:06,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44411'
2023-06-26 18:04:06,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40561'
2023-06-26 18:04:06,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34803'
2023-06-26 18:04:06,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34791'
2023-06-26 18:04:06,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43641'
2023-06-26 18:04:06,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43147'
2023-06-26 18:04:08,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,297 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,297 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,328 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,355 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,356 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,367 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,476 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,477 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,477 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,487 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,500 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,508 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,543 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,546 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:08,598 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:04:08,598 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:04:08,805 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:04:13,818 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45783
2023-06-26 18:04:13,818 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45783
2023-06-26 18:04:13,818 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38981
2023-06-26 18:04:13,818 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:13,818 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:13,818 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:13,819 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:13,819 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-718ui_81
2023-06-26 18:04:13,819 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4066e4de-4d7c-4624-a662-988b7283299d
2023-06-26 18:04:13,819 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ca61a25-8ea5-428f-bd9e-5f6e2e9c12b9
2023-06-26 18:04:14,113 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36831
2023-06-26 18:04:14,113 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36831
2023-06-26 18:04:14,113 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32977
2023-06-26 18:04:14,113 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,113 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,114 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,114 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,114 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0mkjypoc
2023-06-26 18:04:14,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-69792992-ced1-4c4a-af4c-8b60bd0685f7
2023-06-26 18:04:14,141 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33067
2023-06-26 18:04:14,141 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33067
2023-06-26 18:04:14,141 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37983
2023-06-26 18:04:14,141 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,142 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,142 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,142 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,142 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uq2vvw9s
2023-06-26 18:04:14,142 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4eac157b-4773-4823-8e02-387b0dde9932
2023-06-26 18:04:14,368 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41085
2023-06-26 18:04:14,368 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41085
2023-06-26 18:04:14,368 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46819
2023-06-26 18:04:14,368 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,368 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,368 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,368 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,368 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9oon__c5
2023-06-26 18:04:14,369 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a808f59-9b79-41d6-a022-22e2e8282170
2023-06-26 18:04:14,380 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42187
2023-06-26 18:04:14,380 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42187
2023-06-26 18:04:14,381 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45181
2023-06-26 18:04:14,381 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,381 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,381 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,381 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i1b144qj
2023-06-26 18:04:14,381 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2f938ff-c0f9-4e63-a01d-911aa437760a
2023-06-26 18:04:14,387 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46707
2023-06-26 18:04:14,388 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46707
2023-06-26 18:04:14,388 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39915
2023-06-26 18:04:14,388 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,388 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,388 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,388 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,388 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g1iwj_9e
2023-06-26 18:04:14,388 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfd5ef2e-b6ae-4f21-bac8-84a8f02749d9
2023-06-26 18:04:14,390 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39327
2023-06-26 18:04:14,390 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39327
2023-06-26 18:04:14,390 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45607
2023-06-26 18:04:14,390 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,390 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,390 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,390 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,390 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k7vpoa56
2023-06-26 18:04:14,391 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdf735e9-5404-47c8-bade-a78550abd728
2023-06-26 18:04:14,391 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0dffe354-aac5-4b1f-8874-dc8e9dc30203
2023-06-26 18:04:14,416 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36059
2023-06-26 18:04:14,417 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36059
2023-06-26 18:04:14,417 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41285
2023-06-26 18:04:14,417 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,417 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,417 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35801
2023-06-26 18:04:14,417 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,417 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35801
2023-06-26 18:04:14,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-llbkbfmk
2023-06-26 18:04:14,417 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37281
2023-06-26 18:04:14,417 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,417 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,417 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7y5xuy04
2023-06-26 18:04:14,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc628e86-3e42-4830-b97f-1d7ff665329f
2023-06-26 18:04:14,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62d75f7f-ea21-4c13-a328-86f27f484944
2023-06-26 18:04:14,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c1f0116-9387-40a7-a2c3-aa8f7afb9f43
2023-06-26 18:04:14,447 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36459
2023-06-26 18:04:14,447 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36459
2023-06-26 18:04:14,447 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37479
2023-06-26 18:04:14,447 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,447 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,448 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,448 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,448 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-goasbibe
2023-06-26 18:04:14,448 - distributed.worker - INFO - Starting Worker plugin PreImport-01c0fdfc-579c-4a66-8880-699ecd2d4c99
2023-06-26 18:04:14,448 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12bd1f3f-39a1-49c6-ab2f-4206ca25053d
2023-06-26 18:04:14,580 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35363
2023-06-26 18:04:14,580 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35363
2023-06-26 18:04:14,580 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44811
2023-06-26 18:04:14,580 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,580 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,580 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,580 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,580 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pijnb9e_
2023-06-26 18:04:14,581 - distributed.worker - INFO - Starting Worker plugin RMMSetup-acf6b96a-2f7d-4c91-b13d-77ab3111abc6
2023-06-26 18:04:14,589 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32813
2023-06-26 18:04:14,589 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32813
2023-06-26 18:04:14,589 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44173
2023-06-26 18:04:14,589 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,589 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,590 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,590 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m87smwnj
2023-06-26 18:04:14,590 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f833886e-386b-4cb7-b5ba-72b603dbe2e7
2023-06-26 18:04:14,592 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38411
2023-06-26 18:04:14,593 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38411
2023-06-26 18:04:14,592 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41687
2023-06-26 18:04:14,593 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45687
2023-06-26 18:04:14,593 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41687
2023-06-26 18:04:14,593 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,593 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,593 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43121
2023-06-26 18:04:14,593 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,593 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,593 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,593 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h0l8e8fo
2023-06-26 18:04:14,593 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,593 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mfec04ec
2023-06-26 18:04:14,593 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37dc5d02-7fa9-4f25-aa0c-f8faf4dc7a9f
2023-06-26 18:04:14,594 - distributed.worker - INFO - Starting Worker plugin PreImport-5c1e30c8-d7a7-4915-8508-365d1dba283f
2023-06-26 18:04:14,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf3bd4d0-90a0-478b-9643-a28d7dc3dd32
2023-06-26 18:04:14,603 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43225
2023-06-26 18:04:14,604 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43225
2023-06-26 18:04:14,604 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40373
2023-06-26 18:04:14,604 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,604 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,604 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,604 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41971
2023-06-26 18:04:14,604 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,604 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41971
2023-06-26 18:04:14,604 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7pb29rnu
2023-06-26 18:04:14,604 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40429
2023-06-26 18:04:14,604 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:04:14,604 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:14,604 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:04:14,604 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:04:14,604 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g5kaab_y
2023-06-26 18:04:14,605 - distributed.worker - INFO - Starting Worker plugin PreImport-ccaeb575-528a-42dd-ac29-593fe6b25750
2023-06-26 18:04:14,605 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57dee42d-42d9-4d01-9ccb-f85f390b89a4
2023-06-26 18:04:14,605 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51ee4bba-ab46-4379-96fa-9d709d058392
2023-06-26 18:04:17,919 - distributed.worker - INFO - Starting Worker plugin PreImport-032e3c4e-2138-42d5-9c06-ae614db2de33
2023-06-26 18:04:17,920 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:17,940 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:17,940 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:17,945 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:18,037 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6937222-4a61-40a3-b3d1-d4541209f0b4
2023-06-26 18:04:18,038 - distributed.worker - INFO - Starting Worker plugin PreImport-9365da71-9f8d-497f-bec7-9f6647433eb4
2023-06-26 18:04:18,039 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,058 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:18,058 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,059 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:18,319 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebe0887b-2f01-42ba-b155-7cfe108d03ae
2023-06-26 18:04:18,320 - distributed.worker - INFO - Starting Worker plugin PreImport-6e24c2b2-0590-490a-bf91-84971ee1b515
2023-06-26 18:04:18,321 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,356 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:18,356 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,365 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:18,537 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:04:18,538 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:04:18,540 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:04:18,544 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:04:18,544 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:04:18,544 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:04:18,555 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-965b257b-1e7b-4a28-b6e1-4f9102e2bc45
2023-06-26 18:04:18,556 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,563 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f6cbbd2-fb23-43b6-852f-824b68bf5a77
2023-06-26 18:04:18,564 - distributed.worker - INFO - Starting Worker plugin PreImport-fe4c6e60-800c-43c2-b9d6-095887622d16
2023-06-26 18:04:18,565 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,648 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5704310-301a-48c7-a536-867deb775654
2023-06-26 18:04:18,649 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,675 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-269addfe-5847-449f-99da-d2190269baad
2023-06-26 18:04:18,681 - distributed.worker - INFO - Starting Worker plugin PreImport-0d9b99b5-593a-450c-a4c4-529f6f2a09fe
2023-06-26 18:04:18,685 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,688 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbcebc9d-029a-4eb6-8a17-878a6707fae5
2023-06-26 18:04:18,688 - distributed.worker - INFO - Starting Worker plugin PreImport-ed76fb34-9a4f-4bd5-91a6-e4fbb55fe15b
2023-06-26 18:04:18,689 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff728c79-ec6c-4946-9461-acda9b71c62c
2023-06-26 18:04:18,690 - distributed.worker - INFO - Starting Worker plugin PreImport-2d6c45ec-0495-4a2f-8220-868430165685
2023-06-26 18:04:18,691 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,698 - distributed.worker - INFO - Starting Worker plugin PreImport-473b647f-765c-40dc-be87-4d41da43adb4
2023-06-26 18:04:18,699 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,716 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fde17504-a37f-4a0e-85a8-2def78554f40
2023-06-26 18:04:18,717 - distributed.worker - INFO - Starting Worker plugin PreImport-0c3789d3-c401-43d6-b59d-22779a2c07e5
2023-06-26 18:04:18,718 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,724 - distributed.worker - INFO - Starting Worker plugin PreImport-e4c9ec8f-1fa4-4dc5-84f9-09279ba7e15d
2023-06-26 18:04:18,726 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,731 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-167be299-af4e-4593-913a-8742f8969944
2023-06-26 18:04:18,732 - distributed.worker - INFO - Starting Worker plugin PreImport-c50cffff-0270-4a84-876b-d17d1bd5ccab
2023-06-26 18:04:18,734 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-abc50365-a8bc-4b11-b7cf-e7a65f066bf1
2023-06-26 18:04:18,736 - distributed.worker - INFO - Starting Worker plugin PreImport-e83da37f-bbce-4ccc-953b-0a3dd1b20641
2023-06-26 18:04:18,737 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7564693c-bd80-408a-bab8-8c8dcab7b320
2023-06-26 18:04:18,754 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:18,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9ebf6bd-d5c7-46df-925b-6b53dc0a61b5
2023-06-26 18:04:18,755 - distributed.worker - INFO - Starting Worker plugin PreImport-1e1736ee-94f8-41a4-b152-dd4684737add
2023-06-26 18:04:18,757 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,213 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:04:19,213 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:04:19,213 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:04:19,220 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,221 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,221 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,221 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,222 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,222 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,223 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,223 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,224 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,224 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,224 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,224 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,225 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,225 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,225 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,226 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,226 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,226 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,227 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,227 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,227 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,227 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,228 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,229 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,229 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,230 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,232 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,232 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,232 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,233 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,233 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,234 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,234 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,237 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,239 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,239 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,239 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:04:19,239 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:04:19,242 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:04:19,502 - distributed.worker - WARNING - Run Failed
Function: _func_init_all
args:     (b'\xd4\x8f$\xf4s\x88M\x06\x98T\xcc`\xebd\x91\xee', b'X\xa9\xe1\xf0\xc3\xec`\xdc\x02\x00\xd8s\n!\xe4F\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00`dO|\xfd~\x00\x00p\xa8>\x0c\xfe~\x00\x00\xb0iO|\xfd~\x00\x00\xb8iO|\xfd~\x00\x00 \xb6M\xe1\xcfU\x00\x00\xb0\x94\xb5\x8e\xfd~\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xad^\xb8\xdf\xcfU\x00\x00\xf0\x12)\x8e\xfd~\x00\x00\x00\xd0\x17\x9a\x87\xea\xf4', True, {'tcp://10.120.104.11:35801': {'rank': 7, 'port': 35472}, 'tcp://10.120.104.11:45783': {'rank': 1, 'port': 42977}, 'tcp://10.120.104.11:46707': {'rank': 9, 'port': 34978}}, False, 0)
kwargs:   {'dask_worker': <Worker 'tcp://10.120.104.11:46707', status: running, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>}
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 3281, in run
    result = await function(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/raft_dask/common/comms.py", line 446, in _func_init_all
    _func_init_nccl(sessionId, uniqueId, dask_worker=dask_worker)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/raft_dask/common/comms.py", line 511, in _func_init_nccl
    n.init(nWorkers, uniqueId, wid)
  File "nccl.pyx", line 151, in raft_dask.common.nccl.nccl.init
RuntimeError: NCCL_ERROR: b'invalid argument (run with NCCL_DEBUG=WARN for details)'
2023-06-26 18:04:19,506 - distributed.worker - WARNING - Run Failed
Function: _func_init_all
args:     (b'\xd4\x8f$\xf4s\x88M\x06\x98T\xcc`\xebd\x91\xee', b'X\xa9\xe1\xf0\xc3\xec`\xdc\x02\x00\xd8s\n!\xe4F\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00`dO|\xfd~\x00\x00p\xa8>\x0c\xfe~\x00\x00\xb0iO|\xfd~\x00\x00\xb8iO|\xfd~\x00\x00 \xb6M\xe1\xcfU\x00\x00\xb0\x94\xb5\x8e\xfd~\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xad^\xb8\xdf\xcfU\x00\x00\xf0\x12)\x8e\xfd~\x00\x00\x00\xd0\x17\x9a\x87\xea\xf4', True, {'tcp://10.120.104.11:35801': {'rank': 7, 'port': 35472}, 'tcp://10.120.104.11:45783': {'rank': 1, 'port': 42977}, 'tcp://10.120.104.11:46707': {'rank': 9, 'port': 34978}}, False, 0)
kwargs:   {'dask_worker': <Worker 'tcp://10.120.104.11:35801', status: running, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>}
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 3281, in run
    result = await function(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/raft_dask/common/comms.py", line 446, in _func_init_all
    _func_init_nccl(sessionId, uniqueId, dask_worker=dask_worker)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/raft_dask/common/comms.py", line 511, in _func_init_nccl
    n.init(nWorkers, uniqueId, wid)
  File "nccl.pyx", line 151, in raft_dask.common.nccl.nccl.init
RuntimeError: NCCL_ERROR: b'invalid argument (run with NCCL_DEBUG=WARN for details)'
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39327. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36459. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33067. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32813. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35801. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35363. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36059. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36831. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46707. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43225. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41085. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42187. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41687. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41971. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,502 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38411. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:10:36,503 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43375'. Reason: nanny-close
2023-06-26 18:10:36,504 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,505 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38935'. Reason: nanny-close
2023-06-26 18:10:36,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33625'. Reason: nanny-close
2023-06-26 18:10:36,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43629'. Reason: nanny-close
2023-06-26 18:10:36,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44815'. Reason: nanny-close
2023-06-26 18:10:36,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,508 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33235'. Reason: nanny-close
2023-06-26 18:10:36,508 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,508 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43577'. Reason: nanny-close
2023-06-26 18:10:36,508 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,509 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35839'. Reason: nanny-close
2023-06-26 18:10:36,509 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,509 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39017'. Reason: nanny-close
2023-06-26 18:10:36,509 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,510 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36139'. Reason: nanny-close
2023-06-26 18:10:36,510 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,510 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40561'. Reason: nanny-close
2023-06-26 18:10:36,510 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,511 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34803'. Reason: nanny-close
2023-06-26 18:10:36,511 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,511 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34791'. Reason: nanny-close
2023-06-26 18:10:36,511 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43641'. Reason: nanny-close
2023-06-26 18:10:36,512 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44411'. Reason: nanny-close
2023-06-26 18:10:36,512 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,513 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43147'. Reason: nanny-close
2023-06-26 18:10:36,513 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:10:36,525 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35839 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41958 remote=tcp://10.120.104.11:35839>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35839 after 100 s
2023-06-26 18:10:36,526 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36139 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39150 remote=tcp://10.120.104.11:36139>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36139 after 100 s
2023-06-26 18:10:36,846 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43375 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40776 remote=tcp://10.120.104.11:43375>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43375 after 100 s
2023-06-26 18:10:36,852 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34803 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39836 remote=tcp://10.120.104.11:34803>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34803 after 100 s
2023-06-26 18:10:36,852 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33625 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45076 remote=tcp://10.120.104.11:33625>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33625 after 100 s
2023-06-26 18:10:36,853 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39017 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55546 remote=tcp://10.120.104.11:39017>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39017 after 100 s
2023-06-26 18:10:36,854 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43577 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35976 remote=tcp://10.120.104.11:43577>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43577 after 100 s
2023-06-26 18:10:36,855 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33235 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37676 remote=tcp://10.120.104.11:33235>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33235 after 100 s
2023-06-26 18:10:36,856 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44815 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41224 remote=tcp://10.120.104.11:44815>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44815 after 100 s
2023-06-26 18:10:36,860 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34791 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60578 remote=tcp://10.120.104.11:34791>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34791 after 100 s
2023-06-26 18:10:36,860 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43629 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47796 remote=tcp://10.120.104.11:43629>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43629 after 100 s
2023-06-26 18:10:36,861 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44411 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36936 remote=tcp://10.120.104.11:44411>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44411 after 100 s
2023-06-26 18:10:36,865 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40561 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37388 remote=tcp://10.120.104.11:40561>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40561 after 100 s
2023-06-26 18:10:36,870 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43641 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48194 remote=tcp://10.120.104.11:43641>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43641 after 100 s
2023-06-26 18:10:36,879 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43147 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:58054 remote=tcp://10.120.104.11:43147>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43147 after 100 s
2023-06-26 18:10:39,715 - distributed.nanny - WARNING - Worker process still alive after 3.199989624023438 seconds, killing
2023-06-26 18:10:39,715 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:10:39,715 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:10:39,716 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:10:39,716 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:10:39,716 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:10:39,716 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:10:39,716 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:10:39,717 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:10:39,717 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:10:39,717 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:10:39,717 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:10:39,717 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:10:39,718 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:10:39,718 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:10:39,718 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:10:40,481 - distributed.nanny - INFO - Worker process 346705 was killed by signal 9
2023-06-26 18:10:40,507 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,508 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,508 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,509 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,509 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,509 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,509 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,510 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,510 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,511 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,512 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,512 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,512 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,512 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,514 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:10:40,515 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346756 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346753 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346751 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346747 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346745 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346741 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346738 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346735 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346733 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346729 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346726 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346722 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346717 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346713 parent=346662 started daemon>
2023-06-26 18:10:40,516 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=346709 parent=346662 started daemon>
2023-06-26 18:10:41,602 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 346722 exit status was already read will report exitcode 255
2023-06-26 18:10:43,846 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 346713 exit status was already read will report exitcode 255
2023-06-26 18:10:44,635 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 346735 exit status was already read will report exitcode 255
