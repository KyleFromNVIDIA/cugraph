RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 20:19:50,363 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 20:19:50,822 - distributed.scheduler - INFO - State start
2023-06-22 20:19:50,823 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8w3ze61b', purging
2023-06-22 20:19:50,823 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ir8io8v1', purging
2023-06-22 20:19:50,823 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-s0_xhlzi', purging
2023-06-22 20:19:50,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xdj6h1__', purging
2023-06-22 20:19:50,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-hz6k7s75', purging
2023-06-22 20:19:50,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-laiq10w5', purging
2023-06-22 20:19:50,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8nc73ktx', purging
2023-06-22 20:19:50,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ehbonv_r', purging
2023-06-22 20:19:50,833 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 20:19:50,834 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 20:19:50,834 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 20:20:01,141 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:38683', status: init, memory: 0, processing: 0>
2023-06-22 20:20:01,144 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:38683
2023-06-22 20:20:01,144 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53036
2023-06-22 20:20:02,022 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:36141', status: init, memory: 0, processing: 0>
2023-06-22 20:20:02,022 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:36141
2023-06-22 20:20:02,022 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53048
2023-06-22 20:20:02,023 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:40231', status: init, memory: 0, processing: 0>
2023-06-22 20:20:02,023 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:40231
2023-06-22 20:20:02,023 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53062
2023-06-22 20:20:02,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:40197', status: init, memory: 0, processing: 0>
2023-06-22 20:20:02,024 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:40197
2023-06-22 20:20:02,024 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53068
2023-06-22 20:20:02,026 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:38673', status: init, memory: 0, processing: 0>
2023-06-22 20:20:02,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:38673
2023-06-22 20:20:02,026 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53084
2023-06-22 20:20:02,027 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:37503', status: init, memory: 0, processing: 0>
2023-06-22 20:20:02,027 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:37503
2023-06-22 20:20:02,028 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53082
2023-06-22 20:20:02,056 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:40437', status: init, memory: 0, processing: 0>
2023-06-22 20:20:02,056 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:40437
2023-06-22 20:20:02,056 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53098
2023-06-22 20:20:02,059 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:45449', status: init, memory: 0, processing: 0>
2023-06-22 20:20:02,060 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:45449
2023-06-22 20:20:02,060 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53102
2023-06-22 20:20:02,101 - distributed.scheduler - INFO - Receive client connection: Client-2aebdfde-113a-11ee-b6c1-d8c49778ced7
2023-06-22 20:20:02,101 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:53114
2023-06-22 20:20:02,182 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 20:20:52,893 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:20:55,101 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 20:20:58,912 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:22:02,669 - distributed.scheduler - INFO - Remove client Client-2aebdfde-113a-11ee-b6c1-d8c49778ced7
2023-06-22 20:22:02,673 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:53114; closing.
2023-06-22 20:22:02,673 - distributed.scheduler - INFO - Remove client Client-2aebdfde-113a-11ee-b6c1-d8c49778ced7
2023-06-22 20:22:02,674 - distributed.scheduler - INFO - Close client connection: Client-2aebdfde-113a-11ee-b6c1-d8c49778ced7
2023-06-22 20:25:20,513 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 20:25:20,514 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 20:25:20,514 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 20:25:20,516 - distributed.core - INFO - Connection to tcp://10.33.227.169:53048 has been closed.
2023-06-22 20:25:20,517 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:36141', status: running, memory: 0, processing: 0>
2023-06-22 20:25:20,518 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:36141
2023-06-22 20:25:20,518 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:53048>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:53048>: Stream is closed
2023-06-22 20:25:20,521 - distributed.core - INFO - Connection to tcp://10.33.227.169:53036 has been closed.
2023-06-22 20:25:20,522 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:38683', status: running, memory: 0, processing: 0>
2023-06-22 20:25:20,522 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:38683
2023-06-22 20:25:20,522 - distributed.core - INFO - Connection to tcp://10.33.227.169:53062 has been closed.
2023-06-22 20:25:20,522 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:40231', status: running, memory: 0, processing: 0>
2023-06-22 20:25:20,522 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:40231
2023-06-22 20:25:20,522 - distributed.core - INFO - Connection to tcp://10.33.227.169:53068 has been closed.
2023-06-22 20:25:20,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:40197', status: running, memory: 0, processing: 0>
2023-06-22 20:25:20,523 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:40197
2023-06-22 20:25:20,523 - distributed.core - INFO - Connection to tcp://10.33.227.169:53084 has been closed.
2023-06-22 20:25:20,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:38673', status: running, memory: 0, processing: 0>
2023-06-22 20:25:20,523 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:38673
2023-06-22 20:25:20,523 - distributed.core - INFO - Connection to tcp://10.33.227.169:53082 has been closed.
2023-06-22 20:25:20,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:37503', status: running, memory: 0, processing: 0>
2023-06-22 20:25:20,524 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:37503
2023-06-22 20:25:20,524 - distributed.core - INFO - Connection to tcp://10.33.227.169:53098 has been closed.
2023-06-22 20:25:20,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:40437', status: running, memory: 0, processing: 0>
2023-06-22 20:25:20,524 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:40437
2023-06-22 20:25:20,524 - distributed.core - INFO - Connection to tcp://10.33.227.169:53102 has been closed.
2023-06-22 20:25:20,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:45449', status: running, memory: 0, processing: 0>
2023-06-22 20:25:20,524 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:45449
2023-06-22 20:25:20,525 - distributed.scheduler - INFO - Lost all workers
2023-06-22 20:25:20,527 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 20:25:20,528 - distributed.scheduler - INFO - End scheduler
