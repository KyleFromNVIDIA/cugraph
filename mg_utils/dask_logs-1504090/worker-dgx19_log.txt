RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=12G
             --local-directory=/tmp/
             --scheduler-file=/root/work/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-22 23:12:42,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:36493'
2023-06-22 23:12:42,096 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:42617'
2023-06-22 23:12:42,098 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:34721'
2023-06-22 23:12:42,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:38311'
2023-06-22 23:12:42,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:39191'
2023-06-22 23:12:42,107 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:36541'
2023-06-22 23:12:42,108 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:33077'
2023-06-22 23:12:42,110 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:37051'
2023-06-22 23:12:43,561 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 23:12:43,561 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 23:12:43,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 23:12:43,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 23:12:43,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 23:12:43,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 23:12:43,674 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 23:12:43,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 23:12:43,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 23:12:43,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 23:12:43,676 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 23:12:43,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 23:12:43,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 23:12:43,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 23:12:43,677 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 23:12:43,677 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 23:12:43,989 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 23:12:44,017 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 23:12:44,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 23:12:44,116 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 23:12:44,133 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 23:12:44,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 23:12:44,136 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 23:12:44,147 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 23:12:45,625 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:42393
2023-06-22 23:12:45,626 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:42393
2023-06-22 23:12:45,626 - distributed.worker - INFO -          dashboard at:        10.33.227.169:39171
2023-06-22 23:12:45,626 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 23:12:45,626 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:45,626 - distributed.worker - INFO -               Threads:                          1
2023-06-22 23:12:45,626 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 23:12:45,626 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m0ygxag3
2023-06-22 23:12:45,627 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e94ec805-03c9-46e7-8762-3fa3ad5e1d8d
2023-06-22 23:12:45,826 - distributed.worker - INFO - Starting Worker plugin PreImport-0465bdaa-595e-4c5b-9fcd-23f394474287
2023-06-22 23:12:45,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f55c9a0d-57f4-4866-a4d6-d7ab8aa1ff0e
2023-06-22 23:12:45,828 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,131 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,131 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,133 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 23:12:46,190 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:41585
2023-06-22 23:12:46,191 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:41585
2023-06-22 23:12:46,191 - distributed.worker - INFO -          dashboard at:        10.33.227.169:32883
2023-06-22 23:12:46,191 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,191 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,191 - distributed.worker - INFO -               Threads:                          1
2023-06-22 23:12:46,191 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 23:12:46,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8l07hmwp
2023-06-22 23:12:46,192 - distributed.worker - INFO - Starting Worker plugin PreImport-1505159f-e3d9-4614-b962-c10126fa7326
2023-06-22 23:12:46,192 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07ef8182-7c2d-43ad-8766-4afafadb59f5
2023-06-22 23:12:46,193 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0316e060-62e4-4ac8-8a2a-5ac1b8452e98
2023-06-22 23:12:46,320 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,331 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,332 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,333 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 23:12:46,502 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:34115
2023-06-22 23:12:46,502 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:34115
2023-06-22 23:12:46,502 - distributed.worker - INFO -          dashboard at:        10.33.227.169:45299
2023-06-22 23:12:46,502 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,502 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,502 - distributed.worker - INFO -               Threads:                          1
2023-06-22 23:12:46,502 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 23:12:46,502 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e2vhkpts
2023-06-22 23:12:46,503 - distributed.worker - INFO - Starting Worker plugin PreImport-d4ae27e1-8214-4258-bc80-bb27977b46cd
2023-06-22 23:12:46,503 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-347ac123-a84e-4c86-b3d7-7f9800e71c4f
2023-06-22 23:12:46,503 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:44595
2023-06-22 23:12:46,503 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:44595
2023-06-22 23:12:46,503 - distributed.worker - INFO -          dashboard at:        10.33.227.169:45297
2023-06-22 23:12:46,503 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,503 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef57228f-5de2-4966-b551-4cb53d66416f
2023-06-22 23:12:46,503 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,503 - distributed.worker - INFO -               Threads:                          1
2023-06-22 23:12:46,503 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 23:12:46,503 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ajrktdy_
2023-06-22 23:12:46,504 - distributed.worker - INFO - Starting Worker plugin PreImport-bc0528a0-9399-4292-bf18-e8f624d992ea
2023-06-22 23:12:46,504 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3263d97c-af74-4714-80db-9abd362a8c99
2023-06-22 23:12:46,504 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46ddb30e-cb4d-40f1-9d56-511030464cba
2023-06-22 23:12:46,510 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:38535
2023-06-22 23:12:46,511 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:38535
2023-06-22 23:12:46,511 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44237
2023-06-22 23:12:46,511 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,511 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,511 - distributed.worker - INFO -               Threads:                          1
2023-06-22 23:12:46,511 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 23:12:46,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8gzpqld0
2023-06-22 23:12:46,512 - distributed.worker - INFO - Starting Worker plugin PreImport-f257b5a4-7bf8-442c-b16a-fbe6f28e38d9
2023-06-22 23:12:46,512 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22290fbc-b671-4d44-b985-4964cd043cb5
2023-06-22 23:12:46,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc224332-ca58-475f-9136-f5be3b869a8d
2023-06-22 23:12:46,517 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:43195
2023-06-22 23:12:46,517 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:43195
2023-06-22 23:12:46,517 - distributed.worker - INFO -          dashboard at:        10.33.227.169:40355
2023-06-22 23:12:46,518 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,518 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,518 - distributed.worker - INFO -               Threads:                          1
2023-06-22 23:12:46,518 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 23:12:46,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zqrssq_e
2023-06-22 23:12:46,519 - distributed.worker - INFO - Starting Worker plugin PreImport-a851f6ad-e07d-4909-b056-f4a9b8f415e6
2023-06-22 23:12:46,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed5b9c6d-f5de-4351-843d-c8019a66823e
2023-06-22 23:12:46,519 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd11f989-06c9-433c-8247-401275b66841
2023-06-22 23:12:46,522 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:34483
2023-06-22 23:12:46,522 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:34483
2023-06-22 23:12:46,522 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44061
2023-06-22 23:12:46,522 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,522 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,522 - distributed.worker - INFO -               Threads:                          1
2023-06-22 23:12:46,522 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 23:12:46,522 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2b_0qq6j
2023-06-22 23:12:46,523 - distributed.worker - INFO - Starting Worker plugin RMMSetup-960b680b-1442-4cb8-af75-4f96c742c71d
2023-06-22 23:12:46,523 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:36829
2023-06-22 23:12:46,523 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:36829
2023-06-22 23:12:46,523 - distributed.worker - INFO -          dashboard at:        10.33.227.169:39437
2023-06-22 23:12:46,524 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,524 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,524 - distributed.worker - INFO -               Threads:                          1
2023-06-22 23:12:46,524 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 23:12:46,524 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t6zx4f7r
2023-06-22 23:12:46,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70787c42-da4a-44f7-9be5-214c72691d19
2023-06-22 23:12:46,703 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de0f98f9-97fa-4d96-88ad-106ad361f615
2023-06-22 23:12:46,704 - distributed.worker - INFO - Starting Worker plugin PreImport-89be3b6f-7d01-4bb3-a8e6-9fad211391f7
2023-06-22 23:12:46,704 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,704 - distributed.worker - INFO - Starting Worker plugin PreImport-02f93937-1fb9-44d9-a6a4-9c22d78f7730
2023-06-22 23:12:46,704 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,704 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,704 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b2a38d4-c85e-4162-b83b-9deb246cd99e
2023-06-22 23:12:46,704 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,705 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,705 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,714 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,714 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,715 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,715 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,715 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 23:12:46,716 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 23:12:46,718 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,718 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,719 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,720 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,720 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,721 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 23:12:46,721 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 23:12:46,721 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 23:12:46,722 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 23:12:46,723 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 23:12:46,724 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 23:12:48,188 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 23:12:48,188 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 23:12:48,188 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 23:12:48,188 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 23:12:48,188 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 23:12:48,188 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 23:12:48,188 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 23:12:48,191 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 23:12:48,283 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 23:12:48,283 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 23:12:48,283 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 23:12:48,284 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 23:12:48,284 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 23:12:48,284 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 23:12:48,284 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 23:12:48,284 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 23:12:59,319 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 23:12:59,375 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 23:12:59,378 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 23:12:59,449 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 23:12:59,451 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 23:12:59,514 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 23:12:59,669 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 23:12:59,818 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 23:13:05,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:13:05,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:13:05,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:13:05,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:13:05,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:13:05,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:13:05,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:13:05,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 23:13:38,449 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 23:13:38,449 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 23:13:38,454 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 23:13:38,454 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 23:13:38,454 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 23:13:38,454 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 23:13:38,454 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 23:13:38,455 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 23:13:42,721 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-fc74eb92-6c3e-4ce8-8c7d-89bc836b973e
Function:  execute_task
args:      ((<function apply at 0x7f95ba5f2cb0>, <function _call_plc_uniform_neighbor_sample at 0x7f91183343a0>, [b'}>g\xbf.\x1aG\xed\x88p\x86\x02\xc1\x90\xde\xd5', <pylibcugraph.graphs.MGGraph object at 0x7f8f50123390>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', -7608085559592853413], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small 1')"

2023-06-22 23:13:42,734 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-d384a061-808e-4422-9b20-80ca762a1421
Function:  execute_task
args:      ((<function apply at 0x7f51c4226cb0>, <function _call_plc_uniform_neighbor_sample at 0x7f4d101963b0>, [b'}>g\xbf.\x1aG\xed\x88p\x86\x02\xc1\x90\xde\xd5', <pylibcugraph.graphs.MGGraph object at 0x7f4b4d184bf0>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', 2690456601924654812], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small 1')"

2023-06-22 23:13:42,735 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-7e453f6b-b48d-4b20-aeec-6a127f63e9d8
Function:  execute_task
args:      ((<function apply at 0x7f5162c72cb0>, <function _call_plc_uniform_neighbor_sample at 0x7f4cb03c24d0>, [b'}>g\xbf.\x1aG\xed\x88p\x86\x02\xc1\x90\xde\xd5', <pylibcugraph.graphs.MGGraph object at 0x7f4aec11a710>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', -440678789501530780], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small 1')"

2023-06-22 23:13:42,735 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-a86a4164-13a7-4855-bf57-d4aacc17c2f9
Function:  execute_task
args:      ((<function apply at 0x7fd629122cb0>, <function _call_plc_uniform_neighbor_sample at 0x7fd17c76bd00>, [b'}>g\xbf.\x1aG\xed\x88p\x86\x02\xc1\x90\xde\xd5', <pylibcugraph.graphs.MGGraph object at 0x7fcfe01aebd0>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', 1272436555456266585], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small 1')"

2023-06-22 23:13:42,737 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-c5855e6d-f74f-4236-ab70-de06d4da081b
Function:  execute_task
args:      ((<function apply at 0x7effc6052cb0>, <function _call_plc_uniform_neighbor_sample at 0x7efaf87924d0>, [b'}>g\xbf.\x1aG\xed\x88p\x86\x02\xc1\x90\xde\xd5', <pylibcugraph.graphs.MGGraph object at 0x7ef96012a010>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', 1373364230558880460], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small 1')"

2023-06-22 23:13:42,737 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-db7566cb-e2ad-4d54-97eb-6ecfe3c187cf
Function:  execute_task
args:      ((<function apply at 0x7ff4eb412cb0>, <function _call_plc_uniform_neighbor_sample at 0x7ff084388940>, [b'}>g\xbf.\x1aG\xed\x88p\x86\x02\xc1\x90\xde\xd5', <pylibcugraph.graphs.MGGraph object at 0x7fee56b91a30>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', -6612257861657188965], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small 1')"

2023-06-22 23:13:42,737 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-ca2d4f9a-15a1-4b65-b453-ed21588a1c53
Function:  execute_task
args:      ((<function apply at 0x7fccfb866cb0>, <function _call_plc_uniform_neighbor_sample at 0x7fc82076e290>, [b'}>g\xbf.\x1aG\xed\x88p\x86\x02\xc1\x90\xde\xd5', <pylibcugraph.graphs.MGGraph object at 0x7fc672152bd0>, [Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], True, 8, 0, 9, array([10, 25], dtype=int32), False], (<class 'dict'>, [['weight_t', 'float32'], ['with_edge_properties', True], ['random_state', 3715321149955913830], ['return_offsets', False]])))
kwargs:    {}
Exception: "ValueError('start list too small 1')"

2023-06-22 23:19:05,044 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:34483. Reason: worker-close
2023-06-22 23:19:05,044 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:43195. Reason: worker-handle-scheduler-connection-broken
2023-06-22 23:19:05,044 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:41585. Reason: worker-close
2023-06-22 23:19:05,044 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:36829. Reason: worker-close
2023-06-22 23:19:05,044 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:38535. Reason: worker-handle-scheduler-connection-broken
2023-06-22 23:19:05,045 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:44595. Reason: worker-handle-scheduler-connection-broken
2023-06-22 23:19:05,045 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:42393. Reason: worker-handle-scheduler-connection-broken
2023-06-22 23:19:05,045 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:36493'. Reason: nanny-close
2023-06-22 23:19:05,047 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 23:19:05,046 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:35108 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 23:19:05,046 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:35092 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 23:19:05,046 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:35052 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 23:19:05,048 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:42617'. Reason: nanny-close
2023-06-22 23:19:05,049 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 23:19:05,049 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:34721'. Reason: nanny-close
2023-06-22 23:19:05,049 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 23:19:05,050 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:38311'. Reason: nanny-close
2023-06-22 23:19:05,050 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 23:19:05,050 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:39191'. Reason: nanny-close
2023-06-22 23:19:05,051 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 23:19:05,051 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:36541'. Reason: nanny-close
2023-06-22 23:19:05,051 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 23:19:05,052 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:33077'. Reason: nanny-close
2023-06-22 23:19:05,052 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 23:19:05,052 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:37051'. Reason: nanny-close
2023-06-22 23:19:05,053 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 23:19:05,066 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:42617 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:47018 remote=tcp://10.33.227.169:42617>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:42617 after 100 s
2023-06-22 23:19:05,066 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:36541 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:58104 remote=tcp://10.33.227.169:36541>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:36541 after 100 s
2023-06-22 23:19:05,068 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:37051 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:40356 remote=tcp://10.33.227.169:37051>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:37051 after 100 s
2023-06-22 23:19:05,072 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:33077 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:60634 remote=tcp://10.33.227.169:33077>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:33077 after 100 s
2023-06-22 23:19:05,073 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:34721 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:60960 remote=tcp://10.33.227.169:34721>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:34721 after 100 s
2023-06-22 23:19:05,073 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:38311 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:37906 remote=tcp://10.33.227.169:38311>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:38311 after 100 s
2023-06-22 23:19:08,254 - distributed.nanny - WARNING - Worker process still alive after 3.1999865722656256 seconds, killing
2023-06-22 23:19:08,255 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 23:19:08,256 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 23:19:08,256 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 23:19:08,257 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 23:19:08,257 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 23:19:08,258 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 23:19:08,264 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 23:19:09,047 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 23:19:09,051 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 23:19:09,051 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 23:19:09,052 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 23:19:09,052 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 23:19:09,053 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 23:19:09,053 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 23:19:09,054 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 23:19:09,056 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1504353 parent=1504263 started daemon>
2023-06-22 23:19:09,056 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1504351 parent=1504263 started daemon>
2023-06-22 23:19:09,056 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1504347 parent=1504263 started daemon>
2023-06-22 23:19:09,056 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1504344 parent=1504263 started daemon>
2023-06-22 23:19:09,056 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1504341 parent=1504263 started daemon>
2023-06-22 23:19:09,056 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1504338 parent=1504263 started daemon>
2023-06-22 23:19:09,056 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1504335 parent=1504263 started daemon>
2023-06-22 23:19:09,056 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1504332 parent=1504263 started daemon>
2023-06-22 23:19:09,475 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1504338 exit status was already read will report exitcode 255
2023-06-22 23:19:09,710 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1504332 exit status was already read will report exitcode 255
2023-06-22 23:19:09,839 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1504351 exit status was already read will report exitcode 255
2023-06-22 23:19:10,108 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1504344 exit status was already read will report exitcode 255
