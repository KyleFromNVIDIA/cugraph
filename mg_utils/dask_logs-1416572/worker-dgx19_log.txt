RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=12G
             --local-directory=/tmp/
             --scheduler-file=/root/work/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-22 21:18:51,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:43813'
2023-06-22 21:18:51,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:33489'
2023-06-22 21:18:51,574 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:46669'
2023-06-22 21:18:51,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:39725'
2023-06-22 21:18:51,577 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:43985'
2023-06-22 21:18:51,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:36063'
2023-06-22 21:18:51,582 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:40643'
2023-06-22 21:18:51,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:44053'
2023-06-22 21:18:53,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:18:53,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:18:53,136 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:18:53,136 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:18:53,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:18:53,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:18:53,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:18:53,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:18:53,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:18:53,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:18:53,142 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:18:53,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:18:53,145 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:18:53,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:18:53,145 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:18:53,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:18:53,492 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:18:53,556 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:18:53,589 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:18:53,589 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:18:53,592 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:18:53,594 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:18:53,607 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:18:53,610 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:18:56,314 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:33003
2023-06-22 21:18:56,314 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:33003
2023-06-22 21:18:56,314 - distributed.worker - INFO -          dashboard at:        10.33.227.169:45871
2023-06-22 21:18:56,314 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,314 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,314 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:18:56,314 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:18:56,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bfpz592y
2023-06-22 21:18:56,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0053ea60-af3e-4fb3-9b46-abaa891cf337
2023-06-22 21:18:56,321 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:39665
2023-06-22 21:18:56,321 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:39665
2023-06-22 21:18:56,321 - distributed.worker - INFO -          dashboard at:        10.33.227.169:39511
2023-06-22 21:18:56,321 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,321 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,321 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:18:56,321 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:18:56,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-676t1iue
2023-06-22 21:18:56,322 - distributed.worker - INFO - Starting Worker plugin PreImport-685e9e7d-6fa5-4dc2-90a7-e44ede7bef0d
2023-06-22 21:18:56,322 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db6f6cd9-b85d-489e-9a9f-483884a918e3
2023-06-22 21:18:56,322 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dea70239-640e-490f-979e-ba1f15867637
2023-06-22 21:18:56,324 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:46823
2023-06-22 21:18:56,325 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:46823
2023-06-22 21:18:56,325 - distributed.worker - INFO -          dashboard at:        10.33.227.169:38549
2023-06-22 21:18:56,325 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,325 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,325 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:18:56,325 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:18:56,325 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qrjl0ka4
2023-06-22 21:18:56,325 - distributed.worker - INFO - Starting Worker plugin PreImport-528f9e23-e41c-44a4-9428-b3bad72f30a7
2023-06-22 21:18:56,325 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ad3a46dc-5be1-4f3a-8bb5-48c06fe1a58b
2023-06-22 21:18:56,326 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c4650d6-7767-4c76-98ce-e0b769783a6b
2023-06-22 21:18:56,326 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:34955
2023-06-22 21:18:56,327 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:34955
2023-06-22 21:18:56,327 - distributed.worker - INFO -          dashboard at:        10.33.227.169:35409
2023-06-22 21:18:56,327 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,327 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,327 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:18:56,327 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:18:56,327 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-182oplzg
2023-06-22 21:18:56,327 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:46481
2023-06-22 21:18:56,327 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:46481
2023-06-22 21:18:56,327 - distributed.worker - INFO -          dashboard at:        10.33.227.169:42739
2023-06-22 21:18:56,327 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,327 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,327 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:18:56,328 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:18:56,328 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u3qxx481
2023-06-22 21:18:56,328 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:42881
2023-06-22 21:18:56,328 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:42881
2023-06-22 21:18:56,328 - distributed.worker - INFO - Starting Worker plugin PreImport-bf837d78-415a-42fd-ba90-a5e5ebe45751
2023-06-22 21:18:56,328 - distributed.worker - INFO -          dashboard at:        10.33.227.169:38313
2023-06-22 21:18:56,328 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:37927
2023-06-22 21:18:56,328 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,328 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e1c2138-af8e-4a6a-96f0-bd7e2fca27aa
2023-06-22 21:18:56,328 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:37927
2023-06-22 21:18:56,328 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:18:56,328 - distributed.worker - INFO -          dashboard at:        10.33.227.169:43419
2023-06-22 21:18:56,328 - distributed.worker - INFO - Starting Worker plugin PreImport-b3a3e236-63d4-4095-b3c7-4cbcb867ce1f
2023-06-22 21:18:56,328 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:18:56,328 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,328 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q0k244ur
2023-06-22 21:18:56,328 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f8f2205-4a80-40d3-b0ee-327d2ff230fc
2023-06-22 21:18:56,328 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:18:56,328 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:18:56,328 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6e8c9d6n
2023-06-22 21:18:56,328 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd587669-fd95-4386-af50-916764f7b814
2023-06-22 21:18:56,329 - distributed.worker - INFO - Starting Worker plugin PreImport-4b9bdb31-97f9-44aa-ac91-f75f7be5f0fb
2023-06-22 21:18:56,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87bf0280-c8cd-4837-bf75-baab32f7182c
2023-06-22 21:18:56,329 - distributed.worker - INFO - Starting Worker plugin RMMSetup-47f09117-e6cb-4a64-af68-ea931e4b97c4
2023-06-22 21:18:56,329 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f242219-f332-44e4-a595-c7e9bc66d3ee
2023-06-22 21:18:56,329 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:44859
2023-06-22 21:18:56,329 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:44859
2023-06-22 21:18:56,329 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc205902-d55c-48ed-a1dc-d15dec4a90cd
2023-06-22 21:18:56,329 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44525
2023-06-22 21:18:56,329 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,329 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,329 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:18:56,330 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:18:56,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6tut9t0e
2023-06-22 21:18:56,331 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c88b0e5-6f03-4b98-be48-4ce82824c1f2
2023-06-22 21:18:56,573 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,573 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d44163d3-504e-4e5b-9218-23507f1a5e23
2023-06-22 21:18:56,573 - distributed.worker - INFO - Starting Worker plugin PreImport-d280ed5a-4951-4566-ac81-ec39fbb4bac1
2023-06-22 21:18:56,573 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,573 - distributed.worker - INFO - Starting Worker plugin PreImport-7d5a835c-ba43-4e82-8a09-786cc36b1025
2023-06-22 21:18:56,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-abcf5de7-c1d0-45f0-9d26-4b59ea27d900
2023-06-22 21:18:56,574 - distributed.worker - INFO - Starting Worker plugin PreImport-7abd7595-f695-4c1c-a1ed-6c8e9d6ae881
2023-06-22 21:18:56,574 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42b91d1d-6fdf-4243-86ec-4f0c84c4279a
2023-06-22 21:18:56,574 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,574 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,574 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,575 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,575 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,860 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,861 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:18:56,862 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,863 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:18:56,863 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,864 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,864 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,865 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,865 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,866 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:18:56,866 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,866 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,867 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:18:56,867 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:18:56,867 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,867 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,868 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:18:56,868 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:18:56,869 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:18:56,871 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:18:56,872 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:19:02,516 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:02,516 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:02,517 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:02,518 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:02,520 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:02,521 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:02,521 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:02,521 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:02,590 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:19:02,590 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:19:02,590 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:19:02,590 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:19:02,590 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:19:02,590 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:19:02,590 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:19:02,591 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:19:13,735 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:19:13,773 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:19:13,776 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:19:13,776 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:19:13,812 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:19:13,922 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:19:13,958 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:19:14,009 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:19:20,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:19:20,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:19:20,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:19:20,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:19:20,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:19:20,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:19:20,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:19:20,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:19:52,928 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:19:52,933 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:19:52,933 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:19:52,933 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:19:52,933 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:19:52,933 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:19:52,933 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:19:52,934 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:19:57,096 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,096 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,096 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,096 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,096 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,096 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,096 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,096 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,642 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,647 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,647 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,647 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,647 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,648 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,648 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,648 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,946 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,951 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,951 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,952 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,952 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,952 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,952 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:57,952 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,282 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,287 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,287 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,287 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,287 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,287 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,287 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,287 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,610 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,615 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,615 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,615 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,615 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,615 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,615 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,615 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,950 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,955 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,955 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,955 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,955 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,955 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,955 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:58,955 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,309 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,314 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,314 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,314 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,314 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,314 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,315 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,315 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,705 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,710 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,710 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,711 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,711 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,711 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,711 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:19:59,711 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,109 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,114 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,114 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,114 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,114 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,114 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,114 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,115 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,522 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,528 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,528 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,528 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,528 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,528 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,528 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:00,528 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:20:01,658 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:20:01,658 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:20:01,658 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:20:01,658 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:20:01,658 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:20:01,658 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:20:01,658 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:20:01,658 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:27:56,084 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:44859. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:27:56,084 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:39665. Reason: worker-close
2023-06-22 21:27:56,084 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:46823. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:27:56,084 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:46481. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:27:56,084 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:33003. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:27:56,084 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:42881. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:27:56,085 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:34955. Reason: worker-close
2023-06-22 21:27:56,085 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:43813'. Reason: nanny-close
2023-06-22 21:27:56,088 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:27:56,086 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:40378 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:27:56,089 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:33489'. Reason: nanny-close
2023-06-22 21:27:56,090 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:27:56,090 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:46669'. Reason: nanny-close
2023-06-22 21:27:56,091 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:27:56,091 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:39725'. Reason: nanny-close
2023-06-22 21:27:56,092 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:27:56,089 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:40384 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:27:56,092 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:43985'. Reason: nanny-close
2023-06-22 21:27:56,092 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
Process Dask Worker process (from Nanny):
2023-06-22 21:27:56,093 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:36063'. Reason: nanny-close
Traceback (most recent call last):
2023-06-22 21:27:56,093 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 202, in _run
    target(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 999, in _run
    asyncio.run(run())
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 47, in run
    _cancel_all_tasks(loop)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 63, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 685, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 919, in _run
    val = self.callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 707, in _measure_tick
    self.digest_metric("tick-duration", tick_duration)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 2113, in digest_metric
    ServerNode.digest_metric(self, name, value)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1064, in digest_metric
    self.digests_max[name] = max(self.digests_max[name], value)
KeyboardInterrupt
2023-06-22 21:27:56,094 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:40643'. Reason: nanny-close
2023-06-22 21:27:56,094 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:27:56,094 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:44053'. Reason: nanny-close
2023-06-22 21:27:56,095 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:27:56,103 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:43813 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:60408 remote=tcp://10.33.227.169:43813>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:43813 after 100 s
2023-06-22 21:27:56,113 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:44053 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:39316 remote=tcp://10.33.227.169:44053>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:44053 after 100 s
2023-06-22 21:27:56,114 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:33489 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:33384 remote=tcp://10.33.227.169:33489>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:33489 after 100 s
2023-06-22 21:27:56,115 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:46669 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:50586 remote=tcp://10.33.227.169:46669>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:46669 after 100 s
2023-06-22 21:27:56,116 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:43985 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:57598 remote=tcp://10.33.227.169:43985>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:43985 after 100 s
2023-06-22 21:27:56,118 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:40643 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:36234 remote=tcp://10.33.227.169:40643>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:40643 after 100 s
2023-06-22 21:27:56,117 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:36063 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:37106 remote=tcp://10.33.227.169:36063>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:36063 after 100 s
2023-06-22 21:27:57,473 - distributed.nanny - INFO - Worker process 1416749 exited with status 1
2023-06-22 21:27:59,296 - distributed.nanny - WARNING - Worker process still alive after 3.1999815368652347 seconds, killing
2023-06-22 21:27:59,296 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:27:59,297 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing
2023-06-22 21:27:59,299 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:27:59,300 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 21:27:59,300 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-22 21:27:59,301 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:28:00,058 - distributed.nanny - INFO - Worker process 1416761 was killed by signal 9
2023-06-22 21:28:00,088 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:28:00,091 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:28:00,091 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:28:00,094 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:28:00,094 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:28:00,094 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:28:00,096 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1416759 parent=1416717 started daemon>
2023-06-22 21:28:00,096 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1416755 parent=1416717 started daemon>
2023-06-22 21:28:00,096 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1416752 parent=1416717 started daemon>
2023-06-22 21:28:00,096 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1416746 parent=1416717 started daemon>
2023-06-22 21:28:00,096 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1416743 parent=1416717 started daemon>
2023-06-22 21:28:00,096 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1416740 parent=1416717 started daemon>
2023-06-22 21:28:00,446 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1416755 exit status was already read will report exitcode 255
2023-06-22 21:28:00,555 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1416759 exit status was already read will report exitcode 255
2023-06-22 21:28:00,816 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1416752 exit status was already read will report exitcode 255
