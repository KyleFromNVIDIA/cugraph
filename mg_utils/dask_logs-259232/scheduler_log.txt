RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 16:34:03,158 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 16:34:03,671 - distributed.scheduler - INFO - State start
2023-06-26 16:34:03,672 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-sjhllv2k', purging
2023-06-26 16:34:03,672 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o8h98zdl', purging
2023-06-26 16:34:03,673 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1memsgd1', purging
2023-06-26 16:34:03,673 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-17_dpis5', purging
2023-06-26 16:34:03,673 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6k1l5xss', purging
2023-06-26 16:34:03,673 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gae4jepv', purging
2023-06-26 16:34:03,673 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-aijl4g_o', purging
2023-06-26 16:34:03,673 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ui0sr30v', purging
2023-06-26 16:34:03,674 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4lj_vqtf', purging
2023-06-26 16:34:03,674 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ua75wjka', purging
2023-06-26 16:34:03,674 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4ketkvuh', purging
2023-06-26 16:34:03,674 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-178jjvdf', purging
2023-06-26 16:34:03,674 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1b35z5ot', purging
2023-06-26 16:34:03,674 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ucmdj0qp', purging
2023-06-26 16:34:03,675 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cj8vallf', purging
2023-06-26 16:34:03,687 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 16:34:03,687 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 16:34:03,688 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 16:34:22,354 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34651', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,357 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34651
2023-06-26 16:34:22,357 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45828
2023-06-26 16:34:22,420 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33579', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,421 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33579
2023-06-26 16:34:22,421 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45836
2023-06-26 16:34:22,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46249', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,440 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46249
2023-06-26 16:34:22,440 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45848
2023-06-26 16:34:22,455 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38051', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,455 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38051
2023-06-26 16:34:22,455 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45856
2023-06-26 16:34:22,498 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35233', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,499 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35233
2023-06-26 16:34:22,499 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45866
2023-06-26 16:34:22,517 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43639', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,517 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43639
2023-06-26 16:34:22,517 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45872
2023-06-26 16:34:22,583 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42471', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42471
2023-06-26 16:34:22,584 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45878
2023-06-26 16:34:22,625 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40261', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,626 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40261
2023-06-26 16:34:22,626 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45894
2023-06-26 16:34:22,674 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34511', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,674 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34511
2023-06-26 16:34:22,674 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45906
2023-06-26 16:34:22,686 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33491', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,686 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33491
2023-06-26 16:34:22,686 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45930
2023-06-26 16:34:22,697 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42623', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,697 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42623
2023-06-26 16:34:22,697 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45914
2023-06-26 16:34:22,713 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37371', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,713 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37371
2023-06-26 16:34:22,713 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45946
2023-06-26 16:34:22,718 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39223', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,719 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39223
2023-06-26 16:34:22,719 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45954
2023-06-26 16:34:22,721 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33107', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,721 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33107
2023-06-26 16:34:22,722 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45970
2023-06-26 16:34:22,732 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36859', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,733 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36859
2023-06-26 16:34:22,733 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45990
2023-06-26 16:34:22,736 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33835', status: init, memory: 0, processing: 0>
2023-06-26 16:34:22,736 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33835
2023-06-26 16:34:22,736 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45980
2023-06-26 16:34:38,687 - distributed.scheduler - INFO - Receive client connection: Client-57fd5a5c-143f-11ee-b7c4-5cff35c1a711
2023-06-26 16:34:38,688 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45656
2023-06-26 16:34:39,396 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 16:35:30,579 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:44,345 - distributed.protocol.pickle - INFO - Failed to deserialize b'\x80\x05\x95\x9d`\x00\x00\x00\x00\x00\x00\x8c\x1edistributed.protocol.serialize\x94\x8c\x08ToPickle\x94\x93\x94)\x81\x94}\x94\x8c\x04data\x94\x8c\x13dask.highlevelgraph\x94\x8c\x0eHighLevelGraph\x94\x93\x94)\x81\x94}\x94(\x8c\x0cdependencies\x94}\x94(\x8c1__filter_batches-8509657c273f004c94ff54b0d7ad903a\x94\x8f\x94(\x8c\'rename-8a712c9b6b233ea2be42c77b5f37b578\x94\x90h\x0f\x8f\x94u\x8c\x10key_dependencies\x94}\x94\x8c\x06layers\x94}\x94(h\r\x8c\x0edask.blockwise\x94\x8c\tBlockwise\x94\x93\x94)\x81\x94}\x94(\x8c\x0bannotations\x94N\x8c\x16collection_annotations\x94}\x94(\x8c\x0bnpartitions\x94K\x04\x8c\x07columns\x94]\x94(\x8c\x07_BATCH_\x94\x8c\x07_START_\x94e\x8c\x04type\x94\x8c\x18dask_cudf.core.DataFrame\x94\x8c\x0edataframe_type\x94\x8c\x1dcudf.core.dataframe.DataFrame\x94\x8c\rseries_dtypes\x94}\x94(h \x8c\x05numpy\x94\x8c\x05dtype\x94\x93\x94\x8c\x02i4\x94\x89\x88\x87\x94R\x94(K\x03\x8c\x01<\x94NNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK\x00t\x94bh!h*\x8c\x02i8\x94\x89\x88\x87\x94R\x94(K\x03h.NNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK\x00t\x94buu\x8c\x06output\x94h\r\x8c\x0eoutput_indices\x94\x8c\x02.0\x94\x85\x94\x8c\routput_blocks\x94N\x8c\x03dsk\x94}\x94h\r(\x8c\ndask.utils\x94\x8c\x05apply\x94\x93\x94\x8c\x13dask.dataframe.core\x94\x8c\x11apply_and_enforce\x94\x93\x94]\x94(\x8c\x13__dask_blockwise__0\x94\x8c\x13__dask_blockwise__1\x94\x8c\x13__dask_blockwise__2\x94\x8c\x13__dask_blockwise__3\x94\x8c\x13__dask_blockwise__4\x94e\x8c\x08builtins\x94\x8c\x04dict\x94\x93\x94]\x94(]\x94(\x8c\x05_func\x94\x8c\x17cloudpickle.cloudpickle\x94\x8c\x0e_make_function\x94\x93\x94(hM\x8c\r_builtin_type\x94\x93\x94\x8c\x08CodeType\x94\x85\x94R\x94(K\x01K\x00K\x00K\x03K\x05J\x1f\x00\x00\x01C\x16\x88\x00|\x01i\x00|\x02\xa4\x01d\x01|\x00i\x01\xa4\x01\x8e\x01S\x00\x94N\x8c\x0epartition_info\x94\x86\x94)hV\x8c\x04args\x94\x8c\x06kwargs\x94\x87\x94\x8cJ/opt/conda/envs/rapids/lib/python3.10/site-packages/dask/dataframe/core.py\x94\x8c\x04func\x94ME\x1bC\x02\x16\x01\x94\x8c\torig_func\x94\x85\x94)t\x94R\x94}\x94(\x8c\x0b__package__\x94\x8c\x0edask.dataframe\x94\x8c\x08__name__\x94h>\x8c\x08__file__\x94h[uNNhM\x8c\x10_make_empty_cell\x94\x93\x94)R\x94\x85\x94t\x94R\x94\x8c\x1ccloudpickle.cloudpickle_fast\x94\x8c\x12_function_setstate\x94\x93\x94hl}\x94}\x94(heh\\\x8c\x0c__qualname__\x94\x8c\x1cmap_partitions.<locals>.func\x94\x8c\x0f__annotations__\x94}\x94\x8c\x0e__kwdefaults__\x94N\x8c\x0c__defaults__\x94N\x8c\n__module__\x94h>\x8c\x07__doc__\x94N\x8c\x0b__closure__\x94hM\x8c\n_make_cell\x94\x93\x94hQ\x8c\nMethodType\x94\x85\x94R\x94hO(hT(K\x05K\x00K\x00K\x07K\x03KCC^|\x04d\x00u\x00s\nt\x00|\x01\x83\x01d\x01k\x02r\x0cd\x00S\x00|\x04d\x02k\x03r\x1dt\x01|\x04t\x02\x83\x02s\x1dt\x03|\x04\x83\x01\x01\x00t\x04d\x03\x83\x01\x82\x01|\x01|\x02\x19\x00|\x03k\x01}\x05|\x01j\x05|\x05\x19\x00}\x06|\x01|\x05\x0f\x00\x19\x00}\x01|\x06S\x00\x94(NK\x00\x8c\x02sg\x94\x8c\x1fInvalid value of partition_info\x94t\x94(\x8c\x03len\x94\x8c\nisinstance\x94\x8c\x04dict\x94\x8c\x05print\x94\x8c\nValueError\x94\x8c\x03loc\x94t\x94(\x8c\x04self\x94\x8c\x02df\x94\x8c\x0ebatch_col_name\x94\x8c\x0cmax_batch_id\x94hV\x8c\x01f\x94h4t\x94\x8c\\/opt/conda/envs/rapids/lib/python3.10/site-packages/cugraph/gnn/data_loading/bulk_sampler.py\x94\x8c\x10__filter_batches\x94M5\x01C\x12\x14\x08\x04\x01\x12\x01\x08\x01\x08\x01\x0c\x02\n\x01\n\x01\x04\x01\x94))t\x94R\x94}\x94(hc\x8c\x18cugraph.gnn.data_loading\x94he\x8c%cugraph.gnn.data_loading.bulk_sampler\x94hf\x8c\\/opt/conda/envs/rapids/lib/python3.10/site-packages/cugraph/gnn/data_loading/bulk_sampler.py\x94uNNNt\x94R\x94hoh\x9b}\x94}\x94(heh\x92hr\x8c*EXPERIMENTAL__BulkSampler.__filter_batches\x94ht}\x94(h\x8c\x8c\x13cudf.core.dataframe\x94\x8c\tDataFrame\x94\x93\x94h\x8dhG\x8c\x03str\x94\x93\x94h\x8ehG\x8c\x03int\x94\x93\x94hV\x8c\t_operator\x94\x8c\x07getitem\x94\x93\x94\x8c\x06typing\x94\x8c\x05Union\x94\x93\x94hIh\xa4hG\x8c\x04type\x94\x93\x94N\x85\x94R\x94\x87\x94\x86\x94R\x94\x8c\x06return\x94h\xa2uhvNhwN\x85\x94hxh\x98hyNhzN\x8c\x17_cloudpickle_submodules\x94]\x94\x8c\x0b__globals__\x94}\x94u\x86\x94\x86R0\x8c\x0b__mp_main__\x94\x8c\x0bBulkSampler\x94\x93\x94)\x81\x94}\x94(\x8c"_EXPERIMENTAL__BulkSampler__logger\x94\x8c\x07logging\x94\x8c\tgetLogger\x94\x93\x94h\x98\x85\x94R\x94\x8c&_EXPERIMENTAL__BulkSampler__batch_size\x94M\x00\x02\x8c\'_EXPERIMENTAL__BulkSampler__output_path\x94\x8c6/tmp/ramdisk/ogbn_papers100M[4]_b512_f[10, 25]/samples\x94\x8c!_EXPERIMENTAL__BulkSampler__graph\x94\x8c\x1fcugraph.structure.graph_classes\x94\x8c\nMultiGraph\x94\x93\x94)\x81\x94}\x94(\x8c\x05_Impl\x94\x8c=cugraph.structure.graph_implementation.simpleDistributedGraph\x94\x8c\x1asimpleDistributedGraphImpl\x94\x93\x94)\x81\x94}\x94(\x8c\x08edgelist\x94h\xd0\x8c#simpleDistributedGraphImpl.EdgeList\x94\x93\x94)\x81\x94}\x94(\x8c\x0bedgelist_df\x94\x8c\x0edask_cudf.core\x94h\xa1\x93\x94)\x81\x94(h\x08)\x81\x94}\x94(h\x0b}\x94(\x8c\'assign-69a0b9ab47a6438ec6531b802bca2fc7\x94\x8f\x94(\x8c(getitem-f4b4ded0970646abd54aa4dc531871f3\x94\x8c\'rename-de9ff2b24a878be3a858e3f783c9e41e\x94\x90\x8c\'assign-79e7b6b9955aff4f05b61c6a13c74862\x94\x8f\x94(\x8c\'assign-7a31f8143cc223f15cd3762c9b451864\x94\x8c$add-bd81a73d3da345912094025794950413\x94\x90h\xe7\x8f\x94(\x8c-read-parquet-1a83665a6299205a0cf4e9c5910a7701\x94\x8c$add-2b5167922093b28b8b69868e84a15525\x94\x90h\xea\x8f\x94\x8c(getitem-36ffef13e0cce494c224877403ba899d\x94\x8f\x94(h\xea\x90h\xeb\x8f\x94(h\xed\x90\x8c(getitem-0f5db0edc6a99e5906eedb06c6888b74\x94\x8f\x94(h\xe7\x90h\xe8\x8f\x94(h\xf0\x90\x8c._replicate_df-4f833667d75dea42e28fb87b4519fdd8\x94\x8f\x94(h\xe5\x90\x8c(getitem-ce4e0bb2248faa6434d90d1cc3133d9a\x94\x8f\x94(h\xf3\x90\x8c(getitem-7f111edba0184128c8504c60bc98475f\x94\x8f\x94(h\xf5\x90\x8c)to_frame-a81c2b1d1a029a0b3f8e9991e693ad46\x94\x8f\x94(h\xf7\x90h\xe4\x8f\x94(h\xf9\x90h\xe3\x8f\x94(h\xf5\x90uh\x11}\x94h\x13}\x94(h\xe1h\x17)\x81\x94}\x94(h\x1aNh\x1b}\x94(h\x1dK h\x1e]\x94(\x8c\x03src\x94\x8c\x03dst\x94eh"\x8c\x18dask_cudf.core.DataFrame\x94h$\x8c\x1dcudf.core.dataframe.DataFrame\x94h&}\x94(j\x03\x01\x00\x00h2j\x04\x01\x00\x00h2uuh4h\xe1h5\x8c\x02.0\x94\x85\x94h8Nh9}\x94h\xe1(\x8c\x16dask.dataframe.methods\x94\x8c\x06assign\x94\x93\x94\x8c\x13__dask_blockwise__0\x94\x8c\x13__dask_blockwise__1\x94\x8c\x13__dask_blockwise__2\x94t\x94s\x8c\tnumblocks\x94}\x94(h\xe4K \x85\x94h\xe3K \x85\x94u\x8c\x07io_deps\x94}\x94\x8c\x07indices\x94h\xe4j\x08\x01\x00\x00\x85\x94\x86\x94j\x04\x01\x00\x00N\x86\x94h\xe3j\x08\x01\x00\x00\x85\x94\x86\x94\x87\x94\x8c\x0bconcatenate\x94N\x8c\x08new_axes\x94}\x94ubh\xe5h\x17)\x81\x94}\x94(h\x1aNh\x1b}\x94(h\x1dK h\x1e]\x94(\x8c\x03src\x94\x8c\x03dst\x94eh"\x8c\x18dask_cudf.core.DataFrame\x94h$\x8c\x1dcudf.core.dataframe.DataFrame\x94h&}\x94(j&\x01\x00\x00h2j\'\x01\x00\x00h2uuh4h\xe5h5\x8c\x02.0\x94\x85\x94h8Nh9}\x94h\xe5(j\r\x01\x00\x00\x8c\x13__dask_blockwise__0\x94\x8c\x13__dask_blockwise__1\x94\x8c\x13__dask_blockwise__2\x94t\x94sj\x12\x01\x00\x00}\x94(h\xe7K \x85\x94h\xe8K \x85\x94uj\x16\x01\x00\x00}\x94j\x18\x01\x00\x00h\xe7j+\x01\x00\x00\x85\x94\x86\x94j\x04\x01\x00\x00N\x86\x94h\xe8j+\x01\x00\x00\x85\x94\x86\x94\x87\x94j\x1f\x01\x00\x00Nj \x01\x00\x00}\x94ubh\xe7h\x17)\x81\x94}\x94(h\x1aNh\x1b}\x94(h\x1dK h\x1e]\x94(j&\x01\x00\x00j\'\x01\x00\x00eh"\x8c\x18dask_cudf.core.DataFrame\x94h$\x8c\x1dcudf.core.dataframe.DataFrame\x94h&}\x94(j&\x01\x00\x00h2j\'\x01\x00\x00h2uuh4h\xe7h5\x8c\x02.0\x94\x85\x94h8Nh9}\x94h\xe7(j\r\x01\x00\x00\x8c\x13__dask_blockwise__0\x94\x8c\x13__dask_blockwise__1\x94\x8c\x13__dask_blockwise__2\x94t\x94sj\x12\x01\x00\x00}\x94(h\xeaK \x85\x94h\xebK \x85\x94uj\x16\x01\x00\x00}\x94j\x18\x01\x00\x00h\xeajD\x01\x00\x00\x85\x94\x86\x94j\x03\x01\x00\x00N\x86\x94h\xebjD\x01\x00\x00\x85\x94\x86\x94\x87\x94j\x1f\x01\x00\x00Nj \x01\x00\x00}\x94ubh\xea\x8c\x0bdask.layers\x94\x8c\x10DataFrameIOLayer\x94\x93\x94)\x81\x94}\x94(\x8c\x04name\x94h\xea\x8c\x08_columns\x94]\x94(j&\x01\x00\x00j\'\x01\x00\x00e\x8c\x06inputs\x94]\x94(]\x94}\x94\x8c\x05piece\x94\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/0.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/1.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/2.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/3.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/4.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/5.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/6.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/7.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/8.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/9.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/10.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/11.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/12.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/13.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/14.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/15.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/16.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/17.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/18.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/19.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/20.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/21.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/22.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/23.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/24.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/25.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/26.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/27.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/28.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/29.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/30.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/31.parquet\x94]\x94K\x00a]\x94\x87\x94sae\x8c\x07io_func\x94\x8c\x1edask.dataframe.io.parquet.core\x94\x8c\x16ParquetFunctionWrapper\x94\x93\x94)\x81\x94}\x94(\x8c\x06engine\x94\x8c\x14dask_cudf.io.parquet\x94\x8c\nCudfEngine\x94\x93\x94\x8c\x02fs\x94\x8c\x0bfsspec.spec\x94\x8c\rmake_instance\x94\x93\x94\x8c\x1cfsspec.implementations.local\x94\x8c\x0fLocalFileSystem\x94\x93\x94)}\x94\x87\x94R\x94\x8c\x04meta\x94h\x7fhO(hT(K\x03K\x00K\x00K\x04K\x06KCC.d\x01d\x02\x84\x00t\x00|\x01d\x03\x19\x00t\x01t\x02|\x02\x83\x02\x83\x02D\x00\x83\x01}\x02|\x00\xa0\x03|\x01|\x02\xa1\x02}\x03|\x03S\x00\x94(X\x04\x02\x00\x00Perform device-side deserialization tasks.\n\n        Parameters\n        ----------\n        header : dict\n            The metadata required to reconstruct the object.\n        frames : list\n            The Buffers or memoryviews that the object should contain.\n\n        Returns\n        -------\n        Serializable\n            A new instance of `cls` (a subclass of `Serializable`) equivalent\n            to the instance that was serialized to produce the header and\n            frames.\n\n        :meta private:\n        \x94hT(K\x01K\x00K\x00K\x03K\x05KSC&g\x00|\x00]\x0f\\\x02}\x01}\x02|\x01r\x0ft\x00j\x01j\x02\xa0\x03|\x02\xa1\x01n\x01|\x02\x91\x02q\x02S\x00\x94)(\x8c\x04cudf\x94\x8c\x04core\x94\x8c\x06buffer\x94\x8c\tas_buffer\x94t\x94\x8c\x02.0\x94\x8c\x01c\x94h\x8f\x87\x94\x8cD/opt/conda/envs/rapids/lib/python3.10/site-packages/cudf/core/abc.py\x94\x8c\n<listcomp>\x94K\xacC\x08\x06\x00\x06\x02\x14\xff\x06\xff\x94))t\x94R\x94\x8c1Serializable.host_deserialize.<locals>.<listcomp>\x94\x8c\x07is-cuda\x94t\x94(\x8c\x03zip\x94\x8c\x03map\x94\x8c\nmemoryview\x94\x8c\x12device_deserialize\x94t\x94(\x8c\x03cls\x94\x8c\x06header\x94\x8c\x06frames\x94\x8c\x03obj\x94t\x94jA\x02\x00\x00\x8c\x10host_deserialize\x94K\x98C\n\x06\x14\x12\x02\x06\xfe\x0c\x04\x04\x01\x94))t\x94R\x94}\x94(hc\x8c\tcudf.core\x94he\x8c\rcudf.core.abc\x94hfjA\x02\x00\x00uNNNt\x94R\x94hoj[\x02\x00\x00}\x94}\x94(hejS\x02\x00\x00hr\x8c\x1dSerializable.host_deserialize\x94ht}\x94hvNhwNhxjY\x02\x00\x00hyj7\x02\x00\x00hzNh\xb6]\x94h\xb8}\x94j9\x02\x00\x00hM\x8c\tsubimport\x94\x93\x94j9\x02\x00\x00\x85\x94R\x94su\x86\x94\x86R0h\xa2\x86\x94R\x94}\x94(\x8c\x0ftype-serialized\x94C0\x80\x04\x95%\x00\x00\x00\x00\x00\x00\x00\x8c\x13cudf.core.dataframe\x94\x8c\tDataFrame\x94\x93\x94.\x94\x8c\x0ccolumn_names\x94C\x1a\x80\x04\x95\x0f\x00\x00\x00\x00\x00\x00\x00\x8c\x03src\x94\x8c\x03dst\x94\x86\x94.\x94h\x1e}\x94(\x8c\x0ftype-serialized\x94C=\x80\x04\x952\x00\x00\x00\x00\x00\x00\x00\x8c\x1acudf.core.column.numerical\x94\x8c\x0fNumericalColumn\x94\x93\x94.\x94h)CB\x80\x04\x957\x00\x00\x00\x00\x00\x00\x00\x8c\x05numpy\x94\x8c\x05dtype\x94\x93\x94\x8c\x02i8\x94\x89\x88\x87\x94R\x94(K\x03\x8c\x01<\x94NNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK\x00t\x94b.\x94\x8c\x18dtype-is-cudf-'
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
AttributeError: Can't get attribute 'BulkSampler' on <module '__main__' from '/opt/conda/envs/rapids/bin/dask-scheduler'>
2023-06-26 16:35:48,915 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 16:35:48,916 - distributed.scheduler - INFO - Restarting workers and releasing all keys.
2023-06-26 16:35:48,934 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45970; closing.
2023-06-26 16:35:48,934 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33107', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,935 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45930; closing.
2023-06-26 16:35:48,936 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33491', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33491
2023-06-26 16:35:48,937 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45836; closing.
2023-06-26 16:35:48,937 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33579', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,937 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 16:35:48,937 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45980; closing.
2023-06-26 16:35:48,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33835', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,938 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33835
2023-06-26 16:35:48,938 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45906; closing.
2023-06-26 16:35:48,938 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45828; closing.
2023-06-26 16:35:48,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34511', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,939 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34511
2023-06-26 16:35:48,939 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34651', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,939 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34651
2023-06-26 16:35:48,939 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45866; closing.
2023-06-26 16:35:48,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35233', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,940 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35233
2023-06-26 16:35:48,940 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45990; closing.
2023-06-26 16:35:48,940 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45946; closing.
2023-06-26 16:35:48,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36859', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,940 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36859
2023-06-26 16:35:48,941 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37371', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,941 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37371
2023-06-26 16:35:48,941 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45856; closing.
2023-06-26 16:35:48,941 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38051', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,941 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38051
2023-06-26 16:35:48,942 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45872; closing.
2023-06-26 16:35:48,942 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43639', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,943 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43639
2023-06-26 16:35:48,945 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45914; closing.
2023-06-26 16:35:48,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42623', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42623
2023-06-26 16:35:48,945 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45954; closing.
2023-06-26 16:35:48,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39223', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39223
2023-06-26 16:35:48,946 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45878; closing.
2023-06-26 16:35:48,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42471', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42471
2023-06-26 16:35:48,950 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45894; closing.
2023-06-26 16:35:48,950 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40261', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,950 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40261
2023-06-26 16:35:48,984 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45848; closing.
2023-06-26 16:35:48,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46249', status: closing, memory: 0, processing: 0>
2023-06-26 16:35:48,985 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46249
2023-06-26 16:35:48,985 - distributed.scheduler - INFO - Lost all workers
2023-06-26 16:36:01,698 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41521', status: init, memory: 0, processing: 0>
2023-06-26 16:36:01,699 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41521
2023-06-26 16:36:01,699 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38928
2023-06-26 16:36:03,186 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36435', status: init, memory: 0, processing: 0>
2023-06-26 16:36:03,187 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36435
2023-06-26 16:36:03,187 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38938
2023-06-26 16:36:03,349 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34143', status: init, memory: 0, processing: 0>
2023-06-26 16:36:03,350 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34143
2023-06-26 16:36:03,350 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38946
2023-06-26 16:36:03,550 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36179', status: init, memory: 0, processing: 0>
2023-06-26 16:36:03,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36179
2023-06-26 16:36:03,550 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38950
2023-06-26 16:36:03,569 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38751', status: init, memory: 0, processing: 0>
2023-06-26 16:36:03,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38751
2023-06-26 16:36:03,569 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38964
2023-06-26 16:36:03,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42127', status: init, memory: 0, processing: 0>
2023-06-26 16:36:03,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42127
2023-06-26 16:36:03,657 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38972
2023-06-26 16:36:03,989 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42923', status: init, memory: 0, processing: 0>
2023-06-26 16:36:03,990 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42923
2023-06-26 16:36:03,990 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38986
2023-06-26 16:36:08,452 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37457', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,452 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37457
2023-06-26 16:36:08,453 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39056
2023-06-26 16:36:08,479 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42877', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,479 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42877
2023-06-26 16:36:08,479 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39070
2023-06-26 16:36:08,490 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39203', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,490 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39203
2023-06-26 16:36:08,490 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39080
2023-06-26 16:36:08,494 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38679', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,494 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38679
2023-06-26 16:36:08,494 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39096
2023-06-26 16:36:08,496 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34977', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,496 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34977
2023-06-26 16:36:08,496 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39108
2023-06-26 16:36:08,512 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37391', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,513 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37391
2023-06-26 16:36:08,513 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39120
2023-06-26 16:36:08,516 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44811', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,517 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44811
2023-06-26 16:36:08,517 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39116
2023-06-26 16:36:08,518 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46047', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,518 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46047
2023-06-26 16:36:08,518 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39114
2023-06-26 16:36:08,519 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:32883', status: init, memory: 0, processing: 0>
2023-06-26 16:36:08,519 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:32883
2023-06-26 16:36:08,519 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39136
2023-06-26 16:36:08,629 - distributed.scheduler - INFO - Restarting finished.
2023-06-26 16:36:18,508 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 16:36:44,452 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 16:36:44,453 - distributed.scheduler - INFO - Remove client Client-57fd5a5c-143f-11ee-b7c4-5cff35c1a711
2023-06-26 16:36:44,454 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45656; closing.
2023-06-26 16:36:44,454 - distributed.scheduler - INFO - Remove client Client-57fd5a5c-143f-11ee-b7c4-5cff35c1a711
2023-06-26 16:36:44,454 - distributed.scheduler - INFO - Close client connection: Client-57fd5a5c-143f-11ee-b7c4-5cff35c1a711
2023-06-26 16:37:52,034 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 16:37:52,035 - distributed.core - INFO - Connection to tcp://10.120.104.11:39136 has been closed.
2023-06-26 16:37:52,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:32883', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,036 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32883
2023-06-26 16:37:52,036 - distributed.core - INFO - Connection to tcp://10.120.104.11:39080 has been closed.
2023-06-26 16:37:52,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39203', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,036 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39203
2023-06-26 16:37:52,036 - distributed.core - INFO - Connection to tcp://10.120.104.11:39116 has been closed.
2023-06-26 16:37:52,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44811', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,037 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44811
2023-06-26 16:37:52,037 - distributed.core - INFO - Connection to tcp://10.120.104.11:39114 has been closed.
2023-06-26 16:37:52,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46047', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,037 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46047
2023-06-26 16:37:52,037 - distributed.core - INFO - Connection to tcp://10.120.104.11:38928 has been closed.
2023-06-26 16:37:52,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41521', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,037 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41521
2023-06-26 16:37:52,037 - distributed.core - INFO - Connection to tcp://10.120.104.11:39108 has been closed.
2023-06-26 16:37:52,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34977', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,037 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34977
2023-06-26 16:37:52,037 - distributed.core - INFO - Connection to tcp://10.120.104.11:39070 has been closed.
2023-06-26 16:37:52,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42877', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,038 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42877
2023-06-26 16:37:52,038 - distributed.core - INFO - Connection to tcp://10.120.104.11:39096 has been closed.
2023-06-26 16:37:52,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38679', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,038 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38679
2023-06-26 16:37:52,038 - distributed.core - INFO - Connection to tcp://10.120.104.11:39120 has been closed.
2023-06-26 16:37:52,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37391', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,038 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37391
2023-06-26 16:37:52,038 - distributed.core - INFO - Connection to tcp://10.120.104.11:39056 has been closed.
2023-06-26 16:37:52,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37457', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,038 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37457
2023-06-26 16:37:52,039 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 16:37:52,040 - distributed.core - INFO - Connection to tcp://10.120.104.11:38938 has been closed.
2023-06-26 16:37:52,040 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36435', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,040 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36435
2023-06-26 16:37:52,040 - distributed.core - INFO - Connection to tcp://10.120.104.11:38986 has been closed.
2023-06-26 16:37:52,041 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42923', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,041 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42923
2023-06-26 16:37:52,041 - distributed.core - INFO - Connection to tcp://10.120.104.11:38946 has been closed.
2023-06-26 16:37:52,041 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34143', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,041 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34143
2023-06-26 16:37:52,041 - distributed.core - INFO - Connection to tcp://10.120.104.11:38972 has been closed.
2023-06-26 16:37:52,041 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42127', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,041 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42127
2023-06-26 16:37:52,042 - distributed.core - INFO - Connection to tcp://10.120.104.11:38964 has been closed.
2023-06-26 16:37:52,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38751', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,042 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38751
2023-06-26 16:37:52,042 - distributed.core - INFO - Connection to tcp://10.120.104.11:38950 has been closed.
2023-06-26 16:37:52,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36179', status: running, memory: 0, processing: 0>
2023-06-26 16:37:52,042 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36179
2023-06-26 16:37:52,042 - distributed.scheduler - INFO - Lost all workers
2023-06-26 16:37:52,043 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:38946>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,044 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:38950>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,045 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:38938>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,045 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:38964>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,045 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:38972>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,045 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:38986>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,046 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 16:37:52,050 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 16:37:52,051 - distributed.scheduler - INFO - End scheduler
