RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 16:34:10,279 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:32881'
2023-06-26 16:34:10,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35705'
2023-06-26 16:34:10,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37575'
2023-06-26 16:34:10,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44691'
2023-06-26 16:34:10,290 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36035'
2023-06-26 16:34:10,291 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46085'
2023-06-26 16:34:10,293 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35669'
2023-06-26 16:34:10,296 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33697'
2023-06-26 16:34:10,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45985'
2023-06-26 16:34:10,300 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35977'
2023-06-26 16:34:10,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43879'
2023-06-26 16:34:10,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45745'
2023-06-26 16:34:10,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45511'
2023-06-26 16:34:10,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38213'
2023-06-26 16:34:10,315 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33113'
2023-06-26 16:34:10,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40801'
2023-06-26 16:34:11,840 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:11,840 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:11,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:11,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:11,992 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:11,992 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,003 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,003 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,012 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,013 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,013 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,016 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,018 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,018 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:34:12,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:34:12,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,172 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,180 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,181 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,182 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,188 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,192 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,195 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,210 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,210 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:12,216 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:34:18,406 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38051
2023-06-26 16:34:18,406 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38051
2023-06-26 16:34:18,406 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33337
2023-06-26 16:34:18,406 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,406 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,406 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,406 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vibv228m
2023-06-26 16:34:18,407 - distributed.worker - INFO - Starting Worker plugin PreImport-aaa5ffd1-1424-434d-98c8-77f8e455ab54
2023-06-26 16:34:18,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0bf9474-784f-4899-90a6-ad8bac5ef392
2023-06-26 16:34:18,482 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33579
2023-06-26 16:34:18,483 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33579
2023-06-26 16:34:18,483 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33699
2023-06-26 16:34:18,483 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,483 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,483 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,483 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,483 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hzqkccex
2023-06-26 16:34:18,484 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99b31183-3af1-4ffb-bdb1-76e044534c87
2023-06-26 16:34:18,729 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34651
2023-06-26 16:34:18,730 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34651
2023-06-26 16:34:18,730 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40173
2023-06-26 16:34:18,730 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,730 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,730 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,730 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,730 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o5jgaia0
2023-06-26 16:34:18,730 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c875ae4-c757-476d-aa50-892d8522ee21
2023-06-26 16:34:18,736 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46249
2023-06-26 16:34:18,737 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46249
2023-06-26 16:34:18,737 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37127
2023-06-26 16:34:18,737 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,737 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,737 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,737 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,737 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l9uk2ick
2023-06-26 16:34:18,737 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11479d48-059a-4234-a719-b17f819f1ffd
2023-06-26 16:34:18,749 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35233
2023-06-26 16:34:18,749 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35233
2023-06-26 16:34:18,749 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44523
2023-06-26 16:34:18,749 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,749 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,749 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,750 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,750 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-14dakdp2
2023-06-26 16:34:18,750 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d87852b-d136-4953-bfc6-c0d9a7d8eb94
2023-06-26 16:34:18,801 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33491
2023-06-26 16:34:18,801 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33491
2023-06-26 16:34:18,801 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36265
2023-06-26 16:34:18,801 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,801 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,801 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,801 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gw57w_ot
2023-06-26 16:34:18,802 - distributed.worker - INFO - Starting Worker plugin PreImport-3820e820-2f77-49c1-8266-bde6376221cb
2023-06-26 16:34:18,802 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba0c0ac2-1cef-4a80-a2e1-f20533c83282
2023-06-26 16:34:18,826 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42471
2023-06-26 16:34:18,826 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42471
2023-06-26 16:34:18,826 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34511
2023-06-26 16:34:18,826 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42937
2023-06-26 16:34:18,826 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34511
2023-06-26 16:34:18,826 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,826 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,826 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40597
2023-06-26 16:34:18,826 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,826 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,827 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-buikmkvl
2023-06-26 16:34:18,827 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,827 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wvknpdei
2023-06-26 16:34:18,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e616ec9e-52ac-437e-9b21-891ba8e3c996
2023-06-26 16:34:18,827 - distributed.worker - INFO - Starting Worker plugin PreImport-1b7b6f4f-df09-44d6-b7fe-ca58b60d88bc
2023-06-26 16:34:18,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb550336-7d4f-41a9-a5c3-ffa63c2757ec
2023-06-26 16:34:18,832 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37371
2023-06-26 16:34:18,833 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37371
2023-06-26 16:34:18,832 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40261
2023-06-26 16:34:18,833 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41881
2023-06-26 16:34:18,833 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40261
2023-06-26 16:34:18,833 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,833 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41383
2023-06-26 16:34:18,833 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,833 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,833 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nlfrjdvs
2023-06-26 16:34:18,833 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,833 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ggbh6w_8
2023-06-26 16:34:18,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8c72b66-37b5-4879-8aa3-eb1b40ca6ed6
2023-06-26 16:34:18,833 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5aee4454-e1aa-4dae-96dd-b74f7e1fcf77
2023-06-26 16:34:18,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-794e4128-30c2-4c24-8f5c-72eb3199605f
2023-06-26 16:34:18,834 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43639
2023-06-26 16:34:18,834 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43639
2023-06-26 16:34:18,834 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37723
2023-06-26 16:34:18,834 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,834 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,834 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,834 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,834 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9av79oxa
2023-06-26 16:34:18,835 - distributed.worker - INFO - Starting Worker plugin RMMSetup-03b9d206-a463-4c74-9af0-0e0a8a96d78f
2023-06-26 16:34:18,873 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33107
2023-06-26 16:34:18,873 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33107
2023-06-26 16:34:18,873 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41097
2023-06-26 16:34:18,873 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,873 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,873 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,873 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lib5o9aw
2023-06-26 16:34:18,873 - distributed.worker - INFO - Starting Worker plugin RMMSetup-82d083d1-7c73-482a-a298-d01348ca62b5
2023-06-26 16:34:18,879 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39223
2023-06-26 16:34:18,879 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39223
2023-06-26 16:34:18,879 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38767
2023-06-26 16:34:18,879 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,879 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,879 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,880 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,880 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-645p5agf
2023-06-26 16:34:18,880 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2acbaf9-98d1-4e4e-a7a8-cf4aad989f9b
2023-06-26 16:34:18,967 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33835
2023-06-26 16:34:18,968 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33835
2023-06-26 16:34:18,968 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35589
2023-06-26 16:34:18,968 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,968 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,968 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,968 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,968 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-66ftqkf0
2023-06-26 16:34:18,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8e938ee-c3a6-4c4a-a4c2-6e7c6c5ed481
2023-06-26 16:34:18,986 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36859
2023-06-26 16:34:18,986 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36859
2023-06-26 16:34:18,986 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33683
2023-06-26 16:34:18,986 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,986 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,987 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,987 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,987 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u0yp9w43
2023-06-26 16:34:18,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21247f82-668e-43e0-a947-7ecceee9e8b3
2023-06-26 16:34:18,989 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c9d4d90d-d5ec-46a1-8fd5-be2bde8876f2
2023-06-26 16:34:18,989 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42623
2023-06-26 16:34:18,989 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42623
2023-06-26 16:34:18,989 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45889
2023-06-26 16:34:18,989 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:34:18,989 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:18,989 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:34:18,989 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:34:18,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s9y69mup
2023-06-26 16:34:18,990 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-707c615a-8224-428d-ad56-e68998650fc4
2023-06-26 16:34:18,990 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97cb6147-7461-42be-bddd-31089d44cbac
2023-06-26 16:34:22,335 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78f3853f-568c-46b4-826c-a0431772e4e6
2023-06-26 16:34:22,335 - distributed.worker - INFO - Starting Worker plugin PreImport-c4568e5f-c34b-48db-b299-c5221ee07264
2023-06-26 16:34:22,336 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,357 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,357 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,358 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,398 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25ada19d-06e3-4154-a42d-b0d202beca00
2023-06-26 16:34:22,399 - distributed.worker - INFO - Starting Worker plugin PreImport-0022fe16-f856-463d-8509-31d24428c5dc
2023-06-26 16:34:22,400 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e93f430-1137-4bb7-999a-a757f70c2691
2023-06-26 16:34:22,414 - distributed.worker - INFO - Starting Worker plugin PreImport-c765361c-3172-464e-aa0f-4cc20d01391e
2023-06-26 16:34:22,416 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,421 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,421 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,422 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,438 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6604f2d9-8110-4161-a54a-c9df5cae9da3
2023-06-26 16:34:22,439 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,440 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,440 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,443 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,455 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,455 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,456 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7848f9ae-9839-4997-a568-49f427d8e1eb
2023-06-26 16:34:22,480 - distributed.worker - INFO - Starting Worker plugin PreImport-8df1a2b1-bd08-4b03-af2a-54baf09e5b71
2023-06-26 16:34:22,481 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,499 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,499 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8cf57c40-8637-43a9-9c6f-4c40254a8088
2023-06-26 16:34:22,500 - distributed.worker - INFO - Starting Worker plugin PreImport-9216be84-4117-4b0f-94e2-007f17639fd3
2023-06-26 16:34:22,501 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,501 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,517 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,517 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,518 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de336fca-5668-4b3d-a353-3d48e9beaf77
2023-06-26 16:34:22,560 - distributed.worker - INFO - Starting Worker plugin PreImport-14a04141-2316-4046-ad34-242b5060a304
2023-06-26 16:34:22,562 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,584 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,584 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,587 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,604 - distributed.worker - INFO - Starting Worker plugin PreImport-3edf124f-a9c4-4bd7-967b-8933607ea491
2023-06-26 16:34:22,606 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,626 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,626 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,628 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0edb206c-280e-4bff-a992-99625642d6a9
2023-06-26 16:34:22,649 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,670 - distributed.worker - INFO - Starting Worker plugin PreImport-4ad070f2-cba8-4610-bbae-4c2c9de51b77
2023-06-26 16:34:22,671 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,673 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ec7c180-6cf4-4a70-b2a7-030c6ead05ac
2023-06-26 16:34:22,674 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,675 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,675 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,677 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,686 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,687 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,688 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,698 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,698 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,698 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d461ab1-d6d9-4c00-9a8c-f2a9dc8eb4e2
2023-06-26 16:34:22,699 - distributed.worker - INFO - Starting Worker plugin PreImport-cfaf5a16-2fd3-422e-947c-60bee99ea082
2023-06-26 16:34:22,700 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,700 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,702 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7ee5261-bd91-4021-a14d-bf566dbd08a9
2023-06-26 16:34:22,703 - distributed.worker - INFO - Starting Worker plugin PreImport-b3138e04-d25a-41ad-8527-51bb3dda5090
2023-06-26 16:34:22,703 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,709 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-327ffc46-5299-49dd-ba75-07c1306e5b0b
2023-06-26 16:34:22,710 - distributed.worker - INFO - Starting Worker plugin PreImport-ea311cc1-3263-4e1e-b91d-48880a44015f
2023-06-26 16:34:22,710 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,714 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,714 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,715 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,717 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-444dfe29-9a2d-491c-a1b4-6470afd7181d
2023-06-26 16:34:22,717 - distributed.worker - INFO - Starting Worker plugin PreImport-5be9dd37-f64a-4394-8935-378a06da214b
2023-06-26 16:34:22,718 - distributed.worker - INFO - Starting Worker plugin PreImport-0ad5c135-4207-4f58-9ea1-8938dd468551
2023-06-26 16:34:22,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,719 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,720 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,721 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,722 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,722 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,723 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,733 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,733 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,734 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:22,736 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:34:22,736 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:34:22,739 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:34:38,708 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,708 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,708 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,708 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,708 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,708 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,709 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,709 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,712 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,713 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,713 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,715 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,715 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,716 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,716 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,718 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687797278.728904] [exp01:259447:0]            sock.c:470  UCX  ERROR bind(fd=272 addr=0.0.0.0:37269) failed: Address already in use
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,729 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,729 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:38,729 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,408 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,409 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,409 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,409 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,409 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,409 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,409 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:39,409 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:34:42,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:42,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:34:54,676 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:54,722 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:54,868 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:54,909 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,007 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,027 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,111 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,147 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,278 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,280 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,290 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,292 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,326 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,375 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:34:55,407 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:35:01,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:01,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:01,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:01,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:02,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:40,855 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,858 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,858 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,858 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,858 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,858 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,859 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,860 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,861 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,861 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,861 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,861 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,861 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,862 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,863 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:40,865 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:35:44,016 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,022 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,022 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,023 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,025 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:35:44,467 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-cdabcaaacce5125272196b2efeb78bc9-559b4ff9b0d12e3fed6a115a3988c401', 0)
Function:  subgraph_callable-4084f484-ebae-48b3-a8a4-7e04c696
args:      ("('__filter_batches-8509657c273f004c94ff54b0d7ad903a', 0)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:35:44,468 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-cdabcaaacce5125272196b2efeb78bc9-559b4ff9b0d12e3fed6a115a3988c401', 2)
Function:  subgraph_callable-4084f484-ebae-48b3-a8a4-7e04c696
args:      ("('__filter_batches-8509657c273f004c94ff54b0d7ad903a', 2)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:35:44,468 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-cdabcaaacce5125272196b2efeb78bc9-559b4ff9b0d12e3fed6a115a3988c401', 1)
Function:  subgraph_callable-4084f484-ebae-48b3-a8a4-7e04c696
args:      ("('__filter_batches-8509657c273f004c94ff54b0d7ad903a', 1)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:35:44,468 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-cdabcaaacce5125272196b2efeb78bc9-559b4ff9b0d12e3fed6a115a3988c401', 3)
Function:  subgraph_callable-4084f484-ebae-48b3-a8a4-7e04c696
args:      ("('__filter_batches-8509657c273f004c94ff54b0d7ad903a', 3)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:35:44,474 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,474 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,474 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,474 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:44,479 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:35:48,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:35:48,930 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,933 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33107. Reason: scheduler-restart
2023-06-26 16:35:48,933 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,933 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,934 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33491. Reason: scheduler-restart
2023-06-26 16:35:48,934 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,934 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,934 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33579. Reason: scheduler-restart
2023-06-26 16:35:48,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,935 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,935 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33835. Reason: scheduler-restart
2023-06-26 16:35:48,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,935 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,935 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34511. Reason: scheduler-restart
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34651. Reason: scheduler-restart
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33107
2023-06-26 16:35:48,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,936 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35233. Reason: scheduler-restart
2023-06-26 16:35:48,937 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,937 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,937 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36859. Reason: scheduler-restart
2023-06-26 16:35:48,937 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,937 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37371. Reason: scheduler-restart
2023-06-26 16:35:48,937 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,938 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,938 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38051. Reason: scheduler-restart
2023-06-26 16:35:48,938 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,938 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,938 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,939 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,939 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,939 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,939 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,941 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:35:48,941 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43639. Reason: scheduler-restart
2023-06-26 16:35:48,942 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39223. Reason: scheduler-restart
2023-06-26 16:35:48,942 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42623. Reason: scheduler-restart
2023-06-26 16:35:48,942 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33491
2023-06-26 16:35:48,942 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 16:35:48,942 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33835
2023-06-26 16:35:48,942 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34511
2023-06-26 16:35:48,943 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34651
2023-06-26 16:35:48,943 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35233
2023-06-26 16:35:48,943 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36859
2023-06-26 16:35:48,943 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37371
2023-06-26 16:35:48,943 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38051
2023-06-26 16:35:48,943 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,945 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42471. Reason: scheduler-restart
2023-06-26 16:35:48,945 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33491
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33491
2023-06-26 16:35:48,945 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40261. Reason: scheduler-restart
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33835
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33835
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34511
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34511
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34651
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35233
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34651
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36859
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35233
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37371
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36859
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38051
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37371
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38051
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33491
2023-06-26 16:35:48,945 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 16:35:48,945 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33835
2023-06-26 16:35:48,946 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34511
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34651
2023-06-26 16:35:48,946 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35233
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33491
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36859
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37371
2023-06-26 16:35:48,946 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38051
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33835
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34511
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34651
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35233
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36859
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37371
2023-06-26 16:35:48,946 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38051
2023-06-26 16:35:48,947 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,947 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,953 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,955 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,956 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46249. Reason: scheduler-restart
2023-06-26 16:35:48,956 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,960 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43639
2023-06-26 16:35:48,960 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42623
2023-06-26 16:35:48,960 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39223
2023-06-26 16:35:48,960 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42471
2023-06-26 16:35:48,960 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,963 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,971 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,979 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33491
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33579
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33835
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34511
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34651
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35233
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36859
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37371
2023-06-26 16:35:48,980 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38051
2023-06-26 16:35:48,983 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43639
2023-06-26 16:35:48,983 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42623
2023-06-26 16:35:48,983 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39223
2023-06-26 16:35:48,984 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42471
2023-06-26 16:35:48,984 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40261
2023-06-26 16:35:48,986 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:35:48,988 - distributed.nanny - INFO - Worker closed
2023-06-26 16:35:53,160 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:53,169 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:53,698 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:54,717 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:54,734 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:54,735 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:54,737 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:56,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:56,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:56,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:56,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:56,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:56,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:56,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:56,979 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:56,980 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:56,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:56,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:56,985 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:56,986 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:56,986 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:56,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:56,992 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:56,993 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:56,996 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:56,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:56,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:56,998 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:56,998 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:56,999 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:57,002 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:35:57,135 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:57,161 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:57,165 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:57,167 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:57,236 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:57,241 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:58,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:58,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:58,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,795 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:58,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:58,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:58,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:58,890 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:58,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:58,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:58,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:58,979 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41521
2023-06-26 16:35:58,979 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41521
2023-06-26 16:35:58,979 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40425
2023-06-26 16:35:58,979 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:35:58,979 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:35:58,979 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:35:58,979 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:35:58,979 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7qp0pvsp
2023-06-26 16:35:58,979 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8db9af1d-2faa-4e4d-9e43-1caa6bbfd583
2023-06-26 16:35:58,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,984 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:58,989 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:35:58,989 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:35:59,005 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:59,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:59,030 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:59,085 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:59,167 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:35:59,169 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:36:00,422 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36435
2023-06-26 16:36:00,422 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36435
2023-06-26 16:36:00,422 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39503
2023-06-26 16:36:00,422 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:00,422 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:00,422 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:00,422 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:00,422 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-txaswe2d
2023-06-26 16:36:00,423 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a14621c6-c913-4660-9bf8-a805b43d0c40
2023-06-26 16:36:00,439 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36179
2023-06-26 16:36:00,440 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36179
2023-06-26 16:36:00,440 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42881
2023-06-26 16:36:00,440 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:00,440 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:00,440 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:00,440 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:00,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qv4ormz0
2023-06-26 16:36:00,440 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65d37824-abc0-40bc-8169-3b73c0d93d50
2023-06-26 16:36:00,449 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42127
2023-06-26 16:36:00,449 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42127
2023-06-26 16:36:00,449 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33181
2023-06-26 16:36:00,449 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:00,449 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:00,449 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:00,449 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:00,449 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x5sls0db
2023-06-26 16:36:00,450 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0e2e059-48d2-465b-b744-6c1d89d475a8
2023-06-26 16:36:00,454 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42923
2023-06-26 16:36:00,454 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42923
2023-06-26 16:36:00,454 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41929
2023-06-26 16:36:00,454 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:00,454 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:00,454 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:00,455 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:00,455 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x1fy3rbg
2023-06-26 16:36:00,455 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-642aefbe-e4ac-43c3-9ddd-4df341bcbe4b
2023-06-26 16:36:00,455 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d7bade4-ac91-41c3-bb92-90c96ed1ebcd
2023-06-26 16:36:00,463 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34143
2023-06-26 16:36:00,463 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34143
2023-06-26 16:36:00,463 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40057
2023-06-26 16:36:00,463 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:00,463 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:00,463 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:00,463 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:00,463 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qi51ab7a
2023-06-26 16:36:00,464 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8d4b70c-4e29-49e9-a092-9849768b1d37
2023-06-26 16:36:00,467 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38751
2023-06-26 16:36:00,467 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38751
2023-06-26 16:36:00,467 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39501
2023-06-26 16:36:00,467 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:00,467 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:00,467 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:00,467 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:00,467 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-84c1qjcz
2023-06-26 16:36:00,468 - distributed.worker - INFO - Starting Worker plugin PreImport-bd56863e-643c-42f6-8e1d-6051d9e87fb8
2023-06-26 16:36:00,468 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65c231e7-2787-4864-bfca-0b94399df62d
2023-06-26 16:36:01,679 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-261bf6c2-9ccb-49de-a8ea-0d09973c4f97
2023-06-26 16:36:01,680 - distributed.worker - INFO - Starting Worker plugin PreImport-966a7b50-bd26-4bd5-9d24-3008532ac6b3
2023-06-26 16:36:01,682 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:01,699 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:01,699 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:01,702 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:03,148 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0587c5d-05eb-4a4d-b9cc-3d9e913420fa
2023-06-26 16:36:03,149 - distributed.worker - INFO - Starting Worker plugin PreImport-110fb443-f6df-4f35-9c82-6f7045c812e7
2023-06-26 16:36:03,151 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,187 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:03,187 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,190 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:03,323 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93561f7e-55df-43f4-90a9-77412ef7d3d4
2023-06-26 16:36:03,324 - distributed.worker - INFO - Starting Worker plugin PreImport-3c32c0cf-9407-4ee1-bba0-09595b5fc999
2023-06-26 16:36:03,324 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,350 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:03,350 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,351 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:03,522 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ee9def2-760f-413c-aee9-7e02abd7c6c3
2023-06-26 16:36:03,523 - distributed.worker - INFO - Starting Worker plugin PreImport-0ff05989-81e3-4e47-89f4-da61cc8d512d
2023-06-26 16:36:03,524 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,546 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ca9080a-3654-4a55-8f98-09a080cef05b
2023-06-26 16:36:03,547 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,550 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:03,550 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,552 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:03,569 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:03,569 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,571 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:03,630 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2c50c3c-cf6f-4f9c-8799-040709da074f
2023-06-26 16:36:03,630 - distributed.worker - INFO - Starting Worker plugin PreImport-d466eec0-5acc-40bd-af13-b5609c6c16f9
2023-06-26 16:36:03,631 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,657 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:03,657 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,658 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:03,948 - distributed.worker - INFO - Starting Worker plugin PreImport-a02a3488-b30e-4ffd-b02a-c4628b6c50cb
2023-06-26 16:36:03,952 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,990 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:03,990 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:03,993 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:06,103 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42877
2023-06-26 16:36:06,103 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42877
2023-06-26 16:36:06,103 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37105
2023-06-26 16:36:06,103 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,103 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,103 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,104 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,104 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bhs_jctw
2023-06-26 16:36:06,104 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ad779ab-0375-4806-a72b-d3fea88294f6
2023-06-26 16:36:06,104 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04c2aa0d-c2ec-4bd1-bc96-2798ff1f159e
2023-06-26 16:36:06,110 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39203
2023-06-26 16:36:06,110 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39203
2023-06-26 16:36:06,110 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43935
2023-06-26 16:36:06,111 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,111 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,111 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,111 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,111 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-00tifh1s
2023-06-26 16:36:06,112 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6d3c618-0a24-45d2-8ebb-84b8b3dbe16f
2023-06-26 16:36:06,161 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32883
2023-06-26 16:36:06,161 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32883
2023-06-26 16:36:06,161 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43889
2023-06-26 16:36:06,161 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,161 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,161 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,161 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,161 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yfp_x5gj
2023-06-26 16:36:06,162 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d031e0a-621f-4b05-abc3-8e0f435a1fb3
2023-06-26 16:36:06,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1b80bd0-53f0-4019-9910-804ad182b453
2023-06-26 16:36:06,166 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44811
2023-06-26 16:36:06,166 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44811
2023-06-26 16:36:06,166 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41891
2023-06-26 16:36:06,166 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,166 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,166 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,166 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,166 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xvglebjy
2023-06-26 16:36:06,167 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0995436f-c177-4436-ab05-e4d684a95259
2023-06-26 16:36:06,174 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37457
2023-06-26 16:36:06,175 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37457
2023-06-26 16:36:06,175 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44305
2023-06-26 16:36:06,175 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,175 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,175 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,175 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,175 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mm7_sle4
2023-06-26 16:36:06,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1effb83-f111-4193-b037-8a9756715667
2023-06-26 16:36:06,177 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37391
2023-06-26 16:36:06,177 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37391
2023-06-26 16:36:06,177 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41923
2023-06-26 16:36:06,177 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,177 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,177 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,177 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,177 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-85o2uxyn
2023-06-26 16:36:06,178 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3f2f67d-ee56-42ff-ada9-d1a85b4ff4c7
2023-06-26 16:36:06,181 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46047
2023-06-26 16:36:06,181 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46047
2023-06-26 16:36:06,181 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36741
2023-06-26 16:36:06,181 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,181 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,181 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,181 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,181 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q_zq3wxa
2023-06-26 16:36:06,181 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38679
2023-06-26 16:36:06,182 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38679
2023-06-26 16:36:06,182 - distributed.worker - INFO - Starting Worker plugin PreImport-9c2879c6-3f91-40fd-bbd1-fe3679695b9c
2023-06-26 16:36:06,182 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38557
2023-06-26 16:36:06,182 - distributed.worker - INFO - Starting Worker plugin RMMSetup-274af734-954b-4951-82a2-ce9c1ef93e76
2023-06-26 16:36:06,182 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,182 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,182 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,182 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,182 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s4a_qm4x
2023-06-26 16:36:06,183 - distributed.worker - INFO - Starting Worker plugin RMMSetup-956ed548-6487-4ef6-bb32-ad373f726bc3
2023-06-26 16:36:06,186 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34977
2023-06-26 16:36:06,187 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34977
2023-06-26 16:36:06,187 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39583
2023-06-26 16:36:06,187 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:36:06,187 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:06,187 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:36:06,187 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:36:06,187 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s1ovmpbq
2023-06-26 16:36:06,188 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f46d5e8f-d3a6-43b4-8404-86eb5e96bdf4
2023-06-26 16:36:08,438 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6bfeb72a-9ac9-4363-848a-837494f08351
2023-06-26 16:36:08,438 - distributed.worker - INFO - Starting Worker plugin PreImport-2c71a1b0-275d-4851-acc6-bd9ecea42073
2023-06-26 16:36:08,439 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,453 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,453 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,454 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:08,466 - distributed.worker - INFO - Starting Worker plugin PreImport-93a0caa6-a70d-49fb-96de-1b785757ec73
2023-06-26 16:36:08,467 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,470 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7713884-532e-4ade-99db-47f5613314e4
2023-06-26 16:36:08,471 - distributed.worker - INFO - Starting Worker plugin PreImport-3cdba435-8134-427c-8f0a-d3a1cb3a1d7f
2023-06-26 16:36:08,471 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf65ad5d-5b8f-418f-8d7d-b6841cacc24b
2023-06-26 16:36:08,472 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,472 - distributed.worker - INFO - Starting Worker plugin PreImport-0819da1c-d352-4d6d-bac8-8f113da1279c
2023-06-26 16:36:08,474 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,475 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a02c2f4b-7321-4062-ba07-18efa9953e75
2023-06-26 16:36:08,476 - distributed.worker - INFO - Starting Worker plugin PreImport-998f3fe0-2d1a-4dde-b86f-e867f61589c0
2023-06-26 16:36:08,477 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,479 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,479 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,480 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:08,491 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,491 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,493 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:08,494 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,494 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,496 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:08,497 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,497 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,497 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8950370c-7f9c-4c53-a6fb-ab0f2f21159c
2023-06-26 16:36:08,498 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a4642a9-9a23-4702-ac81-db2b92f0b377
2023-06-26 16:36:08,498 - distributed.worker - INFO - Starting Worker plugin PreImport-edca0d1c-0ba1-47a3-bd85-9b2e2ea1cea8
2023-06-26 16:36:08,498 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:08,499 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,499 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,501 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-36e61c53-e15b-4eb5-b29f-32ab75d48b12
2023-06-26 16:36:08,501 - distributed.worker - INFO - Starting Worker plugin PreImport-016fac7c-10ce-4fd0-ab29-b2ac73623983
2023-06-26 16:36:08,502 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,504 - distributed.worker - INFO - Starting Worker plugin PreImport-581e3953-9894-4043-a513-d1ac2ce9c0ba
2023-06-26 16:36:08,505 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,513 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,513 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,514 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:08,517 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,517 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,518 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,519 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,519 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:36:08,519 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:36:08,520 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:08,521 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:08,521 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:36:17,527 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:17,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,012 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,050 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,074 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,258 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,265 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,312 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,314 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,339 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,367 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,401 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,417 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,461 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,479 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,479 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,480 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:36:18,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:18,492 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,492 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,492 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,492 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,492 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,492 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,492 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,493 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,493 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,493 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,493 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,493 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,493 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,493 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,494 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,495 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687797378.503984] [exp01:262421:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:56976) failed: Address already in use
2023-06-26 16:36:18,503 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,504 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687797378.504131] [exp01:262417:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:54284) failed: Address already in use
2023-06-26 16:36:18,506 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:36:18,515 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,515 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:18,520 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:36:21,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:26,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:31,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,220 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:32,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,027 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,028 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,028 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,028 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,028 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,028 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:36:44,028 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36435. Reason: worker-close
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42923. Reason: worker-close
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32883. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39203. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41521. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44811. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46047. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34143. Reason: worker-close
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42127. Reason: worker-close
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38751. Reason: worker-close
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36179. Reason: worker-close
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34977. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42877. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37391. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,034 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38679. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,035 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37457. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:37:52,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:32881'. Reason: nanny-close
2023-06-26 16:37:52,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,035 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38946 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,035 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38950 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,035 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38972 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,035 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38964 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35705'. Reason: nanny-close
2023-06-26 16:37:52,035 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38938 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,035 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:38986 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:37:52,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37575'. Reason: nanny-close
2023-06-26 16:37:52,038 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,038 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44691'. Reason: nanny-close
2023-06-26 16:37:52,038 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,038 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36035'. Reason: nanny-close
2023-06-26 16:37:52,039 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,039 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46085'. Reason: nanny-close
2023-06-26 16:37:52,039 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,039 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35669'. Reason: nanny-close
2023-06-26 16:37:52,040 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,040 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33697'. Reason: nanny-close
2023-06-26 16:37:52,040 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,040 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45985'. Reason: nanny-close
2023-06-26 16:37:52,040 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,041 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35977'. Reason: nanny-close
2023-06-26 16:37:52,041 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,041 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43879'. Reason: nanny-close
2023-06-26 16:37:52,042 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,042 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45745'. Reason: nanny-close
2023-06-26 16:37:52,042 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,042 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45511'. Reason: nanny-close
2023-06-26 16:37:52,043 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,043 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38213'. Reason: nanny-close
2023-06-26 16:37:52,043 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,043 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33113'. Reason: nanny-close
2023-06-26 16:37:52,043 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,044 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40801'. Reason: nanny-close
2023-06-26 16:37:52,044 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:37:52,052 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46085 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45496 remote=tcp://10.120.104.11:46085>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46085 after 100 s
2023-06-26 16:37:52,053 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38213 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:43130 remote=tcp://10.120.104.11:38213>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38213 after 100 s
2023-06-26 16:37:52,053 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35705 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34320 remote=tcp://10.120.104.11:35705>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35705 after 100 s
2023-06-26 16:37:52,055 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45985 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:58548 remote=tcp://10.120.104.11:45985>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45985 after 100 s
2023-06-26 16:37:52,056 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45511 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39772 remote=tcp://10.120.104.11:45511>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45511 after 100 s
2023-06-26 16:37:52,057 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33113 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:51310 remote=tcp://10.120.104.11:33113>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33113 after 100 s
2023-06-26 16:37:52,057 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40801 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41726 remote=tcp://10.120.104.11:40801>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40801 after 100 s
2023-06-26 16:37:52,057 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37575 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59514 remote=tcp://10.120.104.11:37575>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37575 after 100 s
2023-06-26 16:37:52,059 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45745 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:53224 remote=tcp://10.120.104.11:45745>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45745 after 100 s
2023-06-26 16:37:52,058 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35669 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 747, in _run_callback
    ret = gen.convert_yielded(ret)
  File "/opt/conda/envs/rapids/lib/python3.10/functools.py", line 889, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 878, in convert_yielded
    return _wrap_awaitable(yielded)  # type: ignore
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 615, in ensure_future
    return _ensure_future(coro_or_future, loop=loop)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 636, in _ensure_future
    return loop.create_task(coro_or_future)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 438, in create_task
    task = tasks.Task(coro, loop=self, name=name)
  File "/opt/conda/envs/rapids/lib/python3.10/_weakrefset.py", line 89, in add
    self.data.add(ref(item, self._remove))
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36992 remote=tcp://10.120.104.11:35669>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35669 after 100 s
2023-06-26 16:37:52,060 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33697 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:58818 remote=tcp://10.120.104.11:33697>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33697 after 100 s
2023-06-26 16:37:52,061 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43879 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55148 remote=tcp://10.120.104.11:43879>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43879 after 100 s
2023-06-26 16:37:52,061 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36035 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41466 remote=tcp://10.120.104.11:36035>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36035 after 100 s
2023-06-26 16:37:52,062 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35977 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37688 remote=tcp://10.120.104.11:35977>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35977 after 100 s
2023-06-26 16:37:52,070 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44691 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55214 remote=tcp://10.120.104.11:44691>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44691 after 100 s
2023-06-26 16:37:55,247 - distributed.nanny - WARNING - Worker process still alive after 3.199997711181641 seconds, killing
2023-06-26 16:37:55,247 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:37:55,248 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:37:55,248 - distributed.nanny - WARNING - Worker process still alive after 3.1999963378906253 seconds, killing
2023-06-26 16:37:55,249 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:37:55,249 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:37:55,249 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:37:55,250 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:37:55,250 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:37:55,251 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:37:55,251 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 16:37:55,252 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:37:55,252 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:37:55,253 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:37:55,254 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:37:55,254 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:37:56,042 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,043 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,044 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,044 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,044 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,044 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,044 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,045 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,045 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,045 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,046 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,046 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,046 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,046 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,047 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,047 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262444 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262441 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262438 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262433 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262428 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262425 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262421 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262417 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262412 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262374 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262371 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262368 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262365 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262344 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262326 parent=259368 started daemon>
2023-06-26 16:37:56,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=262323 parent=259368 started daemon>
2023-06-26 16:38:00,165 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 262441 exit status was already read will report exitcode 255
2023-06-26 16:38:00,903 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 262368 exit status was already read will report exitcode 255
2023-06-26 16:38:01,398 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 262344 exit status was already read will report exitcode 255
2023-06-26 16:38:01,631 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 262374 exit status was already read will report exitcode 255
2023-06-26 16:38:02,882 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 262433 exit status was already read will report exitcode 255
