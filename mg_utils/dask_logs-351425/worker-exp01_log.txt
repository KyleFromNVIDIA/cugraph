RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 18:11:08,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44583'
2023-06-26 18:11:08,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42763'
2023-06-26 18:11:08,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40611'
2023-06-26 18:11:08,590 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38205'
2023-06-26 18:11:08,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35013'
2023-06-26 18:11:08,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38159'
2023-06-26 18:11:08,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44507'
2023-06-26 18:11:08,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45615'
2023-06-26 18:11:08,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41235'
2023-06-26 18:11:08,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42443'
2023-06-26 18:11:08,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39455'
2023-06-26 18:11:08,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45745'
2023-06-26 18:11:08,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40547'
2023-06-26 18:11:08,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44591'
2023-06-26 18:11:08,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34465'
2023-06-26 18:11:08,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45121'
2023-06-26 18:11:10,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,190 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,238 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,239 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,340 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:11:10,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:11:10,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,422 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,481 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,481 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,489 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,508 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,513 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,515 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:10,518 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:11:17,632 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35317
2023-06-26 18:11:17,633 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35317
2023-06-26 18:11:17,633 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34001
2023-06-26 18:11:17,633 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,633 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,633 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,633 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,633 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gqjho1nk
2023-06-26 18:11:17,634 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d019fef-bad4-4131-8fb6-566170abce69
2023-06-26 18:11:17,651 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33867
2023-06-26 18:11:17,652 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33867
2023-06-26 18:11:17,652 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37617
2023-06-26 18:11:17,652 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,652 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,652 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,652 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,652 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_8e0dcu_
2023-06-26 18:11:17,652 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45877
2023-06-26 18:11:17,652 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45877
2023-06-26 18:11:17,652 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5fb73d9-8c21-4b26-887a-44ecad1e8d7c
2023-06-26 18:11:17,652 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46847
2023-06-26 18:11:17,652 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,652 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,652 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,653 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jq32tsrw
2023-06-26 18:11:17,653 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c25b023e-b784-4337-9b71-549e5d63d831
2023-06-26 18:11:17,653 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45747
2023-06-26 18:11:17,653 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45747
2023-06-26 18:11:17,653 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36345
2023-06-26 18:11:17,653 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,654 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,654 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-svco0vqj
2023-06-26 18:11:17,655 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ffa2ec7-701d-474e-91f1-09e5bf644ec8
2023-06-26 18:11:17,661 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36923
2023-06-26 18:11:17,662 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36923
2023-06-26 18:11:17,662 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38133
2023-06-26 18:11:17,662 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,662 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,662 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,662 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,662 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-koxmxeqf
2023-06-26 18:11:17,662 - distributed.worker - INFO - Starting Worker plugin PreImport-96662a4c-723d-4f38-bbd8-ab68ab3c5bc8
2023-06-26 18:11:17,662 - distributed.worker - INFO - Starting Worker plugin RMMSetup-227504ca-a257-4547-9916-66264be9819c
2023-06-26 18:11:17,665 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45069
2023-06-26 18:11:17,665 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45069
2023-06-26 18:11:17,665 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41201
2023-06-26 18:11:17,665 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,665 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,666 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,666 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ykeusiu6
2023-06-26 18:11:17,666 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45455
2023-06-26 18:11:17,666 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45455
2023-06-26 18:11:17,666 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb22f601-2457-4863-9691-c6c18c9ac071
2023-06-26 18:11:17,666 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34437
2023-06-26 18:11:17,666 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,666 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,666 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,666 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5hogrgna
2023-06-26 18:11:17,666 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0670da3-aca6-4287-9e1c-7e7e2d771d3b
2023-06-26 18:11:17,667 - distributed.worker - INFO - Starting Worker plugin PreImport-392d5ff6-b412-4ed0-8a58-b2d8b114cffc
2023-06-26 18:11:17,667 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28c205fd-8799-41bb-82b3-c9d4dedaec77
2023-06-26 18:11:17,670 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41175
2023-06-26 18:11:17,670 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41175
2023-06-26 18:11:17,670 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42525
2023-06-26 18:11:17,671 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,671 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,671 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,671 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,671 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f02n6a8g
2023-06-26 18:11:17,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7bc3cf23-ea4c-4a5c-90c8-bdb431c602a1
2023-06-26 18:11:17,682 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44209
2023-06-26 18:11:17,682 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44209
2023-06-26 18:11:17,682 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33065
2023-06-26 18:11:17,682 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,682 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,682 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,682 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,682 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6zl2cihr
2023-06-26 18:11:17,683 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ececed75-91b5-42a2-a548-c1d953ca88d6
2023-06-26 18:11:17,688 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33357
2023-06-26 18:11:17,688 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33357
2023-06-26 18:11:17,688 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38611
2023-06-26 18:11:17,688 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,688 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,688 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,688 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,688 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zbngzl34
2023-06-26 18:11:17,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72a64ca3-526e-4fca-af42-7be34ad83d21
2023-06-26 18:11:17,692 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44593
2023-06-26 18:11:17,692 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44593
2023-06-26 18:11:17,692 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45773
2023-06-26 18:11:17,692 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,692 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,692 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,692 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pqnh0e2u
2023-06-26 18:11:17,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4861cfd2-2f3a-4b06-8f07-99473569d536
2023-06-26 18:11:17,693 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38513
2023-06-26 18:11:17,693 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38513
2023-06-26 18:11:17,693 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38401
2023-06-26 18:11:17,693 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,693 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,693 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,694 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,694 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mawyme1_
2023-06-26 18:11:17,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f97e50bd-6bf7-46f8-be55-ff26e93c302d
2023-06-26 18:11:17,695 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbd01905-9088-4810-9df5-18344308deb5
2023-06-26 18:11:17,696 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42497
2023-06-26 18:11:17,696 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42497
2023-06-26 18:11:17,696 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36427
2023-06-26 18:11:17,696 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,697 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,697 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,697 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,697 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p5niobgn
2023-06-26 18:11:17,697 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb67923d-ca47-430f-82b1-873527af8b76
2023-06-26 18:11:17,708 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38745
2023-06-26 18:11:17,708 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38745
2023-06-26 18:11:17,708 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38831
2023-06-26 18:11:17,708 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,708 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,708 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,708 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wmln5_dv
2023-06-26 18:11:17,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10ce18b8-8288-4235-a374-7dcaa53c7c87
2023-06-26 18:11:17,711 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43051
2023-06-26 18:11:17,711 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43051
2023-06-26 18:11:17,711 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37263
2023-06-26 18:11:17,711 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,711 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,711 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,711 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,711 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2md5awwn
2023-06-26 18:11:17,712 - distributed.worker - INFO - Starting Worker plugin PreImport-91ed444c-4ecd-4b40-bd53-e66f348baaba
2023-06-26 18:11:17,712 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ae5e91d-11f4-419d-af66-9fb1fb646923
2023-06-26 18:11:17,727 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41285
2023-06-26 18:11:17,727 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41285
2023-06-26 18:11:17,727 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39211
2023-06-26 18:11:17,727 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:11:17,727 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:17,727 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:11:17,728 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:11:17,728 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xklb47sg
2023-06-26 18:11:17,728 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e3153f9-b17f-498f-af42-bbc5c1a20266
2023-06-26 18:11:17,729 - distributed.worker - INFO - Starting Worker plugin RMMSetup-36e97e52-bfef-4097-935d-5a4b12b9f9ac
2023-06-26 18:11:21,158 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99bef6e2-88c8-44d7-9d82-14c837a1da60
2023-06-26 18:11:21,159 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,180 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,180 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,181 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,271 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c983721d-e012-4e12-8a82-8faa6bd33809
2023-06-26 18:11:21,272 - distributed.worker - INFO - Starting Worker plugin PreImport-eaeea34f-3440-47a9-9658-12c5ea9c5569
2023-06-26 18:11:21,273 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,296 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,296 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,299 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5571b5b6-9739-4878-966e-662eee87bd9c
2023-06-26 18:11:21,309 - distributed.worker - INFO - Starting Worker plugin PreImport-f88c4fb3-d33b-4584-93c7-c4c874123723
2023-06-26 18:11:21,310 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,327 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,327 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,328 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,391 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db7d16f5-65d8-426c-80c9-9449ecd122ec
2023-06-26 18:11:21,391 - distributed.worker - INFO - Starting Worker plugin PreImport-c8187ef2-269d-4331-8c3c-c063a3f24c24
2023-06-26 18:11:21,392 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,413 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,415 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,455 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23816f97-9d51-40a9-8e7c-4786175813e4
2023-06-26 18:11:21,456 - distributed.worker - INFO - Starting Worker plugin PreImport-b2aad2f6-fcbe-49c3-b689-ade1fe5fd64e
2023-06-26 18:11:21,458 - distributed.worker - INFO - Starting Worker plugin PreImport-47390777-52c7-4dfa-ab47-860afd741481
2023-06-26 18:11:21,459 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,460 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,479 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a2481440-bcd4-4d6e-b2c8-89d2a8dcbc37
2023-06-26 18:11:21,479 - distributed.worker - INFO - Starting Worker plugin PreImport-c741ec45-6524-41c9-81bd-3ef23f6b9252
2023-06-26 18:11:21,483 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,483 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,484 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,485 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,486 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,487 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,489 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,493 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2139c492-6231-437b-a7ae-3d6f8add4b03
2023-06-26 18:11:21,494 - distributed.worker - INFO - Starting Worker plugin PreImport-f6f38004-4792-46ba-bf19-4c31dc954b52
2023-06-26 18:11:21,494 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,503 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3803a753-56ee-431d-8d86-d32dd6d7fe60
2023-06-26 18:11:21,505 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,508 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,508 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,510 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,510 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,510 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,513 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,518 - distributed.worker - INFO - Starting Worker plugin PreImport-3f3222c8-78be-4226-8ee3-bc6d7fb0cb20
2023-06-26 18:11:21,519 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,524 - distributed.worker - INFO - Starting Worker plugin PreImport-d25dbb9c-7488-46f0-ba37-b8570f5202a0
2023-06-26 18:11:21,524 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d41fdf57-e9aa-4be6-8e06-0cd263af079f
2023-06-26 18:11:21,524 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45947bc6-db84-4451-8661-bb46937bb8fc
2023-06-26 18:11:21,525 - distributed.worker - INFO - Starting Worker plugin PreImport-2e927ca3-a1f3-43ea-94d9-fce5e6a0d6f7
2023-06-26 18:11:21,525 - distributed.worker - INFO - Starting Worker plugin PreImport-82da206b-5c4a-45c8-820f-846afdbf7cdf
2023-06-26 18:11:21,525 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,526 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eae78740-2f1a-4844-ae29-b475de784830
2023-06-26 18:11:21,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,527 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,531 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,531 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,534 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-584da471-5266-4fc3-b523-6e8aa9ec0f94
2023-06-26 18:11:21,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f88dd48-4e22-4c3e-810a-989673ac589c
2023-06-26 18:11:21,536 - distributed.worker - INFO - Starting Worker plugin PreImport-a59d998e-44ad-4927-9b86-2a1835fe78f1
2023-06-26 18:11:21,537 - distributed.worker - INFO - Starting Worker plugin PreImport-7af345a1-85ef-4cdb-bc87-8dcd1138aefa
2023-06-26 18:11:21,537 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,539 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,539 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,540 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,541 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,542 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,543 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,543 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,543 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,544 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,545 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,548 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,548 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,548 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,548 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,550 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,550 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,554 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,555 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,558 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:21,561 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:11:21,561 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:11:21,564 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:11:25,411 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,412 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,412 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,412 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,412 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,413 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,413 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,414 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,415 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,415 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,415 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,415 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,415 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,417 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,417 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,417 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,431 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,431 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:25,430 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:26,117 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:11:29,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:40,983 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,192 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,255 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,380 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,381 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,491 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,542 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,545 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,551 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,552 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,565 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,621 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,648 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,747 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:41,810 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:42,400 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:11:48,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:11:48,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:28,230 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,232 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,232 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,233 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,238 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,239 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,239 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,239 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,240 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,240 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,240 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,240 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,240 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,240 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,240 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:28,240 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 18:12:31,341 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,347 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,347 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,347 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,347 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,347 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,347 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,348 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,348 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,348 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,348 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,348 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,348 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,348 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,348 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:31,349 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:12:38,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:12:38,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,714 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,728 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,729 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,729 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:07,744 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:13:12,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:12,291 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,294 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33357. Reason: scheduler-restart
2023-06-26 18:13:12,294 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,294 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,294 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,295 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33867. Reason: scheduler-restart
2023-06-26 18:13:12,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,295 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35317. Reason: scheduler-restart
2023-06-26 18:13:12,295 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,296 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,296 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36923. Reason: scheduler-restart
2023-06-26 18:13:12,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,296 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,296 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38513. Reason: scheduler-restart
2023-06-26 18:13:12,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,296 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,296 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,296 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,296 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,296 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,296 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,296 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,296 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,297 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,297 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,297 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,297 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,297 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,297 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33357
2023-06-26 18:13:12,297 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,297 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38745. Reason: scheduler-restart
2023-06-26 18:13:12,297 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41175. Reason: scheduler-restart
2023-06-26 18:13:12,297 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,297 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,297 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41285. Reason: scheduler-restart
2023-06-26 18:13:12,297 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,298 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,298 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42497. Reason: scheduler-restart
2023-06-26 18:13:12,298 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,298 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,298 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,298 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,298 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,299 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43051. Reason: scheduler-restart
2023-06-26 18:13:12,299 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,299 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,299 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,299 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,300 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,300 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,300 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,300 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,301 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,301 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,301 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,301 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:13:12,303 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44209. Reason: scheduler-restart
2023-06-26 18:13:12,306 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,307 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44593. Reason: scheduler-restart
2023-06-26 18:13:12,307 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33867
2023-06-26 18:13:12,307 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45455. Reason: scheduler-restart
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35317
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36923
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38513
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33867
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38745
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33867
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35317
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41175
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35317
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36923
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36923
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38513
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41285
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38513
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38745
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38745
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41175
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42497
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41175
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41285
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41285
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42497
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43051
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42497
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43051
2023-06-26 18:13:12,308 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43051
2023-06-26 18:13:12,309 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,315 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45877. Reason: scheduler-restart
2023-06-26 18:13:12,316 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45747. Reason: scheduler-restart
2023-06-26 18:13:12,317 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33867
2023-06-26 18:13:12,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35317
2023-06-26 18:13:12,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36923
2023-06-26 18:13:12,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38513
2023-06-26 18:13:12,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38745
2023-06-26 18:13:12,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41175
2023-06-26 18:13:12,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41285
2023-06-26 18:13:12,317 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42497
2023-06-26 18:13:12,318 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43051
2023-06-26 18:13:12,318 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44593
2023-06-26 18:13:12,318 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33867
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35317
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36923
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38513
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38745
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41175
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41285
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42497
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43051
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44593
2023-06-26 18:13:12,319 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44593
2023-06-26 18:13:12,320 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45069. Reason: scheduler-restart
2023-06-26 18:13:12,320 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44209
2023-06-26 18:13:12,320 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44209
2023-06-26 18:13:12,320 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,320 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,320 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33867
2023-06-26 18:13:12,320 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35317
2023-06-26 18:13:12,320 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36923
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38513
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38745
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41175
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41285
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42497
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43051
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44209
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44593
2023-06-26 18:13:12,321 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44209
2023-06-26 18:13:12,322 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45455
2023-06-26 18:13:12,322 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,322 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb400, tag: 0x90359151da9320ea>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb400, tag: 0x90359151da9320ea>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb080, tag: 0x56d106e7b28060b2>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb080, tag: 0x56d106e7b28060b2>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb380, tag: 0x59a183284023493d>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb380, tag: 0x59a183284023493d>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb100, tag: 0x52dbf5e29c4207e9>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb100, tag: 0x52dbf5e29c4207e9>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb440, tag: 0xf89520303b109fdd>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb440, tag: 0xf89520303b109fdd>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb240, tag: 0xfdf438ca4d4819d5>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb240, tag: 0xfdf438ca4d4819d5>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb2c0, tag: 0x3f8bbb6ff04a9935>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb2c0, tag: 0x3f8bbb6ff04a9935>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb340, tag: 0x45bd88863c4cc66b>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb340, tag: 0x45bd88863c4cc66b>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f87106bb5c0, tag: 0xc3d31795fc218f34>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f87106bb5c0, tag: 0xc3d31795fc218f34>: 
2023-06-26 18:13:12,325 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,331 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45455
2023-06-26 18:13:12,332 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,333 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,333 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,336 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44593
2023-06-26 18:13:12,336 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://10.120.104.11:44209'. Shutting down.
2023-06-26 18:13:12,337 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:13:12,338 - distributed.nanny - INFO - Worker closed
2023-06-26 18:13:12,339 - distributed.nanny - INFO - Worker closed
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-14655' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
2023-06-26 18:13:15,194 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:15,500 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:15,787 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:16,024 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:16,467 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:18,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:18,322 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:18,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:18,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:18,552 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:18,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:18,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:18,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:18,560 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,561 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:18,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:18,572 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,573 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,576 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,578 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,583 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,584 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,586 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,596 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:13:18,715 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:18,742 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:18,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:18,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,090 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42085
2023-06-26 18:13:20,091 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42085
2023-06-26 18:13:20,091 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38219
2023-06-26 18:13:20,091 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:20,091 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:20,091 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:20,091 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:20,091 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5hvlexef
2023-06-26 18:13:20,092 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8bca076-c6e5-43c4-ab9d-18d8b92c87bf
2023-06-26 18:13:20,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,201 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,227 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,227 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,275 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,275 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,314 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:13:20,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:13:20,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,379 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,451 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:20,511 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:13:21,058 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44675
2023-06-26 18:13:21,058 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44675
2023-06-26 18:13:21,058 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34367
2023-06-26 18:13:21,058 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:21,058 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:21,059 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:21,059 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:21,059 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1idt2af4
2023-06-26 18:13:21,059 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c31c06b4-e74a-4b24-b742-d9019b29ddde
2023-06-26 18:13:21,110 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36677
2023-06-26 18:13:21,110 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36677
2023-06-26 18:13:21,110 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42421
2023-06-26 18:13:21,110 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:21,110 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:21,110 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:21,110 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:21,110 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-crs92ypl
2023-06-26 18:13:21,111 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f5beca3-840a-4114-af0f-18e604b275a6
2023-06-26 18:13:21,179 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39267
2023-06-26 18:13:21,180 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39267
2023-06-26 18:13:21,180 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45603
2023-06-26 18:13:21,180 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:21,180 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:21,180 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:21,180 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:21,180 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-czd_jhp0
2023-06-26 18:13:21,181 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0cab32b-9032-4093-b59f-c934c541a380
2023-06-26 18:13:21,181 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebad02a9-26c1-4d3c-8352-641fbf9004e7
2023-06-26 18:13:21,233 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33091
2023-06-26 18:13:21,233 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33091
2023-06-26 18:13:21,233 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40649
2023-06-26 18:13:21,233 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:21,234 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:21,234 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:21,234 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:21,234 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-shox3wvs
2023-06-26 18:13:21,234 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b262d92-c22a-4ac0-a2ed-1247dcca864f
2023-06-26 18:13:21,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3626c485-9cbc-4155-bed2-5fe98bc582a5
2023-06-26 18:13:21,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4f013af-9d5b-4c8a-b049-27f4583de2b6
2023-06-26 18:13:21,843 - distributed.worker - INFO - Starting Worker plugin PreImport-d4d1f7ce-33de-4f90-b904-a2e2c99705ec
2023-06-26 18:13:21,844 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:21,863 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:21,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:21,866 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:23,402 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-464b094f-e4d0-4413-ab81-5456c28fba93
2023-06-26 18:13:23,402 - distributed.worker - INFO - Starting Worker plugin PreImport-bc1aab5c-dbb8-41c0-91e6-9962f8fcdbf4
2023-06-26 18:13:23,403 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:23,414 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:23,414 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:23,415 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:23,477 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3897496b-63a2-40b4-9480-c404523c78dd
2023-06-26 18:13:23,478 - distributed.worker - INFO - Starting Worker plugin PreImport-eb48594f-cd87-4bb0-b2dc-ef1fb7a8f580
2023-06-26 18:13:23,479 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:23,491 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:23,491 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:23,493 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:23,643 - distributed.worker - INFO - Starting Worker plugin PreImport-d227db2c-e8c6-4e37-852c-a5116302a1aa
2023-06-26 18:13:23,644 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:23,653 - distributed.worker - INFO - Starting Worker plugin PreImport-6e77cba5-e48d-4841-b57c-c92de6f96ea3
2023-06-26 18:13:23,655 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:23,655 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:23,655 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:23,656 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:23,686 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:23,686 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:23,689 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:25,853 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43617
2023-06-26 18:13:25,854 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43617
2023-06-26 18:13:25,854 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41717
2023-06-26 18:13:25,854 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:25,854 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:25,854 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:25,854 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:25,854 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q_lsyu4e
2023-06-26 18:13:25,855 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d39008bd-cc2a-4d33-8001-d3714ef74353
2023-06-26 18:13:26,789 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36395
2023-06-26 18:13:26,789 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36395
2023-06-26 18:13:26,789 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45191
2023-06-26 18:13:26,789 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:26,789 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:26,789 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:26,789 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:26,789 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5vmq3kfv
2023-06-26 18:13:26,790 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d03acf32-b59f-4a73-9ca5-0dceb0539594
2023-06-26 18:13:26,791 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45543
2023-06-26 18:13:26,791 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45543
2023-06-26 18:13:26,791 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37059
2023-06-26 18:13:26,791 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:26,791 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:26,791 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:26,791 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:26,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ovhbeww
2023-06-26 18:13:26,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bed77e4f-282a-42dc-ae10-bcee44d0929c
2023-06-26 18:13:26,798 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37053
2023-06-26 18:13:26,798 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37053
2023-06-26 18:13:26,798 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37939
2023-06-26 18:13:26,798 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:26,798 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:26,798 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:26,798 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:26,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_tvwqx7d
2023-06-26 18:13:26,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30e0ef7d-35e6-49c3-b6c7-fe45044793ed
2023-06-26 18:13:26,859 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38633
2023-06-26 18:13:26,859 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38633
2023-06-26 18:13:26,859 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39649
2023-06-26 18:13:26,859 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:26,859 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:26,860 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:26,860 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:26,860 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uy9vu6ay
2023-06-26 18:13:26,860 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da32d898-2456-4381-8fc8-66b3956b87c3
2023-06-26 18:13:27,068 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42129
2023-06-26 18:13:27,068 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42129
2023-06-26 18:13:27,068 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40069
2023-06-26 18:13:27,068 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:27,068 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:27,068 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:27,068 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:27,068 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x28dbcxo
2023-06-26 18:13:27,069 - distributed.worker - INFO - Starting Worker plugin PreImport-bc71aa1f-a504-4cec-a709-fc5d32c0186a
2023-06-26 18:13:27,069 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53b40e57-c0b2-468f-b74d-4bbc7236967f
2023-06-26 18:13:27,098 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45587
2023-06-26 18:13:27,098 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45587
2023-06-26 18:13:27,098 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40137
2023-06-26 18:13:27,098 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:27,098 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:27,098 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:27,098 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:27,098 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7j57htpv
2023-06-26 18:13:27,099 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afe1774d-d618-4009-91ba-ef26b4508fc9
2023-06-26 18:13:27,135 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43417
2023-06-26 18:13:27,136 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43417
2023-06-26 18:13:27,136 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37837
2023-06-26 18:13:27,136 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:27,136 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:27,136 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:27,136 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:27,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3bg2dnr3
2023-06-26 18:13:27,136 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d43b2cd-9ba2-4be7-8754-4b8e4c6c52c3
2023-06-26 18:13:27,146 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34921
2023-06-26 18:13:27,147 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34921
2023-06-26 18:13:27,147 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45307
2023-06-26 18:13:27,147 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:27,147 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:27,147 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:27,147 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42681
2023-06-26 18:13:27,147 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42681
2023-06-26 18:13:27,147 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:27,147 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35263
2023-06-26 18:13:27,147 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4q_v5awl
2023-06-26 18:13:27,147 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:27,147 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:27,147 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:27,147 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:27,147 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ag3pbiju
2023-06-26 18:13:27,148 - distributed.worker - INFO - Starting Worker plugin PreImport-1e034409-1269-45cf-a01a-49226b4b8455
2023-06-26 18:13:27,148 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4aa2d56f-5f41-4ba8-9ed7-e2a31f23d967
2023-06-26 18:13:27,148 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb6587c6-034c-477c-af99-0accf71c7e82
2023-06-26 18:13:27,148 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0daa529-c596-4de8-b4ea-bae6628f6b2e
2023-06-26 18:13:27,161 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45155
2023-06-26 18:13:27,161 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45155
2023-06-26 18:13:27,161 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34587
2023-06-26 18:13:27,161 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:13:27,161 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:27,161 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:13:27,161 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:13:27,161 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pct8_wh3
2023-06-26 18:13:27,162 - distributed.worker - INFO - Starting Worker plugin PreImport-0bd7c537-6966-41f2-a508-b737e6e9dfd7
2023-06-26 18:13:27,162 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7867032b-5ba3-479c-b1f7-c0d0b972025f
2023-06-26 18:13:27,749 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd2d1dd7-151a-444a-91d6-a9ba99f40eda
2023-06-26 18:13:27,750 - distributed.worker - INFO - Starting Worker plugin PreImport-cd341516-0d49-4e04-8d2a-1b0e5ca099cf
2023-06-26 18:13:27,751 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:27,767 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:27,767 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:27,774 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,372 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f136bc58-d883-4e21-863c-cc7c8625924c
2023-06-26 18:13:29,372 - distributed.worker - INFO - Starting Worker plugin PreImport-95d68f27-7af4-4f66-b1f2-20b5b6ed97fe
2023-06-26 18:13:29,373 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,388 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,388 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,389 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,481 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d08b850-bae9-4b4e-886e-65a41d684df8
2023-06-26 18:13:29,482 - distributed.worker - INFO - Starting Worker plugin PreImport-cadc1236-002c-4d5f-a47e-24b2e2105ed4
2023-06-26 18:13:29,483 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,504 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,504 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,507 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c5788f5-78d6-4e26-a8ef-dc0014b5c5b1
2023-06-26 18:13:29,549 - distributed.worker - INFO - Starting Worker plugin PreImport-d33266d1-dfa2-484f-aee7-bfaa84ac9c59
2023-06-26 18:13:29,549 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,563 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,563 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,564 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,577 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f24b9776-16ca-4d66-b1ff-d9adebda14af
2023-06-26 18:13:29,578 - distributed.worker - INFO - Starting Worker plugin PreImport-50f5a204-dc88-41b0-9c75-839e291ea20f
2023-06-26 18:13:29,579 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,585 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f14580f-f4d1-42cd-906f-9290f6168ff8
2023-06-26 18:13:29,586 - distributed.worker - INFO - Starting Worker plugin PreImport-627f8dd0-1ee9-4a21-9b6a-a70b83b7cfc3
2023-06-26 18:13:29,586 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ee2da3e-2a06-4480-b338-ee50f2269432
2023-06-26 18:13:29,586 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,587 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,599 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,599 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,600 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,600 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,601 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,601 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,601 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,602 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,602 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,606 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47230736-7aa7-484d-af34-0e9e3f9b72c3
2023-06-26 18:13:29,608 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,614 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7304ee6-979c-4b34-9f93-4ec4a3114a09
2023-06-26 18:13:29,615 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,626 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,627 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,627 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,627 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,629 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,629 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,635 - distributed.worker - INFO - Starting Worker plugin PreImport-1e599efc-59a1-4c6e-a530-e8cd228703eb
2023-06-26 18:13:29,636 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,638 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc4ad45e-e6bf-4fe5-aa56-f729c0a00660
2023-06-26 18:13:29,639 - distributed.worker - INFO - Starting Worker plugin PreImport-7cf28a93-9535-4c94-89c7-8bfe31652840
2023-06-26 18:13:29,641 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,652 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,652 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,654 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:29,661 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:13:29,661 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:13:29,663 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:13:38,863 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:38,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,008 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,138 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,222 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,235 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,367 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,446 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,450 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,469 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,514 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,588 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,627 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,656 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,703 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,742 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,750 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:13:39,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,762 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,763 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,763 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:13:39,771 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,772 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,774 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:13:39,783 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,783 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,783 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,783 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:39,784 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:13:42,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:42,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:48,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:48,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:49,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:53,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:53,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:53,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:53,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:53,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:13:53,886 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:13:53,887 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,757 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,758 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,758 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,758 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,758 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:14:05,758 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43617. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36395. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43417. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42085. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45155. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45543. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39267. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34921. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45587. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33091. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36677. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44675. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37053. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42681. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38633. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42129. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:17:02,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44583'. Reason: nanny-close
2023-06-26 18:17:02,881 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42763'. Reason: nanny-close
2023-06-26 18:17:02,883 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,883 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40611'. Reason: nanny-close
2023-06-26 18:17:02,884 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,884 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38205'. Reason: nanny-close
2023-06-26 18:17:02,884 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,884 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35013'. Reason: nanny-close
2023-06-26 18:17:02,885 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,885 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38159'. Reason: nanny-close
2023-06-26 18:17:02,886 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,886 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44507'. Reason: nanny-close
2023-06-26 18:17:02,886 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,887 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45615'. Reason: nanny-close
2023-06-26 18:17:02,887 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,887 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41235'. Reason: nanny-close
2023-06-26 18:17:02,887 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42443'. Reason: nanny-close
2023-06-26 18:17:02,888 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39455'. Reason: nanny-close
2023-06-26 18:17:02,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,889 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45745'. Reason: nanny-close
2023-06-26 18:17:02,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,889 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40547'. Reason: nanny-close
2023-06-26 18:17:02,890 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44591'. Reason: nanny-close
2023-06-26 18:17:02,890 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34465'. Reason: nanny-close
2023-06-26 18:17:02,891 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,891 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45121'. Reason: nanny-close
2023-06-26 18:17:02,891 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:17:02,902 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40611 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:38910 remote=tcp://10.120.104.11:40611>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40611 after 100 s
2023-06-26 18:17:02,905 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42763 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:44216 remote=tcp://10.120.104.11:42763>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42763 after 100 s
2023-06-26 18:17:02,905 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38205 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36550 remote=tcp://10.120.104.11:38205>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38205 after 100 s
2023-06-26 18:17:02,906 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35013 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:57474 remote=tcp://10.120.104.11:35013>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35013 after 100 s
2023-06-26 18:17:02,907 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39455 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39418 remote=tcp://10.120.104.11:39455>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39455 after 100 s
2023-06-26 18:17:02,908 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45615 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39498 remote=tcp://10.120.104.11:45615>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45615 after 100 s
2023-06-26 18:17:02,909 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44507 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:57738 remote=tcp://10.120.104.11:44507>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44507 after 100 s
2023-06-26 18:17:02,909 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34465 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33150 remote=tcp://10.120.104.11:34465>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34465 after 100 s
2023-06-26 18:17:02,909 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38159 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50412 remote=tcp://10.120.104.11:38159>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38159 after 100 s
2023-06-26 18:17:02,909 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41235 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52448 remote=tcp://10.120.104.11:41235>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41235 after 100 s
2023-06-26 18:17:02,910 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42443 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55908 remote=tcp://10.120.104.11:42443>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42443 after 100 s
2023-06-26 18:17:02,911 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44591 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36488 remote=tcp://10.120.104.11:44591>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44591 after 100 s
2023-06-26 18:17:02,911 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40547 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34010 remote=tcp://10.120.104.11:40547>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40547 after 100 s
2023-06-26 18:17:02,911 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45121 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47682 remote=tcp://10.120.104.11:45121>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45121 after 100 s
2023-06-26 18:17:02,912 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45745 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48062 remote=tcp://10.120.104.11:45745>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45745 after 100 s
2023-06-26 18:17:06,101 - distributed.nanny - WARNING - Worker process still alive after 3.199997711181641 seconds, killing
2023-06-26 18:17:06,101 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-26 18:17:06,101 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:17:06,102 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:17:06,102 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:17:06,103 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:17:06,104 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:17:06,104 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:17:06,105 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:17:06,105 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:17:06,106 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:17:06,106 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:17:06,106 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:17:06,106 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:17:06,108 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:17:06,108 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:17:06,882 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,884 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,884 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,885 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,885 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,886 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,887 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,887 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,887 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,889 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,889 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,889 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,890 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,891 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,891 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,892 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:17:06,893 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354938 parent=351576 started daemon>
2023-06-26 18:17:06,893 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354935 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354932 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354929 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354926 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354923 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354920 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354917 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354912 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354909 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354894 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354868 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354851 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354847 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354843 parent=351576 started daemon>
2023-06-26 18:17:06,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=354840 parent=351576 started daemon>
2023-06-26 18:17:10,376 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 354909 exit status was already read will report exitcode 255
2023-06-26 18:17:11,697 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 354926 exit status was already read will report exitcode 255
2023-06-26 18:17:12,683 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 354851 exit status was already read will report exitcode 255
2023-06-26 18:17:13,674 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 354912 exit status was already read will report exitcode 255
