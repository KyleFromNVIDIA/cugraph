RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 20:26:10,552 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 20:26:11,062 - distributed.scheduler - INFO - State start
2023-06-26 20:26:11,063 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-r55v8kl_', purging
2023-06-26 20:26:11,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5ogwp1vf', purging
2023-06-26 20:26:11,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-h1roj8h3', purging
2023-06-26 20:26:11,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-aqkn0mmx', purging
2023-06-26 20:26:11,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-v2p_pbed', purging
2023-06-26 20:26:11,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_l7hdin4', purging
2023-06-26 20:26:11,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gad983f7', purging
2023-06-26 20:26:11,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-y7o37rjg', purging
2023-06-26 20:26:11,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1p9uia_r', purging
2023-06-26 20:26:11,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7a00ppw8', purging
2023-06-26 20:26:11,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-34ir4xiy', purging
2023-06-26 20:26:11,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-f8shw1us', purging
2023-06-26 20:26:11,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jeminlbw', purging
2023-06-26 20:26:11,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rflbypwo', purging
2023-06-26 20:26:11,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1ci7hbbn', purging
2023-06-26 20:26:11,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-89erdmfx', purging
2023-06-26 20:26:11,078 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 20:26:11,079 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 20:26:11,079 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 20:26:27,612 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39769', status: init, memory: 0, processing: 0>
2023-06-26 20:26:27,615 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39769
2023-06-26 20:26:27,615 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:45672
2023-06-26 20:26:29,391 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41129', status: init, memory: 0, processing: 0>
2023-06-26 20:26:29,392 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41129
2023-06-26 20:26:29,392 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38216
2023-06-26 20:26:29,525 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41765', status: init, memory: 0, processing: 0>
2023-06-26 20:26:29,525 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41765
2023-06-26 20:26:29,525 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38232
2023-06-26 20:26:29,650 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39375', status: init, memory: 0, processing: 0>
2023-06-26 20:26:29,651 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39375
2023-06-26 20:26:29,651 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38244
2023-06-26 20:26:30,119 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33817', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,119 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33817
2023-06-26 20:26:30,120 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38254
2023-06-26 20:26:30,234 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34127', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,235 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34127
2023-06-26 20:26:30,235 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38260
2023-06-26 20:26:30,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44909', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,247 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44909
2023-06-26 20:26:30,247 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38272
2023-06-26 20:26:30,265 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33713', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,265 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33713
2023-06-26 20:26:30,265 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38276
2023-06-26 20:26:30,283 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42301', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,283 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42301
2023-06-26 20:26:30,283 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38292
2023-06-26 20:26:30,289 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44225', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,289 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44225
2023-06-26 20:26:30,289 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38296
2023-06-26 20:26:30,308 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38003', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,308 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38003
2023-06-26 20:26:30,308 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38298
2023-06-26 20:26:30,320 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39569', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,321 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39569
2023-06-26 20:26:30,321 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38300
2023-06-26 20:26:30,323 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44415', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44415
2023-06-26 20:26:30,324 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38322
2023-06-26 20:26:30,329 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40825', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,329 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40825
2023-06-26 20:26:30,329 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38328
2023-06-26 20:26:30,330 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39781', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,331 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39781
2023-06-26 20:26:30,331 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38306
2023-06-26 20:26:30,344 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37083', status: init, memory: 0, processing: 0>
2023-06-26 20:26:30,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37083
2023-06-26 20:26:30,344 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:38332
2023-06-26 20:26:48,533 - distributed.scheduler - INFO - Receive client connection: Client-c6d32904-145f-11ee-b7dc-5cff35c1a711
2023-06-26 20:26:48,534 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:33566
2023-06-26 20:26:49,250 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 20:27:40,311 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,881 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 20:28:31,883 - distributed.scheduler - INFO - Restarting workers and releasing all keys.
2023-06-26 20:28:31,905 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38276; closing.
2023-06-26 20:28:31,906 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33713', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,906 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,907 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38254; closing.
2023-06-26 20:28:31,908 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38260; closing.
2023-06-26 20:28:31,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33817', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33817
2023-06-26 20:28:31,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34127', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34127
2023-06-26 20:28:31,909 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38332; closing.
2023-06-26 20:28:31,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37083', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,910 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37083
2023-06-26 20:28:31,910 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38298; closing.
2023-06-26 20:28:31,910 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38244; closing.
2023-06-26 20:28:31,910 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38003', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,910 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38003
2023-06-26 20:28:31,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39375', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,911 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39375
2023-06-26 20:28:31,911 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:45672; closing.
2023-06-26 20:28:31,911 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38300; closing.
2023-06-26 20:28:31,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39769', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,911 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39769
2023-06-26 20:28:31,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39569', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,912 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39569
2023-06-26 20:28:31,912 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38306; closing.
2023-06-26 20:28:31,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39781', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,912 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39781
2023-06-26 20:28:31,913 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38328; closing.
2023-06-26 20:28:31,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40825', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,913 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40825
2023-06-26 20:28:31,915 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38216; closing.
2023-06-26 20:28:31,915 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41129', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,915 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41129
2023-06-26 20:28:31,920 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38292; closing.
2023-06-26 20:28:31,920 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42301', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42301
2023-06-26 20:28:31,920 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38232; closing.
2023-06-26 20:28:31,921 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41765', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,921 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41765
2023-06-26 20:28:31,932 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38272; closing.
2023-06-26 20:28:31,933 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44909', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,933 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44909
2023-06-26 20:28:31,934 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38296; closing.
2023-06-26 20:28:31,934 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44225', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44225
2023-06-26 20:28:31,940 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:38322; closing.
2023-06-26 20:28:31,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44415', status: closing, memory: 0, processing: 0>
2023-06-26 20:28:31,940 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44415
2023-06-26 20:28:31,940 - distributed.scheduler - INFO - Lost all workers
2023-06-26 20:28:41,653 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41261', status: init, memory: 0, processing: 0>
2023-06-26 20:28:41,653 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41261
2023-06-26 20:28:41,653 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58326
2023-06-26 20:28:43,053 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34953', status: init, memory: 0, processing: 0>
2023-06-26 20:28:43,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34953
2023-06-26 20:28:43,053 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58342
2023-06-26 20:28:43,059 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44649', status: init, memory: 0, processing: 0>
2023-06-26 20:28:43,060 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44649
2023-06-26 20:28:43,060 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58348
2023-06-26 20:28:43,335 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46115', status: init, memory: 0, processing: 0>
2023-06-26 20:28:43,335 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46115
2023-06-26 20:28:43,335 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58354
2023-06-26 20:28:43,406 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42039', status: init, memory: 0, processing: 0>
2023-06-26 20:28:43,407 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42039
2023-06-26 20:28:43,407 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58362
2023-06-26 20:28:49,746 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33067', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,747 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33067
2023-06-26 20:28:49,747 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52524
2023-06-26 20:28:49,758 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44335', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,758 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44335
2023-06-26 20:28:49,758 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52536
2023-06-26 20:28:49,765 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35345', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,766 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35345
2023-06-26 20:28:49,766 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52552
2023-06-26 20:28:49,776 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36679', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,777 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36679
2023-06-26 20:28:49,777 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52580
2023-06-26 20:28:49,777 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37793', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,778 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37793
2023-06-26 20:28:49,778 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52568
2023-06-26 20:28:49,785 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34409', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,786 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34409
2023-06-26 20:28:49,786 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52584
2023-06-26 20:28:49,795 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43189', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,796 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43189
2023-06-26 20:28:49,796 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52592
2023-06-26 20:28:49,826 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:32855', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,826 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:32855
2023-06-26 20:28:49,826 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52602
2023-06-26 20:28:49,827 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42299', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,827 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42299
2023-06-26 20:28:49,827 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52620
2023-06-26 20:28:49,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37141', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,829 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37141
2023-06-26 20:28:49,829 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52604
2023-06-26 20:28:49,835 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36863', status: init, memory: 0, processing: 0>
2023-06-26 20:28:49,836 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36863
2023-06-26 20:28:49,836 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52636
2023-06-26 20:28:49,946 - distributed.scheduler - INFO - Restarting finished.
2023-06-26 20:28:59,970 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 20:29:23,217 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 20:29:23,218 - distributed.scheduler - INFO - Remove client Client-c6d32904-145f-11ee-b7dc-5cff35c1a711
2023-06-26 20:29:23,218 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:33566; closing.
2023-06-26 20:29:23,218 - distributed.scheduler - INFO - Remove client Client-c6d32904-145f-11ee-b7dc-5cff35c1a711
2023-06-26 20:29:23,219 - distributed.scheduler - INFO - Close client connection: Client-c6d32904-145f-11ee-b7dc-5cff35c1a711
2023-06-26 20:37:45,098 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 20:37:45,099 - distributed.core - INFO - Connection to tcp://10.120.104.11:58342 has been closed.
2023-06-26 20:37:45,099 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34953', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,100 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34953
2023-06-26 20:37:45,100 - distributed.core - INFO - Connection to tcp://10.120.104.11:52536 has been closed.
2023-06-26 20:37:45,100 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44335', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,100 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44335
2023-06-26 20:37:45,101 - distributed.core - INFO - Connection to tcp://10.120.104.11:52584 has been closed.
2023-06-26 20:37:45,101 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34409', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,101 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34409
2023-06-26 20:37:45,101 - distributed.core - INFO - Connection to tcp://10.120.104.11:52604 has been closed.
2023-06-26 20:37:45,101 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37141', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,101 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37141
2023-06-26 20:37:45,101 - distributed.core - INFO - Connection to tcp://10.120.104.11:52568 has been closed.
2023-06-26 20:37:45,101 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37793', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,101 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37793
2023-06-26 20:37:45,101 - distributed.core - INFO - Connection to tcp://10.120.104.11:52636 has been closed.
2023-06-26 20:37:45,102 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36863', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,102 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36863
2023-06-26 20:37:45,102 - distributed.core - INFO - Connection to tcp://10.120.104.11:52552 has been closed.
2023-06-26 20:37:45,102 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35345', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,102 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35345
2023-06-26 20:37:45,103 - distributed.core - INFO - Connection to tcp://10.120.104.11:58354 has been closed.
2023-06-26 20:37:45,103 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46115', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,103 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46115
2023-06-26 20:37:45,103 - distributed.core - INFO - Connection to tcp://10.120.104.11:52602 has been closed.
2023-06-26 20:37:45,103 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:32855', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,104 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32855
2023-06-26 20:37:45,104 - distributed.core - INFO - Connection to tcp://10.120.104.11:52592 has been closed.
2023-06-26 20:37:45,104 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43189', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,104 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43189
2023-06-26 20:37:45,104 - distributed.core - INFO - Connection to tcp://10.120.104.11:52524 has been closed.
2023-06-26 20:37:45,104 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33067', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,104 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33067
2023-06-26 20:37:45,104 - distributed.core - INFO - Connection to tcp://10.120.104.11:58362 has been closed.
2023-06-26 20:37:45,104 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42039', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,104 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42039
2023-06-26 20:37:45,104 - distributed.core - INFO - Connection to tcp://10.120.104.11:58348 has been closed.
2023-06-26 20:37:45,104 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44649', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,105 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44649
2023-06-26 20:37:45,105 - distributed.core - INFO - Connection to tcp://10.120.104.11:52620 has been closed.
2023-06-26 20:37:45,105 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42299', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,105 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42299
2023-06-26 20:37:45,105 - distributed.core - INFO - Connection to tcp://10.120.104.11:52580 has been closed.
2023-06-26 20:37:45,105 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36679', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,105 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36679
2023-06-26 20:37:45,106 - distributed.core - INFO - Connection to tcp://10.120.104.11:58326 has been closed.
2023-06-26 20:37:45,106 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41261', status: running, memory: 0, processing: 0>
2023-06-26 20:37:45,106 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41261
2023-06-26 20:37:45,106 - distributed.scheduler - INFO - Lost all workers
2023-06-26 20:37:45,106 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 20:37:45,106 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:52602>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,108 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:52524>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,109 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:52580>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,109 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58326>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58326>: Stream is closed
2023-06-26 20:37:45,109 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58362>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,109 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:52620>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,109 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:52592>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,109 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58348>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,109 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58354>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,110 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 20:37:45,113 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 20:37:45,114 - distributed.scheduler - INFO - End scheduler
