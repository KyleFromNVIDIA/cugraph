RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 20:26:17,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39541'
2023-06-26 20:26:17,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45711'
2023-06-26 20:26:17,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43479'
2023-06-26 20:26:17,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36917'
2023-06-26 20:26:17,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37869'
2023-06-26 20:26:17,636 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45269'
2023-06-26 20:26:17,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46429'
2023-06-26 20:26:17,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37229'
2023-06-26 20:26:17,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43305'
2023-06-26 20:26:17,644 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45975'
2023-06-26 20:26:17,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44141'
2023-06-26 20:26:17,649 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44601'
2023-06-26 20:26:17,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42073'
2023-06-26 20:26:17,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36391'
2023-06-26 20:26:17,655 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33913'
2023-06-26 20:26:17,659 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38505'
2023-06-26 20:26:19,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,298 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,298 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,335 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,346 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,346 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,384 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:26:19,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:26:19,464 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,470 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,476 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,489 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,489 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,523 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,523 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,537 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,550 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,550 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,553 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:19,567 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:26:25,689 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39769
2023-06-26 20:26:25,690 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39769
2023-06-26 20:26:25,690 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42817
2023-06-26 20:26:25,690 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:25,690 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:25,690 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:25,690 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:25,690 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-23_6sanl
2023-06-26 20:26:25,691 - distributed.worker - INFO - Starting Worker plugin PreImport-0b8fd745-8b06-47ad-81b7-28b2d66c2297
2023-06-26 20:26:25,691 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac5e972d-b8ae-40b4-93c8-df134de258d0
2023-06-26 20:26:25,817 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41129
2023-06-26 20:26:25,817 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41129
2023-06-26 20:26:25,817 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46077
2023-06-26 20:26:25,817 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:25,817 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:25,817 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:25,817 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:25,817 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h6284phh
2023-06-26 20:26:25,818 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-161dbdf5-e003-4af9-9165-abf301599056
2023-06-26 20:26:25,818 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31980a3d-bd43-4015-80e3-30f4abee1833
2023-06-26 20:26:25,828 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39375
2023-06-26 20:26:25,828 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39375
2023-06-26 20:26:25,828 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45819
2023-06-26 20:26:25,828 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:25,828 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:25,828 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:25,828 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:25,828 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3o81yg2b
2023-06-26 20:26:25,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8300363c-1e4e-4c99-ac38-a4aa7d8d83c9
2023-06-26 20:26:25,829 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ec5e271-ad43-45f0-a776-e376c22283ae
2023-06-26 20:26:25,830 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41765
2023-06-26 20:26:25,830 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41765
2023-06-26 20:26:25,830 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42095
2023-06-26 20:26:25,831 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:25,831 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:25,831 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:25,831 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:25,831 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_l4gogh6
2023-06-26 20:26:25,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cb09e3b-37db-4489-ad37-db28b85ae5e5
2023-06-26 20:26:26,533 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38003
2023-06-26 20:26:26,533 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38003
2023-06-26 20:26:26,534 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34947
2023-06-26 20:26:26,534 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,534 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,534 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,534 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3r6ha45o
2023-06-26 20:26:26,534 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3729a520-4334-4a51-ba6a-b4f4f184c397
2023-06-26 20:26:26,534 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4eaad895-9d49-43d5-a3f5-39bd9665587e
2023-06-26 20:26:26,542 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42301
2023-06-26 20:26:26,543 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42301
2023-06-26 20:26:26,543 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36099
2023-06-26 20:26:26,543 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,543 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,543 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,543 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,543 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a7hfu5fr
2023-06-26 20:26:26,544 - distributed.worker - INFO - Starting Worker plugin PreImport-8fa0967a-a214-4c47-a442-84e403d38f4e
2023-06-26 20:26:26,544 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2be016c-30f1-4cc4-945a-370fb3a5f47d
2023-06-26 20:26:26,545 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33713
2023-06-26 20:26:26,545 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33713
2023-06-26 20:26:26,545 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40185
2023-06-26 20:26:26,545 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,545 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,545 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,545 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nsypkd4i
2023-06-26 20:26:26,546 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b480f19c-8899-4459-b085-3cbac734612b
2023-06-26 20:26:26,546 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34127
2023-06-26 20:26:26,546 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34127
2023-06-26 20:26:26,546 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46223
2023-06-26 20:26:26,546 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,546 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,547 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,547 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fpavqhgg
2023-06-26 20:26:26,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da471f91-b6df-4e99-b579-227b7d18997c
2023-06-26 20:26:26,562 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44225
2023-06-26 20:26:26,562 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44225
2023-06-26 20:26:26,562 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34601
2023-06-26 20:26:26,562 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,562 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,562 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,562 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,562 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s9wbwkfd
2023-06-26 20:26:26,563 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a82ec53-54db-4097-a786-5c965c5b6c69
2023-06-26 20:26:26,585 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39781
2023-06-26 20:26:26,585 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39781
2023-06-26 20:26:26,585 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38401
2023-06-26 20:26:26,585 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,585 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,585 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,585 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1dmmf32t
2023-06-26 20:26:26,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f39915b2-2b12-418c-b8b5-ab54cad01908
2023-06-26 20:26:26,606 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44415
2023-06-26 20:26:26,606 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44415
2023-06-26 20:26:26,607 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43301
2023-06-26 20:26:26,607 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,607 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,607 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,607 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,607 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ppxxnti9
2023-06-26 20:26:26,607 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f5e6168f-02e8-455c-a524-f04b86dc2678
2023-06-26 20:26:26,631 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40825
2023-06-26 20:26:26,632 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40825
2023-06-26 20:26:26,632 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36339
2023-06-26 20:26:26,632 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,632 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,632 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,632 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-26j_i0r4
2023-06-26 20:26:26,634 - distributed.worker - INFO - Starting Worker plugin PreImport-43fada4e-e555-4abb-9f0d-c3a1ef27a3f9
2023-06-26 20:26:26,634 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2f615b9-c683-4efb-897f-e268dddd7986
2023-06-26 20:26:26,644 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39569
2023-06-26 20:26:26,645 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39569
2023-06-26 20:26:26,645 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33845
2023-06-26 20:26:26,645 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,645 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,645 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,645 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-re9kp8s5
2023-06-26 20:26:26,646 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0915a72e-6d54-4315-990c-f90054f91cc7
2023-06-26 20:26:26,663 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37083
2023-06-26 20:26:26,663 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37083
2023-06-26 20:26:26,663 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44913
2023-06-26 20:26:26,663 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,663 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,663 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33817
2023-06-26 20:26:26,663 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,663 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33817
2023-06-26 20:26:26,663 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,663 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oe99_7km
2023-06-26 20:26:26,663 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46659
2023-06-26 20:26:26,664 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,664 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,664 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,664 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,664 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u78ymu_2
2023-06-26 20:26:26,664 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f590e8ac-a432-4be6-9119-8028376b5188
2023-06-26 20:26:26,664 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd20f209-3208-4b11-967a-736789f0ee4b
2023-06-26 20:26:26,664 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44909
2023-06-26 20:26:26,665 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44909
2023-06-26 20:26:26,665 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41711
2023-06-26 20:26:26,665 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:26:26,665 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:26,665 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:26:26,665 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:26:26,665 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1r81uksr
2023-06-26 20:26:26,665 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afcbe0f8-8358-4371-a0f5-b7036b9942c5
2023-06-26 20:26:27,590 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c79abdd6-ac82-44b1-bb1d-dc26d838b24c
2023-06-26 20:26:27,592 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:27,615 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:27,615 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:27,617 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:29,369 - distributed.worker - INFO - Starting Worker plugin PreImport-509dd26a-1e15-497c-83df-8c134468b07e
2023-06-26 20:26:29,371 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:29,392 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:29,392 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:29,394 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:29,506 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89237623-9ebd-447f-8e31-3684947cc506
2023-06-26 20:26:29,506 - distributed.worker - INFO - Starting Worker plugin PreImport-213afeab-c251-45bb-b5db-d9b0a01335fd
2023-06-26 20:26:29,507 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:29,525 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:29,525 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:29,528 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:29,638 - distributed.worker - INFO - Starting Worker plugin PreImport-6131d52a-ac9f-4dd8-8d62-a26ab6ada869
2023-06-26 20:26:29,639 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:29,651 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:29,651 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:29,652 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,105 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f08f006a-e3eb-4930-95e1-8414fe92c294
2023-06-26 20:26:30,105 - distributed.worker - INFO - Starting Worker plugin PreImport-64302969-1584-4d17-aee2-7c24f1120b0e
2023-06-26 20:26:30,105 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,120 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,120 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,121 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,218 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6617559d-c2c0-4a70-8a8b-5469b9438ef9
2023-06-26 20:26:30,219 - distributed.worker - INFO - Starting Worker plugin PreImport-dbf5ea65-f50b-4b30-9634-0dacf2b0d0ac
2023-06-26 20:26:30,219 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,234 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8eb65dea-a238-4783-b44a-db155f4956f5
2023-06-26 20:26:30,235 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e236e1df-31b7-4d23-8071-2d64affbc60a
2023-06-26 20:26:30,235 - distributed.worker - INFO - Starting Worker plugin PreImport-ab082720-3482-4bcd-88d9-3f918f3769a0
2023-06-26 20:26:30,235 - distributed.worker - INFO - Starting Worker plugin PreImport-6d74d958-63ca-4407-baba-6bba4685e4ce
2023-06-26 20:26:30,235 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,235 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,235 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,236 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,239 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,247 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,249 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,264 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec639195-4bdb-4ac7-abd1-90b153732ab6
2023-06-26 20:26:30,265 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,266 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,266 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,268 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,274 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91c70ad0-e767-46bb-b5bf-d18896b5493d
2023-06-26 20:26:30,274 - distributed.worker - INFO - Starting Worker plugin PreImport-b7270478-676d-443f-9caa-00b7cd67cc29
2023-06-26 20:26:30,275 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,283 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,283 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,286 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,289 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,289 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,290 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,291 - distributed.worker - INFO - Starting Worker plugin PreImport-ad164fe9-588f-48b1-bb76-b0f52ed07f32
2023-06-26 20:26:30,292 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,299 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b99caa87-51b2-448b-a827-3c38dc09daa1
2023-06-26 20:26:30,300 - distributed.worker - INFO - Starting Worker plugin PreImport-db120c59-f62d-4a73-8141-e09217a2a6b2
2023-06-26 20:26:30,307 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,307 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1d14d01f-5cbc-4c8c-93cf-04ad7ad17cb1
2023-06-26 20:26:30,307 - distributed.worker - INFO - Starting Worker plugin PreImport-9ffeb5cc-2c3d-46a0-a92c-8812133f1f60
2023-06-26 20:26:30,308 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,308 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,310 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,310 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d3c6b3f7-c796-4080-9033-63a53bc89b48
2023-06-26 20:26:30,311 - distributed.worker - INFO - Starting Worker plugin PreImport-a7c5d451-a2ce-4745-b8a7-05e68d8255bd
2023-06-26 20:26:30,312 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,315 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3600454-de48-42e3-946d-6c74418ca626
2023-06-26 20:26:30,316 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,316 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6522a7a4-8ea1-4a29-b648-a78d6ef2a916
2023-06-26 20:26:30,317 - distributed.worker - INFO - Starting Worker plugin PreImport-58f2e66d-3b56-412f-8a8a-6f4286f7a0c0
2023-06-26 20:26:30,318 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,321 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,321 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,323 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,324 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,324 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,325 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,329 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,329 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,331 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,331 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,331 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,334 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:30,345 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:26:30,345 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:26:30,348 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:26:48,549 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,549 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,549 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,549 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,550 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,550 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,550 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,550 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,550 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,551 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,553 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,554 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,556 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,556 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,556 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,557 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:26:48,567 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,567 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,567 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,567 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,567 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,567 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,567 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:48,568 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:49,264 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:26:52,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:04,620 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:04,873 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:04,895 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:04,966 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,061 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,075 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,099 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,184 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,246 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,266 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,283 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,286 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,304 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,317 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,432 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:05,485 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:27:11,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:11,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:27:50,316 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,316 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,317 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,320 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,320 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,322 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,323 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,323 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,323 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,323 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,323 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,323 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,326 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,327 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,327 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:50,327 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:27:53,488 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,494 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:27:53,495 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:00,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:00,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:27,438 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,439 - distributed.worker - INFO - Run out-of-band function '_get_allocation_counts'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,454 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,455 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,455 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,455 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:28:27,466 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,466 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:27,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:28:31,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:31,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,904 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33713. Reason: scheduler-restart
2023-06-26 20:28:31,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,905 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33817. Reason: scheduler-restart
2023-06-26 20:28:31,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,905 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,905 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34127. Reason: scheduler-restart
2023-06-26 20:28:31,906 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,906 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37083. Reason: scheduler-restart
2023-06-26 20:28:31,906 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,906 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,907 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,907 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38003. Reason: scheduler-restart
2023-06-26 20:28:31,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,907 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,907 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39375. Reason: scheduler-restart
2023-06-26 20:28:31,907 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,907 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,907 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,907 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,907 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,908 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,908 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33713
2023-06-26 20:28:31,908 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39769. Reason: scheduler-restart
2023-06-26 20:28:31,908 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,908 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39569. Reason: scheduler-restart
2023-06-26 20:28:31,909 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,909 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,909 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39781. Reason: scheduler-restart
2023-06-26 20:28:31,909 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,909 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,909 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,909 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40825. Reason: scheduler-restart
2023-06-26 20:28:31,909 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,909 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,909 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,910 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,910 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41129. Reason: scheduler-restart
2023-06-26 20:28:31,910 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,910 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,910 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,911 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,911 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,912 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,912 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,913 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,913 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:28:31,917 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,917 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44415. Reason: scheduler-restart
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33817
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34127
2023-06-26 20:28:31,918 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42301. Reason: scheduler-restart
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37083
2023-06-26 20:28:31,918 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44225. Reason: scheduler-restart
2023-06-26 20:28:31,918 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41765. Reason: scheduler-restart
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38003
2023-06-26 20:28:31,918 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39375
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39769
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39569
2023-06-26 20:28:31,918 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39781
2023-06-26 20:28:31,918 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40825
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33817
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34127
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33817
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37083
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33817
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38003
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34127
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39375
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34127
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39769
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37083
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39569
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38003
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37083
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39781
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39375
2023-06-26 20:28:31,919 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40825
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38003
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39769
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39375
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39569
2023-06-26 20:28:31,919 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39769
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39781
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40825
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39569
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39781
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40825
2023-06-26 20:28:31,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41129
2023-06-26 20:28:31,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41129
2023-06-26 20:28:31,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41129
2023-06-26 20:28:31,920 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,920 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,929 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,930 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,930 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44909. Reason: scheduler-restart
2023-06-26 20:28:31,930 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33817
2023-06-26 20:28:31,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34127
2023-06-26 20:28:31,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37083
2023-06-26 20:28:31,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38003
2023-06-26 20:28:31,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39375
2023-06-26 20:28:31,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39769
2023-06-26 20:28:31,931 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39569
2023-06-26 20:28:31,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39781
2023-06-26 20:28:31,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40825
2023-06-26 20:28:31,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41129
2023-06-26 20:28:31,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42301
2023-06-26 20:28:31,932 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41765
2023-06-26 20:28:31,933 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,933 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33817
2023-06-26 20:28:31,933 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34127
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37083
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38003
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39375
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39769
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39569
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42301
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39781
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40825
2023-06-26 20:28:31,934 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41765
2023-06-26 20:28:31,935 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,935 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44909
2023-06-26 20:28:31,935 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,935 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41129
2023-06-26 20:28:31,940 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:31,940 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42301
2023-06-26 20:28:31,940 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41765
2023-06-26 20:28:31,953 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44909
2023-06-26 20:28:31,953 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44225
2023-06-26 20:28:31,953 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:28:31,963 - distributed.nanny - INFO - Worker closed
2023-06-26 20:28:33,908 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:34,821 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:35,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:35,737 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:35,905 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:36,798 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:37,558 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:37,559 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:38,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:38,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:38,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:38,489 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,492 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,494 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,496 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,498 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,500 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,507 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,508 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,510 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,513 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,516 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:28:38,665 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:38,722 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:39,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:39,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:39,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:39,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:39,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:39,438 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:39,764 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41261
2023-06-26 20:28:39,764 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41261
2023-06-26 20:28:39,764 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41219
2023-06-26 20:28:39,764 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:39,764 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:39,764 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:39,764 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:39,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aruwrlmj
2023-06-26 20:28:39,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-306b53ce-01e3-4693-a3e7-f3720c1c7cc5
2023-06-26 20:28:40,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,188 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,205 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,235 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,235 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,255 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,271 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,271 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,271 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,271 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:28:40,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:28:40,359 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,391 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,417 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44649
2023-06-26 20:28:40,417 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44649
2023-06-26 20:28:40,417 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39237
2023-06-26 20:28:40,417 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:40,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:40,417 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:40,417 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:40,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0b0fhf6l
2023-06-26 20:28:40,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de3b3762-4ce1-4987-82b4-b7d9ad8a0582
2023-06-26 20:28:40,425 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34953
2023-06-26 20:28:40,426 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34953
2023-06-26 20:28:40,426 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39175
2023-06-26 20:28:40,426 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:40,426 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:40,426 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:40,426 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:40,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o4xtlkme
2023-06-26 20:28:40,426 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b49cde5-10d6-47a0-95f8-117f409e9938
2023-06-26 20:28:40,435 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,439 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,451 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,453 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:40,527 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:28:41,091 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46115
2023-06-26 20:28:41,091 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46115
2023-06-26 20:28:41,091 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38887
2023-06-26 20:28:41,091 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:41,091 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:41,091 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:41,091 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:41,091 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jkleuo2h
2023-06-26 20:28:41,092 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23857bc0-3af3-4caf-9f63-fc744fb75b08
2023-06-26 20:28:41,453 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42039
2023-06-26 20:28:41,453 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42039
2023-06-26 20:28:41,453 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46693
2023-06-26 20:28:41,453 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:41,454 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:41,454 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:41,454 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:41,454 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ljoxb_co
2023-06-26 20:28:41,454 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80570324-de5d-4158-8df0-86735924d0b9
2023-06-26 20:28:41,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e30089e-51ff-4349-a2f6-2ea39822c326
2023-06-26 20:28:41,635 - distributed.worker - INFO - Starting Worker plugin PreImport-1aeab822-4671-4b1d-96b4-acf160d949ec
2023-06-26 20:28:41,636 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:41,654 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:41,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:41,656 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:43,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9830b8a-f5f1-493a-afbb-6b9c9b093a22
2023-06-26 20:28:43,037 - distributed.worker - INFO - Starting Worker plugin PreImport-4c823eed-bd29-4cb1-8f1f-4ad891ee9a77
2023-06-26 20:28:43,038 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:43,049 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54246bfc-df95-401b-9f3e-79f1b27620e2
2023-06-26 20:28:43,050 - distributed.worker - INFO - Starting Worker plugin PreImport-38bf4196-646f-4e52-abaf-65385515c0ef
2023-06-26 20:28:43,050 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:43,054 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:43,054 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:43,056 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:43,060 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:43,060 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:43,061 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:43,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-299d4581-c75a-42c1-a57f-34120e8526c6
2023-06-26 20:28:43,318 - distributed.worker - INFO - Starting Worker plugin PreImport-1b69e717-db88-4916-b188-eb1ab170ed71
2023-06-26 20:28:43,320 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:43,336 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:43,336 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:43,338 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:43,387 - distributed.worker - INFO - Starting Worker plugin PreImport-4168240c-c7c7-4c03-b6a5-0174ca943ed9
2023-06-26 20:28:43,388 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef5fe4f3-99a5-4f63-8be9-2a67b40c0580
2023-06-26 20:28:43,390 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:43,407 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:43,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:43,409 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:47,009 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35345
2023-06-26 20:28:47,009 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35345
2023-06-26 20:28:47,009 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41239
2023-06-26 20:28:47,009 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,009 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,009 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,010 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,010 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-94j7v4l5
2023-06-26 20:28:47,010 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62c6b918-2dc5-45d2-b8e0-8f82da3c8637
2023-06-26 20:28:47,017 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32855
2023-06-26 20:28:47,017 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32855
2023-06-26 20:28:47,017 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35517
2023-06-26 20:28:47,017 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,017 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,017 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,018 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,018 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9p900i2w
2023-06-26 20:28:47,018 - distributed.worker - INFO - Starting Worker plugin PreImport-69211ab6-9360-483f-838d-9aa907ca9836
2023-06-26 20:28:47,019 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4672550-561b-4b7c-bf07-61d875d8315c
2023-06-26 20:28:47,051 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43189
2023-06-26 20:28:47,051 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43189
2023-06-26 20:28:47,051 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33481
2023-06-26 20:28:47,051 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,051 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,051 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,052 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,052 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z4zculaf
2023-06-26 20:28:47,052 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3eef87b-9d7e-40ab-83a4-c3cdb3fa8e8f
2023-06-26 20:28:47,052 - distributed.worker - INFO - Starting Worker plugin PreImport-932172bc-09c6-4cd6-98ff-42035ec332a5
2023-06-26 20:28:47,052 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e2f85e4-f9e5-4f66-b02e-1eae6aef3b30
2023-06-26 20:28:47,063 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36679
2023-06-26 20:28:47,064 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36679
2023-06-26 20:28:47,064 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36755
2023-06-26 20:28:47,064 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,064 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,064 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,064 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,064 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-esk6_7ua
2023-06-26 20:28:47,065 - distributed.worker - INFO - Starting Worker plugin PreImport-7b913dff-80b4-4e79-9a77-ca13ca62e4e3
2023-06-26 20:28:47,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ebee387-ab45-4685-8698-8f7f3718a833
2023-06-26 20:28:47,068 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37793
2023-06-26 20:28:47,069 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37793
2023-06-26 20:28:47,069 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38737
2023-06-26 20:28:47,069 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,069 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,069 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,069 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,069 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oef_1uce
2023-06-26 20:28:47,069 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-378e002f-73dd-458b-8413-cd0adf58af0d
2023-06-26 20:28:47,072 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0263396-bd63-45b3-a399-aaa6f8bea6d9
2023-06-26 20:28:47,078 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44335
2023-06-26 20:28:47,078 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44335
2023-06-26 20:28:47,078 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35385
2023-06-26 20:28:47,078 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,078 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,078 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,078 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,078 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x84__cn3
2023-06-26 20:28:47,079 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85d8e78e-39bd-4e9d-8fc7-af9f22981f5e
2023-06-26 20:28:47,084 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42299
2023-06-26 20:28:47,084 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42299
2023-06-26 20:28:47,084 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40739
2023-06-26 20:28:47,084 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,084 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,085 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,085 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pz3rey5o
2023-06-26 20:28:47,085 - distributed.worker - INFO - Starting Worker plugin PreImport-19f69e34-ac2f-41dc-9fbd-57339334506c
2023-06-26 20:28:47,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37f0ed70-025c-4255-a3ca-32832a6abb30
2023-06-26 20:28:47,088 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37141
2023-06-26 20:28:47,088 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37141
2023-06-26 20:28:47,088 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43413
2023-06-26 20:28:47,088 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,088 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,088 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,088 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,088 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rbkv7iu1
2023-06-26 20:28:47,089 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a96af8d2-87ff-4565-b38a-4c5f178b3c3e
2023-06-26 20:28:47,100 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33067
2023-06-26 20:28:47,100 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33067
2023-06-26 20:28:47,100 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39641
2023-06-26 20:28:47,100 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,100 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,101 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,101 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,101 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z99vlz95
2023-06-26 20:28:47,100 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36863
2023-06-26 20:28:47,101 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36863
2023-06-26 20:28:47,101 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39893
2023-06-26 20:28:47,101 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,101 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,101 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,101 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,101 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8we79fbq
2023-06-26 20:28:47,101 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d694885-22ef-4f6c-b039-f03f340558bb
2023-06-26 20:28:47,101 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9a564d3-e1cd-4071-a78d-e5c48c3a9350
2023-06-26 20:28:47,113 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34409
2023-06-26 20:28:47,114 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34409
2023-06-26 20:28:47,114 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41037
2023-06-26 20:28:47,114 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:28:47,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:47,114 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:28:47,114 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:28:47,114 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-srqrrlf8
2023-06-26 20:28:47,115 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0f8eb4ea-60e4-437e-9edb-251965f4a48c
2023-06-26 20:28:49,718 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03d37c2f-5853-41be-869e-46a37d5fcfc9
2023-06-26 20:28:49,718 - distributed.worker - INFO - Starting Worker plugin PreImport-8c8e6d52-b9ba-4d11-ad01-c8feaf5dfd83
2023-06-26 20:28:49,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,736 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f874727a-b71d-424b-82a0-79dffa11cc75
2023-06-26 20:28:49,736 - distributed.worker - INFO - Starting Worker plugin PreImport-3adf07bb-b469-48ed-bff9-84bd6612dddb
2023-06-26 20:28:49,738 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4944204d-6161-47d8-8dc6-733d13d0e4b1
2023-06-26 20:28:49,747 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,747 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,748 - distributed.worker - INFO - Starting Worker plugin PreImport-da41f829-7f09-485f-ba45-c24dcd1a0e5b
2023-06-26 20:28:49,749 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,749 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,749 - distributed.worker - INFO - Starting Worker plugin PreImport-294c509c-cb68-463f-94cf-98eab431dccc
2023-06-26 20:28:49,752 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,756 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08a68649-262a-4a92-87f2-4a6697d529bf
2023-06-26 20:28:49,758 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,758 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,758 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,761 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6402cb0-9eab-47ca-ad42-dec71689381a
2023-06-26 20:28:49,765 - distributed.worker - INFO - Starting Worker plugin PreImport-9e745bf8-24bd-4124-a197-9b710179ad8b
2023-06-26 20:28:49,766 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,766 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,766 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,768 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,777 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,777 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,778 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,778 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,779 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,781 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,783 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,786 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,786 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,789 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,796 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,796 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,797 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e1bd961-6120-4da4-bfee-87fe5e00eeb7
2023-06-26 20:28:49,806 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,810 - distributed.worker - INFO - Starting Worker plugin PreImport-a95cc10e-b898-45e3-be35-f0ecaab5406c
2023-06-26 20:28:49,810 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e82d62c4-5dc6-4f3b-8645-3ebb2860c834
2023-06-26 20:28:49,811 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,814 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e29002f-f558-49fc-a394-8aa17d7867e1
2023-06-26 20:28:49,815 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,816 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca630659-ea4d-401e-a840-b4215c092b55
2023-06-26 20:28:49,817 - distributed.worker - INFO - Starting Worker plugin PreImport-f3c9e4ba-f8cc-4d05-8fa7-8f5eecc574da
2023-06-26 20:28:49,818 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,827 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,827 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,828 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,829 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,830 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,830 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,832 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:49,836 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:28:49,836 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:28:49,839 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:28:59,182 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,350 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,357 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,372 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,405 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,456 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,474 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,504 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,710 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,715 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,719 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,732 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,792 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,815 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,820 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,942 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:28:59,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,955 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:28:59,964 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,964 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,965 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:28:59,978 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,978 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,978 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,978 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,978 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,978 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,978 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,978 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,979 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,979 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,979 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,979 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,979 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,979 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,979 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:28:59,979 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:29:03,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,920 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,921 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,921 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:10,921 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:29:22,776 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,776 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,776 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,776 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,776 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,776 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:29:22,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34953. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44335. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36863. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35345. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37793. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34409. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37141. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32855. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,098 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43189. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,098 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41261. Reason: worker-close
2023-06-26 20:37:45,097 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46115. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,098 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33067. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,098 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44649. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,098 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42039. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,098 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42299. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,098 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36679. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:37:45,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46429'. Reason: nanny-close
2023-06-26 20:37:45,099 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,099 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37229'. Reason: nanny-close
2023-06-26 20:37:45,100 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43305'. Reason: nanny-close
2023-06-26 20:37:45,099 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:58326 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:37:45,101 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,101 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45975'. Reason: nanny-close
2023-06-26 20:37:45,101 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,101 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39541'. Reason: nanny-close
2023-06-26 20:37:45,102 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,102 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45711'. Reason: nanny-close
2023-06-26 20:37:45,102 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,102 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43479'. Reason: nanny-close
2023-06-26 20:37:45,103 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,103 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36917'. Reason: nanny-close
2023-06-26 20:37:45,103 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37869'. Reason: nanny-close
2023-06-26 20:37:45,104 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45269'. Reason: nanny-close
2023-06-26 20:37:45,104 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44141'. Reason: nanny-close
2023-06-26 20:37:45,105 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44601'. Reason: nanny-close
2023-06-26 20:37:45,105 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42073'. Reason: nanny-close
2023-06-26 20:37:45,106 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36391'. Reason: nanny-close
2023-06-26 20:37:45,106 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33913'. Reason: nanny-close
2023-06-26 20:37:45,107 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,107 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38505'. Reason: nanny-close
2023-06-26 20:37:45,107 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:37:45,118 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43479 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:49402 remote=tcp://10.120.104.11:43479>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43479 after 100 s
2023-06-26 20:37:45,119 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46429 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36598 remote=tcp://10.120.104.11:46429>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46429 after 100 s
2023-06-26 20:37:45,121 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37229 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36406 remote=tcp://10.120.104.11:37229>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37229 after 100 s
2023-06-26 20:37:45,121 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45269 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48652 remote=tcp://10.120.104.11:45269>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45269 after 100 s
2023-06-26 20:37:45,121 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45975 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47922 remote=tcp://10.120.104.11:45975>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45975 after 100 s
2023-06-26 20:37:45,122 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44601 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41376 remote=tcp://10.120.104.11:44601>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44601 after 100 s
2023-06-26 20:37:45,122 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39541 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50816 remote=tcp://10.120.104.11:39541>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39541 after 100 s
2023-06-26 20:37:45,123 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36917 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47734 remote=tcp://10.120.104.11:36917>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36917 after 100 s
2023-06-26 20:37:45,124 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37869 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37080 remote=tcp://10.120.104.11:37869>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37869 after 100 s
2023-06-26 20:37:45,127 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36391 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40606 remote=tcp://10.120.104.11:36391>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36391 after 100 s
2023-06-26 20:37:45,127 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33913 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52942 remote=tcp://10.120.104.11:33913>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33913 after 100 s
2023-06-26 20:37:45,127 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38505 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56982 remote=tcp://10.120.104.11:38505>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38505 after 100 s
2023-06-26 20:37:45,129 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43305 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36392 remote=tcp://10.120.104.11:43305>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43305 after 100 s
2023-06-26 20:37:45,130 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44141 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42832 remote=tcp://10.120.104.11:44141>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44141 after 100 s
2023-06-26 20:37:45,132 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45711 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41216 remote=tcp://10.120.104.11:45711>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45711 after 100 s
2023-06-26 20:37:45,136 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42073 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59180 remote=tcp://10.120.104.11:42073>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42073 after 100 s
2023-06-26 20:37:48,308 - distributed.nanny - WARNING - Worker process still alive after 3.1999958801269535 seconds, killing
2023-06-26 20:37:48,308 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:37:48,309 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:37:48,309 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,310 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:37:48,310 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,311 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,311 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,312 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,313 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,313 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,313 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,314 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,315 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,315 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:48,316 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:37:49,100 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,101 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,102 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,102 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,102 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,102 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,103 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,104 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,104 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,105 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,105 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,105 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,107 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,107 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,107 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,107 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492263 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492260 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492257 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492254 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492251 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492248 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492245 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492242 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492239 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492236 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492233 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492210 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492207 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492196 parent=488921 started daemon>
2023-06-26 20:37:49,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492168 parent=488921 started daemon>
2023-06-26 20:37:49,110 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=492157 parent=488921 started daemon>
2023-06-26 20:37:54,287 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 492257 exit status was already read will report exitcode 255
2023-06-26 20:37:54,805 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 492248 exit status was already read will report exitcode 255
2023-06-26 20:37:55,084 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 492251 exit status was already read will report exitcode 255
2023-06-26 20:37:55,341 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 492260 exit status was already read will report exitcode 255
2023-06-26 20:37:56,062 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 492233 exit status was already read will report exitcode 255
