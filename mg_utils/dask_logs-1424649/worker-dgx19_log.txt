RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=12G
             --local-directory=/tmp/
             --scheduler-file=/root/work/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-22 21:30:03,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:46031'
2023-06-22 21:30:03,042 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:40207'
2023-06-22 21:30:03,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:34135'
2023-06-22 21:30:03,047 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:43675'
2023-06-22 21:30:03,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:41157'
2023-06-22 21:30:03,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:39287'
2023-06-22 21:30:03,054 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:39155'
2023-06-22 21:30:03,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:44031'
2023-06-22 21:30:04,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:30:04,508 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:30:04,570 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:30:04,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:30:04,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:30:04,575 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:30:04,576 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:30:04,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:30:04,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:30:04,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:30:04,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:30:04,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:30:04,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:30:04,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:30:04,579 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 21:30:04,579 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 21:30:04,910 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:30:04,976 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:30:05,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:30:05,035 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:30:05,042 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:30:05,044 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:30:05,046 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:30:05,047 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 21:30:06,554 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:39679
2023-06-22 21:30:06,554 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:39679
2023-06-22 21:30:06,554 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44895
2023-06-22 21:30:06,554 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:30:06,554 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:06,554 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:30:06,554 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:30:06,554 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-krzqf_sd
2023-06-22 21:30:06,556 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28379d9a-198b-4242-986a-fe3cc23fa6d9
2023-06-22 21:30:06,760 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c929a1dc-105b-4494-b386-82ce92799765
2023-06-22 21:30:06,761 - distributed.worker - INFO - Starting Worker plugin PreImport-d16c7069-fc57-4c2d-b361-9453315116b4
2023-06-22 21:30:06,762 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,100 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,101 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,103 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:30:07,152 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:45355
2023-06-22 21:30:07,153 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:45355
2023-06-22 21:30:07,153 - distributed.worker - INFO -          dashboard at:        10.33.227.169:46489
2023-06-22 21:30:07,153 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,153 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,153 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:30:07,153 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:30:07,153 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-go6325n6
2023-06-22 21:30:07,154 - distributed.worker - INFO - Starting Worker plugin PreImport-ab3decce-9b09-4949-9a63-0f1c42d0516a
2023-06-22 21:30:07,154 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99bab1d6-0856-4210-b77b-b6f49606045d
2023-06-22 21:30:07,154 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b3e6389-197f-4143-a881-537aa55c4d51
2023-06-22 21:30:07,284 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,296 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,296 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,297 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:30:07,424 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:37139
2023-06-22 21:30:07,424 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:37139
2023-06-22 21:30:07,424 - distributed.worker - INFO -          dashboard at:        10.33.227.169:42545
2023-06-22 21:30:07,424 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,424 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,424 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:30:07,425 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:30:07,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3e09vvzp
2023-06-22 21:30:07,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bfca88e2-c7d6-4eb6-87c1-17de2503eae2
2023-06-22 21:30:07,433 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:33259
2023-06-22 21:30:07,433 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:33259
2023-06-22 21:30:07,433 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44049
2023-06-22 21:30:07,433 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,433 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,433 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:30:07,433 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:30:07,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rwc312m7
2023-06-22 21:30:07,434 - distributed.worker - INFO - Starting Worker plugin PreImport-60b9311a-4fdf-48e8-8fd3-aa69ac8f0674
2023-06-22 21:30:07,434 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9c3c16d-6857-4966-8dc9-9fc65da783af
2023-06-22 21:30:07,434 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eeb2218a-3550-4f35-91e1-01e3e337c4af
2023-06-22 21:30:07,435 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:33261
2023-06-22 21:30:07,435 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:33261
2023-06-22 21:30:07,435 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44225
2023-06-22 21:30:07,435 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,436 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,436 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:30:07,436 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:30:07,436 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-frbh3_db
2023-06-22 21:30:07,436 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:38713
2023-06-22 21:30:07,436 - distributed.worker - INFO - Starting Worker plugin PreImport-18b80a86-00da-4d92-b5be-fac5a3663bff
2023-06-22 21:30:07,436 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:38713
2023-06-22 21:30:07,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cfd51e9f-33c4-4969-b9b6-b0adabd65207
2023-06-22 21:30:07,436 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44539
2023-06-22 21:30:07,436 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,436 - distributed.worker - INFO - Starting Worker plugin RMMSetup-513acc9e-8000-4593-8067-caa6fb229931
2023-06-22 21:30:07,436 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,436 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:30:07,437 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:30:07,437 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u56dsn65
2023-06-22 21:30:07,437 - distributed.worker - INFO - Starting Worker plugin PreImport-6570c975-fc93-4f03-aad2-55fbf6e43a6b
2023-06-22 21:30:07,437 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-975d51e2-a2d1-4f90-99c2-1ec1a3192a2d
2023-06-22 21:30:07,437 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0375e16a-1533-452e-8c36-12b6df1ad614
2023-06-22 21:30:07,440 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:42333
2023-06-22 21:30:07,440 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:42333
2023-06-22 21:30:07,440 - distributed.worker - INFO -          dashboard at:        10.33.227.169:37239
2023-06-22 21:30:07,440 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,440 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,440 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:30:07,440 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:30:07,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-59xws8at
2023-06-22 21:30:07,441 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:42829
2023-06-22 21:30:07,441 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:42829
2023-06-22 21:30:07,441 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44231
2023-06-22 21:30:07,441 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,441 - distributed.worker - INFO - Starting Worker plugin PreImport-1cf5d5cc-14a8-40a6-aa63-0bd092093c15
2023-06-22 21:30:07,441 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,441 - distributed.worker - INFO -               Threads:                          1
2023-06-22 21:30:07,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e29fbfd-e324-4eaf-b0b7-db4518420faf
2023-06-22 21:30:07,441 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 21:30:07,441 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ancgwhir
2023-06-22 21:30:07,442 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c510d33-c2bd-4295-85a1-d22d0ab488a6
2023-06-22 21:30:07,443 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df8c197e-74ac-41d1-8278-4e948c4e2369
2023-06-22 21:30:07,637 - distributed.worker - INFO - Starting Worker plugin PreImport-57d529b8-5dfc-4259-8ed4-ed18fe166fec
2023-06-22 21:30:07,637 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed5c9861-0e52-4aff-bd5c-4717666cc364
2023-06-22 21:30:07,637 - distributed.worker - INFO - Starting Worker plugin PreImport-eb10d394-e2e9-47eb-8d5d-5db5359396f9
2023-06-22 21:30:07,638 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,638 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6086090c-c0a7-4594-bc72-f7c4b926ea5e
2023-06-22 21:30:07,638 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,638 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,639 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,639 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,640 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,648 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,648 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,649 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,649 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,650 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:30:07,650 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,650 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,650 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:30:07,651 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:30:07,652 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,652 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,654 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,655 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:30:07,656 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 21:30:07,656 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 21:30:07,656 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:30:07,659 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 21:30:08,737 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:30:08,737 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:30:08,737 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:30:08,738 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:30:08,738 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:30:08,738 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:30:08,738 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:30:08,740 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:30:08,833 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:30:08,833 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:30:08,833 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:30:08,833 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:30:08,834 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:30:08,834 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:30:08,834 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:30:08,834 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 21:30:20,014 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:30:20,060 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:30:20,094 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:30:20,149 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:30:20,157 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:30:20,163 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:30:20,190 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:30:20,274 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 21:30:26,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:30:26,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:30:26,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:30:26,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:30:26,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:30:26,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:30:26,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:30:26,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:31:00,088 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:31:00,088 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:31:00,092 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:31:00,093 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:31:00,093 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:31:00,093 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:31:00,093 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:31:00,093 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 21:31:04,294 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,294 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,294 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,294 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,294 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,294 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,294 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,295 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,829 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,834 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,834 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,834 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,834 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,834 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,834 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:04,834 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,128 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,133 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,133 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,133 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,133 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,134 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,134 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,134 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,461 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,466 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,466 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,466 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,467 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,467 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,467 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,467 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,791 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,795 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,795 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,796 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,796 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,796 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,796 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:05,796 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,138 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,143 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,143 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,143 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,143 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,143 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,143 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,144 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,517 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,523 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,523 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,523 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,523 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,523 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,524 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,524 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,897 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:06,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,299 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,306 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,306 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,307 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,307 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,307 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,307 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,307 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,722 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:07,728 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 21:31:08,791 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:31:08,791 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:31:08,791 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:31:08,791 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:31:08,791 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:31:08,791 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:31:08,791 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:31:08,792 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-22 21:32:50,364 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:45355. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:32:50,364 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:37139. Reason: worker-close
2023-06-22 21:32:50,364 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:42829. Reason: worker-close
2023-06-22 21:32:50,364 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:42333. Reason: worker-close
2023-06-22 21:32:50,364 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:39679. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:32:50,364 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:33259. Reason: worker-close
2023-06-22 21:32:50,364 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:33261. Reason: worker-handle-scheduler-connection-broken
2023-06-22 21:32:50,364 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:38713. Reason: worker-close
2023-06-22 21:32:50,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:46031'. Reason: nanny-close
2023-06-22 21:32:50,366 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:59764 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:32:50,367 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:32:50,366 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:59792 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:32:50,366 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:59804 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:32:50,367 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:59790 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1894, in _run_once
    handle = self._ready.popleft()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:32:50,369 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:40207'. Reason: nanny-close
2023-06-22 21:32:50,367 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:59762 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:32:50,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:32:50,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:34135'. Reason: nanny-close
2023-06-22 21:32:50,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:32:50,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:43675'. Reason: nanny-close
2023-06-22 21:32:50,372 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:32:50,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:41157'. Reason: nanny-close
2023-06-22 21:32:50,373 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:32:50,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:39287'. Reason: nanny-close
2023-06-22 21:32:50,373 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:32:50,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:39155'. Reason: nanny-close
2023-06-22 21:32:50,374 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:32:50,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:44031'. Reason: nanny-close
2023-06-22 21:32:50,375 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 21:32:50,379 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:46031 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:44750 remote=tcp://10.33.227.169:46031>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:46031 after 100 s
2023-06-22 21:32:50,383 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:40207 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:58650 remote=tcp://10.33.227.169:40207>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:40207 after 100 s
2023-06-22 21:32:50,385 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:43675 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:49940 remote=tcp://10.33.227.169:43675>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:43675 after 100 s
2023-06-22 21:32:50,385 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:34135 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:51538 remote=tcp://10.33.227.169:34135>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:34135 after 100 s
2023-06-22 21:32:50,386 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:39287 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:36322 remote=tcp://10.33.227.169:39287>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:39287 after 100 s
2023-06-22 21:32:50,390 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:41157 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1894, in _run_once
    handle = self._ready.popleft()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:47546 remote=tcp://10.33.227.169:41157>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:41157 after 100 s
2023-06-22 21:32:50,394 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:39155 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:38470 remote=tcp://10.33.227.169:39155>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:39155 after 100 s
2023-06-22 21:32:50,396 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:44031 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:33016 remote=tcp://10.33.227.169:44031>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:44031 after 100 s
2023-06-22 21:32:53,576 - distributed.nanny - WARNING - Worker process still alive after 3.199981689453125 seconds, killing
2023-06-22 21:32:53,576 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing
2023-06-22 21:32:53,578 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:32:53,578 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:32:53,580 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 21:32:53,581 - distributed.nanny - WARNING - Worker process still alive after 3.1999922180175786 seconds, killing
2023-06-22 21:32:53,581 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 21:32:53,582 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 21:32:54,413 - distributed.nanny - INFO - Worker process 1424869 was killed by signal 9
2023-06-22 21:32:54,416 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:32:54,418 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:32:54,418 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:32:54,419 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:32:54,419 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:32:54,419 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:32:54,419 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 21:32:54,421 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1424890 parent=1424853 started daemon>
2023-06-22 21:32:54,421 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1424887 parent=1424853 started daemon>
2023-06-22 21:32:54,421 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1424884 parent=1424853 started daemon>
2023-06-22 21:32:54,422 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1424881 parent=1424853 started daemon>
2023-06-22 21:32:54,422 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1424878 parent=1424853 started daemon>
2023-06-22 21:32:54,422 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1424875 parent=1424853 started daemon>
2023-06-22 21:32:54,422 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1424872 parent=1424853 started daemon>
2023-06-22 21:32:55,275 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1424881 exit status was already read will report exitcode 255
