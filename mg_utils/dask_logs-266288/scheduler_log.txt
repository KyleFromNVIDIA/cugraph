RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 16:39:02,423 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 16:39:02,947 - distributed.scheduler - INFO - State start
2023-06-26 16:39:02,948 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-txaswe2d', purging
2023-06-26 16:39:02,948 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-84c1qjcz', purging
2023-06-26 16:39:02,948 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-q_zq3wxa', purging
2023-06-26 16:39:02,949 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yfp_x5gj', purging
2023-06-26 16:39:02,949 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qi51ab7a', purging
2023-06-26 16:39:02,949 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-mm7_sle4', purging
2023-06-26 16:39:02,949 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7qp0pvsp', purging
2023-06-26 16:39:02,949 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qv4ormz0', purging
2023-06-26 16:39:02,950 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x1fy3rbg', purging
2023-06-26 16:39:02,950 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x5sls0db', purging
2023-06-26 16:39:02,950 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-s1ovmpbq', purging
2023-06-26 16:39:02,950 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-bhs_jctw', purging
2023-06-26 16:39:02,950 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-00tifh1s', purging
2023-06-26 16:39:02,950 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-85o2uxyn', purging
2023-06-26 16:39:02,951 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xvglebjy', purging
2023-06-26 16:39:02,951 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-s4a_qm4x', purging
2023-06-26 16:39:02,963 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 16:39:02,964 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 16:39:02,964 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 16:39:16,545 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41563', status: init, memory: 0, processing: 0>
2023-06-26 16:39:16,558 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41563
2023-06-26 16:39:16,559 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57376
2023-06-26 16:39:19,267 - distributed.scheduler - INFO - Receive client connection: Client-ff3a6b13-143f-11ee-90a8-5cff35c1a711
2023-06-26 16:39:19,267 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57748
2023-06-26 16:39:20,852 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 16:39:21,061 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34391', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,062 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34391
2023-06-26 16:39:21,062 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57760
2023-06-26 16:39:21,222 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34883', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,222 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34883
2023-06-26 16:39:21,222 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57770
2023-06-26 16:39:21,291 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33183', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,291 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33183
2023-06-26 16:39:21,291 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57776
2023-06-26 16:39:21,363 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44715', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,363 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44715
2023-06-26 16:39:21,363 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57804
2023-06-26 16:39:21,383 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34199', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,384 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34199
2023-06-26 16:39:21,384 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57810
2023-06-26 16:39:21,390 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39355', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,390 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39355
2023-06-26 16:39:21,390 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57814
2023-06-26 16:39:21,422 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:45713', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,423 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:45713
2023-06-26 16:39:21,423 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57828
2023-06-26 16:39:21,437 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37721', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,437 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37721
2023-06-26 16:39:21,437 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57838
2023-06-26 16:39:21,452 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38043', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,452 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38043
2023-06-26 16:39:21,452 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57850
2023-06-26 16:39:21,465 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:45277', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,466 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:45277
2023-06-26 16:39:21,466 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57864
2023-06-26 16:39:21,470 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:32833', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,470 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:32833
2023-06-26 16:39:21,470 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57856
2023-06-26 16:39:21,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43621', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,497 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43621
2023-06-26 16:39:21,497 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57876
2023-06-26 16:39:21,498 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44973', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44973
2023-06-26 16:39:21,498 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57890
2023-06-26 16:39:21,498 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40827', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,499 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40827
2023-06-26 16:39:21,499 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57886
2023-06-26 16:39:21,502 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41647', status: init, memory: 0, processing: 0>
2023-06-26 16:39:21,502 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41647
2023-06-26 16:39:21,502 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:57904
2023-06-26 16:39:21,626 - distributed.scheduler - INFO - Remove client Client-ff3a6b13-143f-11ee-90a8-5cff35c1a711
2023-06-26 16:39:21,626 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57748; closing.
2023-06-26 16:39:21,626 - distributed.scheduler - INFO - Remove client Client-ff3a6b13-143f-11ee-90a8-5cff35c1a711
2023-06-26 16:39:21,626 - distributed.scheduler - INFO - Close client connection: Client-ff3a6b13-143f-11ee-90a8-5cff35c1a711
2023-06-26 16:39:47,840 - distributed.scheduler - INFO - Receive client connection: Client-1042473c-1440-11ee-93b7-5cff35c1a711
2023-06-26 16:39:47,841 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:43900
2023-06-26 16:39:47,882 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 16:40:38,755 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:52,218 - distributed.protocol.pickle - INFO - Failed to deserialize b'\x80\x05\x95\x9d`\x00\x00\x00\x00\x00\x00\x8c\x1edistributed.protocol.serialize\x94\x8c\x08ToPickle\x94\x93\x94)\x81\x94}\x94\x8c\x04data\x94\x8c\x13dask.highlevelgraph\x94\x8c\x0eHighLevelGraph\x94\x93\x94)\x81\x94}\x94(\x8c\x0cdependencies\x94}\x94(\x8c1__filter_batches-9e3f1cbb113d5c1295fa52a071797485\x94\x8f\x94(\x8c\'rename-52cf25519d3cbc0d982a2dc8f41d852a\x94\x90h\x0f\x8f\x94u\x8c\x10key_dependencies\x94}\x94\x8c\x06layers\x94}\x94(h\r\x8c\x0edask.blockwise\x94\x8c\tBlockwise\x94\x93\x94)\x81\x94}\x94(\x8c\x0bannotations\x94N\x8c\x16collection_annotations\x94}\x94(\x8c\x0bnpartitions\x94K\x04\x8c\x07columns\x94]\x94(\x8c\x07_BATCH_\x94\x8c\x07_START_\x94e\x8c\x04type\x94\x8c\x18dask_cudf.core.DataFrame\x94\x8c\x0edataframe_type\x94\x8c\x1dcudf.core.dataframe.DataFrame\x94\x8c\rseries_dtypes\x94}\x94(h \x8c\x05numpy\x94\x8c\x05dtype\x94\x93\x94\x8c\x02i4\x94\x89\x88\x87\x94R\x94(K\x03\x8c\x01<\x94NNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK\x00t\x94bh!h*\x8c\x02i8\x94\x89\x88\x87\x94R\x94(K\x03h.NNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK\x00t\x94buu\x8c\x06output\x94h\r\x8c\x0eoutput_indices\x94\x8c\x02.0\x94\x85\x94\x8c\routput_blocks\x94N\x8c\x03dsk\x94}\x94h\r(\x8c\ndask.utils\x94\x8c\x05apply\x94\x93\x94\x8c\x13dask.dataframe.core\x94\x8c\x11apply_and_enforce\x94\x93\x94]\x94(\x8c\x13__dask_blockwise__0\x94\x8c\x13__dask_blockwise__1\x94\x8c\x13__dask_blockwise__2\x94\x8c\x13__dask_blockwise__3\x94\x8c\x13__dask_blockwise__4\x94e\x8c\x08builtins\x94\x8c\x04dict\x94\x93\x94]\x94(]\x94(\x8c\x05_func\x94\x8c\x17cloudpickle.cloudpickle\x94\x8c\x0e_make_function\x94\x93\x94(hM\x8c\r_builtin_type\x94\x93\x94\x8c\x08CodeType\x94\x85\x94R\x94(K\x01K\x00K\x00K\x03K\x05J\x1f\x00\x00\x01C\x16\x88\x00|\x01i\x00|\x02\xa4\x01d\x01|\x00i\x01\xa4\x01\x8e\x01S\x00\x94N\x8c\x0epartition_info\x94\x86\x94)hV\x8c\x04args\x94\x8c\x06kwargs\x94\x87\x94\x8cJ/opt/conda/envs/rapids/lib/python3.10/site-packages/dask/dataframe/core.py\x94\x8c\x04func\x94ME\x1bC\x02\x16\x01\x94\x8c\torig_func\x94\x85\x94)t\x94R\x94}\x94(\x8c\x0b__package__\x94\x8c\x0edask.dataframe\x94\x8c\x08__name__\x94h>\x8c\x08__file__\x94h[uNNhM\x8c\x10_make_empty_cell\x94\x93\x94)R\x94\x85\x94t\x94R\x94\x8c\x1ccloudpickle.cloudpickle_fast\x94\x8c\x12_function_setstate\x94\x93\x94hl}\x94}\x94(heh\\\x8c\x0c__qualname__\x94\x8c\x1cmap_partitions.<locals>.func\x94\x8c\x0f__annotations__\x94}\x94\x8c\x0e__kwdefaults__\x94N\x8c\x0c__defaults__\x94N\x8c\n__module__\x94h>\x8c\x07__doc__\x94N\x8c\x0b__closure__\x94hM\x8c\n_make_cell\x94\x93\x94hQ\x8c\nMethodType\x94\x85\x94R\x94hO(hT(K\x05K\x00K\x00K\x07K\x03KCC^|\x04d\x00u\x00s\nt\x00|\x01\x83\x01d\x01k\x02r\x0cd\x00S\x00|\x04d\x02k\x03r\x1dt\x01|\x04t\x02\x83\x02s\x1dt\x03|\x04\x83\x01\x01\x00t\x04d\x03\x83\x01\x82\x01|\x01|\x02\x19\x00|\x03k\x01}\x05|\x01j\x05|\x05\x19\x00}\x06|\x01|\x05\x0f\x00\x19\x00}\x01|\x06S\x00\x94(NK\x00\x8c\x02sg\x94\x8c\x1fInvalid value of partition_info\x94t\x94(\x8c\x03len\x94\x8c\nisinstance\x94\x8c\x04dict\x94\x8c\x05print\x94\x8c\nValueError\x94\x8c\x03loc\x94t\x94(\x8c\x04self\x94\x8c\x02df\x94\x8c\x0ebatch_col_name\x94\x8c\x0cmax_batch_id\x94hV\x8c\x01f\x94h4t\x94\x8c\\/opt/conda/envs/rapids/lib/python3.10/site-packages/cugraph/gnn/data_loading/bulk_sampler.py\x94\x8c\x10__filter_batches\x94M4\x01C\x12\x14\x08\x04\x01\x12\x01\x08\x01\x08\x01\x0c\x02\n\x01\n\x01\x04\x01\x94))t\x94R\x94}\x94(hc\x8c\x18cugraph.gnn.data_loading\x94he\x8c%cugraph.gnn.data_loading.bulk_sampler\x94hf\x8c\\/opt/conda/envs/rapids/lib/python3.10/site-packages/cugraph/gnn/data_loading/bulk_sampler.py\x94uNNNt\x94R\x94hoh\x9b}\x94}\x94(heh\x92hr\x8c*EXPERIMENTAL__BulkSampler.__filter_batches\x94ht}\x94(h\x8c\x8c\x13cudf.core.dataframe\x94\x8c\tDataFrame\x94\x93\x94h\x8dhG\x8c\x03str\x94\x93\x94h\x8ehG\x8c\x03int\x94\x93\x94hV\x8c\t_operator\x94\x8c\x07getitem\x94\x93\x94\x8c\x06typing\x94\x8c\x05Union\x94\x93\x94hIh\xa4hG\x8c\x04type\x94\x93\x94N\x85\x94R\x94\x87\x94\x86\x94R\x94\x8c\x06return\x94h\xa2uhvNhwN\x85\x94hxh\x98hyNhzN\x8c\x17_cloudpickle_submodules\x94]\x94\x8c\x0b__globals__\x94}\x94u\x86\x94\x86R0\x8c\x0b__mp_main__\x94\x8c\x0bBulkSampler\x94\x93\x94)\x81\x94}\x94(\x8c"_EXPERIMENTAL__BulkSampler__logger\x94\x8c\x07logging\x94\x8c\tgetLogger\x94\x93\x94h\x98\x85\x94R\x94\x8c&_EXPERIMENTAL__BulkSampler__batch_size\x94M\x00\x02\x8c\'_EXPERIMENTAL__BulkSampler__output_path\x94\x8c6/tmp/ramdisk/ogbn_papers100M[4]_b512_f[10, 25]/samples\x94\x8c!_EXPERIMENTAL__BulkSampler__graph\x94\x8c\x1fcugraph.structure.graph_classes\x94\x8c\nMultiGraph\x94\x93\x94)\x81\x94}\x94(\x8c\x05_Impl\x94\x8c=cugraph.structure.graph_implementation.simpleDistributedGraph\x94\x8c\x1asimpleDistributedGraphImpl\x94\x93\x94)\x81\x94}\x94(\x8c\x08edgelist\x94h\xd0\x8c#simpleDistributedGraphImpl.EdgeList\x94\x93\x94)\x81\x94}\x94(\x8c\x0bedgelist_df\x94\x8c\x0edask_cudf.core\x94h\xa1\x93\x94)\x81\x94(h\x08)\x81\x94}\x94(h\x0b}\x94(\x8c\'assign-d4715e2bbb2ce718214721122459d2da\x94\x8f\x94(\x8c(getitem-d7ab54722c046c546499b8160433063c\x94\x8c\'rename-2ac21ccfe73477eec136742f114e3477\x94\x90\x8c\'assign-79e7b6b9955aff4f05b61c6a13c74862\x94\x8f\x94(\x8c\'assign-7a31f8143cc223f15cd3762c9b451864\x94\x8c$add-bd81a73d3da345912094025794950413\x94\x90h\xe7\x8f\x94(\x8c-read-parquet-1a83665a6299205a0cf4e9c5910a7701\x94\x8c$add-2b5167922093b28b8b69868e84a15525\x94\x90h\xea\x8f\x94\x8c(getitem-36ffef13e0cce494c224877403ba899d\x94\x8f\x94(h\xea\x90h\xeb\x8f\x94(h\xed\x90\x8c(getitem-0f5db0edc6a99e5906eedb06c6888b74\x94\x8f\x94(h\xe7\x90h\xe8\x8f\x94(h\xf0\x90\x8c._replicate_df-27a9f24f889ef13127ff26397d281e49\x94\x8f\x94(h\xe5\x90\x8c(getitem-39a702f2ce3c3e154500962a70de3c25\x94\x8f\x94(h\xf3\x90\x8c(getitem-a8b6b35c02594e6072514661af5cb3b9\x94\x8f\x94(h\xf5\x90\x8c)to_frame-56e9a5ed2459805a0d5fec579d89fefc\x94\x8f\x94(h\xf7\x90h\xe4\x8f\x94(h\xf9\x90h\xe3\x8f\x94(h\xf5\x90uh\x11}\x94h\x13}\x94(h\xe1h\x17)\x81\x94}\x94(h\x1aNh\x1b}\x94(h\x1dK h\x1e]\x94(\x8c\x03src\x94\x8c\x03dst\x94eh"\x8c\x18dask_cudf.core.DataFrame\x94h$\x8c\x1dcudf.core.dataframe.DataFrame\x94h&}\x94(j\x03\x01\x00\x00h2j\x04\x01\x00\x00h2uuh4h\xe1h5\x8c\x02.0\x94\x85\x94h8Nh9}\x94h\xe1(\x8c\x16dask.dataframe.methods\x94\x8c\x06assign\x94\x93\x94\x8c\x13__dask_blockwise__0\x94\x8c\x13__dask_blockwise__1\x94\x8c\x13__dask_blockwise__2\x94t\x94s\x8c\tnumblocks\x94}\x94(h\xe4K \x85\x94h\xe3K \x85\x94u\x8c\x07io_deps\x94}\x94\x8c\x07indices\x94h\xe4j\x08\x01\x00\x00\x85\x94\x86\x94j\x04\x01\x00\x00N\x86\x94h\xe3j\x08\x01\x00\x00\x85\x94\x86\x94\x87\x94\x8c\x0bconcatenate\x94N\x8c\x08new_axes\x94}\x94ubh\xe5h\x17)\x81\x94}\x94(h\x1aNh\x1b}\x94(h\x1dK h\x1e]\x94(\x8c\x03src\x94\x8c\x03dst\x94eh"\x8c\x18dask_cudf.core.DataFrame\x94h$\x8c\x1dcudf.core.dataframe.DataFrame\x94h&}\x94(j&\x01\x00\x00h2j\'\x01\x00\x00h2uuh4h\xe5h5\x8c\x02.0\x94\x85\x94h8Nh9}\x94h\xe5(j\r\x01\x00\x00\x8c\x13__dask_blockwise__0\x94\x8c\x13__dask_blockwise__1\x94\x8c\x13__dask_blockwise__2\x94t\x94sj\x12\x01\x00\x00}\x94(h\xe7K \x85\x94h\xe8K \x85\x94uj\x16\x01\x00\x00}\x94j\x18\x01\x00\x00h\xe7j+\x01\x00\x00\x85\x94\x86\x94j\x04\x01\x00\x00N\x86\x94h\xe8j+\x01\x00\x00\x85\x94\x86\x94\x87\x94j\x1f\x01\x00\x00Nj \x01\x00\x00}\x94ubh\xe7h\x17)\x81\x94}\x94(h\x1aNh\x1b}\x94(h\x1dK h\x1e]\x94(j&\x01\x00\x00j\'\x01\x00\x00eh"\x8c\x18dask_cudf.core.DataFrame\x94h$\x8c\x1dcudf.core.dataframe.DataFrame\x94h&}\x94(j&\x01\x00\x00h2j\'\x01\x00\x00h2uuh4h\xe7h5\x8c\x02.0\x94\x85\x94h8Nh9}\x94h\xe7(j\r\x01\x00\x00\x8c\x13__dask_blockwise__0\x94\x8c\x13__dask_blockwise__1\x94\x8c\x13__dask_blockwise__2\x94t\x94sj\x12\x01\x00\x00}\x94(h\xeaK \x85\x94h\xebK \x85\x94uj\x16\x01\x00\x00}\x94j\x18\x01\x00\x00h\xeajD\x01\x00\x00\x85\x94\x86\x94j\x03\x01\x00\x00N\x86\x94h\xebjD\x01\x00\x00\x85\x94\x86\x94\x87\x94j\x1f\x01\x00\x00Nj \x01\x00\x00}\x94ubh\xea\x8c\x0bdask.layers\x94\x8c\x10DataFrameIOLayer\x94\x93\x94)\x81\x94}\x94(\x8c\x04name\x94h\xea\x8c\x08_columns\x94]\x94(j&\x01\x00\x00j\'\x01\x00\x00e\x8c\x06inputs\x94]\x94(]\x94}\x94\x8c\x05piece\x94\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/0.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/1.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/2.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/3.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/4.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/5.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/6.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/7.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/8.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8cZ/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/9.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/10.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/11.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/12.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/13.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/14.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/15.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/16.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/17.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/18.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/19.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/20.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/21.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/22.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/23.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/24.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/25.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/26.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/27.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/28.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/29.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/30.parquet\x94]\x94K\x00a]\x94\x87\x94sa]\x94}\x94jb\x01\x00\x00\x8c[/datasets/abarghi/ogbn_papers100M/parquet/paper__cites__paper/edge_index.parquet/31.parquet\x94]\x94K\x00a]\x94\x87\x94sae\x8c\x07io_func\x94\x8c\x1edask.dataframe.io.parquet.core\x94\x8c\x16ParquetFunctionWrapper\x94\x93\x94)\x81\x94}\x94(\x8c\x06engine\x94\x8c\x14dask_cudf.io.parquet\x94\x8c\nCudfEngine\x94\x93\x94\x8c\x02fs\x94\x8c\x0bfsspec.spec\x94\x8c\rmake_instance\x94\x93\x94\x8c\x1cfsspec.implementations.local\x94\x8c\x0fLocalFileSystem\x94\x93\x94)}\x94\x87\x94R\x94\x8c\x04meta\x94h\x7fhO(hT(K\x03K\x00K\x00K\x04K\x06KCC.d\x01d\x02\x84\x00t\x00|\x01d\x03\x19\x00t\x01t\x02|\x02\x83\x02\x83\x02D\x00\x83\x01}\x02|\x00\xa0\x03|\x01|\x02\xa1\x02}\x03|\x03S\x00\x94(X\x04\x02\x00\x00Perform device-side deserialization tasks.\n\n        Parameters\n        ----------\n        header : dict\n            The metadata required to reconstruct the object.\n        frames : list\n            The Buffers or memoryviews that the object should contain.\n\n        Returns\n        -------\n        Serializable\n            A new instance of `cls` (a subclass of `Serializable`) equivalent\n            to the instance that was serialized to produce the header and\n            frames.\n\n        :meta private:\n        \x94hT(K\x01K\x00K\x00K\x03K\x05KSC&g\x00|\x00]\x0f\\\x02}\x01}\x02|\x01r\x0ft\x00j\x01j\x02\xa0\x03|\x02\xa1\x01n\x01|\x02\x91\x02q\x02S\x00\x94)(\x8c\x04cudf\x94\x8c\x04core\x94\x8c\x06buffer\x94\x8c\tas_buffer\x94t\x94\x8c\x02.0\x94\x8c\x01c\x94h\x8f\x87\x94\x8cD/opt/conda/envs/rapids/lib/python3.10/site-packages/cudf/core/abc.py\x94\x8c\n<listcomp>\x94K\xacC\x08\x06\x00\x06\x02\x14\xff\x06\xff\x94))t\x94R\x94\x8c1Serializable.host_deserialize.<locals>.<listcomp>\x94\x8c\x07is-cuda\x94t\x94(\x8c\x03zip\x94\x8c\x03map\x94\x8c\nmemoryview\x94\x8c\x12device_deserialize\x94t\x94(\x8c\x03cls\x94\x8c\x06header\x94\x8c\x06frames\x94\x8c\x03obj\x94t\x94jA\x02\x00\x00\x8c\x10host_deserialize\x94K\x98C\n\x06\x14\x12\x02\x06\xfe\x0c\x04\x04\x01\x94))t\x94R\x94}\x94(hc\x8c\tcudf.core\x94he\x8c\rcudf.core.abc\x94hfjA\x02\x00\x00uNNNt\x94R\x94hoj[\x02\x00\x00}\x94}\x94(hejS\x02\x00\x00hr\x8c\x1dSerializable.host_deserialize\x94ht}\x94hvNhwNhxjY\x02\x00\x00hyj7\x02\x00\x00hzNh\xb6]\x94h\xb8}\x94j9\x02\x00\x00hM\x8c\tsubimport\x94\x93\x94j9\x02\x00\x00\x85\x94R\x94su\x86\x94\x86R0h\xa2\x86\x94R\x94}\x94(\x8c\x0ftype-serialized\x94C0\x80\x04\x95%\x00\x00\x00\x00\x00\x00\x00\x8c\x13cudf.core.dataframe\x94\x8c\tDataFrame\x94\x93\x94.\x94\x8c\x0ccolumn_names\x94C\x1a\x80\x04\x95\x0f\x00\x00\x00\x00\x00\x00\x00\x8c\x03src\x94\x8c\x03dst\x94\x86\x94.\x94h\x1e}\x94(\x8c\x0ftype-serialized\x94C=\x80\x04\x952\x00\x00\x00\x00\x00\x00\x00\x8c\x1acudf.core.column.numerical\x94\x8c\x0fNumericalColumn\x94\x93\x94.\x94h)CB\x80\x04\x957\x00\x00\x00\x00\x00\x00\x00\x8c\x05numpy\x94\x8c\x05dtype\x94\x93\x94\x8c\x02i8\x94\x89\x88\x87\x94R\x94(K\x03\x8c\x01<\x94NNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK\x00t\x94b.\x94\x8c\x18dtype-is-cudf-'
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
AttributeError: Can't get attribute 'BulkSampler' on <module '__main__' from '/opt/conda/envs/rapids/bin/dask-scheduler'>
2023-06-26 16:40:56,689 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 16:40:56,690 - distributed.scheduler - INFO - Restarting workers and releasing all keys.
2023-06-26 16:40:56,709 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57856; closing.
2023-06-26 16:40:56,709 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:32833', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,709 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57776; closing.
2023-06-26 16:40:56,711 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33183', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,711 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33183
2023-06-26 16:40:56,711 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57810; closing.
2023-06-26 16:40:56,711 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57760; closing.
2023-06-26 16:40:56,712 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34199', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,712 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34199
2023-06-26 16:40:56,712 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34391', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,712 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34391
2023-06-26 16:40:56,712 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57770; closing.
2023-06-26 16:40:56,713 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57838; closing.
2023-06-26 16:40:56,713 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34883', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34883
2023-06-26 16:40:56,713 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37721', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,713 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37721
2023-06-26 16:40:56,713 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57850; closing.
2023-06-26 16:40:56,714 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38043', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,714 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38043
2023-06-26 16:40:56,715 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57814; closing.
2023-06-26 16:40:56,715 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57886; closing.
2023-06-26 16:40:56,715 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57376; closing.
2023-06-26 16:40:56,715 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57904; closing.
2023-06-26 16:40:56,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39355', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,715 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 16:40:56,716 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40827', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,716 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40827
2023-06-26 16:40:56,716 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41563', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,716 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41563
2023-06-26 16:40:56,716 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41647', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,716 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41647
2023-06-26 16:40:56,720 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57890; closing.
2023-06-26 16:40:56,720 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44973', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,720 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44973
2023-06-26 16:40:56,721 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57876; closing.
2023-06-26 16:40:56,722 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43621', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43621
2023-06-26 16:40:56,734 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57804; closing.
2023-06-26 16:40:56,734 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44715', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,734 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44715
2023-06-26 16:40:56,741 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57864; closing.
2023-06-26 16:40:56,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:45277', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45277
2023-06-26 16:40:56,755 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:57828; closing.
2023-06-26 16:40:56,755 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:45713', status: closing, memory: 0, processing: 0>
2023-06-26 16:40:56,755 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45713
2023-06-26 16:40:56,755 - distributed.scheduler - INFO - Lost all workers
2023-06-26 16:41:09,620 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33723', status: init, memory: 0, processing: 0>
2023-06-26 16:41:09,620 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33723
2023-06-26 16:41:09,620 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58392
2023-06-26 16:41:09,692 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39393', status: init, memory: 0, processing: 0>
2023-06-26 16:41:09,692 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39393
2023-06-26 16:41:09,692 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58394
2023-06-26 16:41:12,324 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:45701', status: init, memory: 0, processing: 0>
2023-06-26 16:41:12,325 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:45701
2023-06-26 16:41:12,325 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58432
2023-06-26 16:41:12,349 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43557', status: init, memory: 0, processing: 0>
2023-06-26 16:41:12,350 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43557
2023-06-26 16:41:12,350 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58434
2023-06-26 16:41:12,353 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42819', status: init, memory: 0, processing: 0>
2023-06-26 16:41:12,353 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42819
2023-06-26 16:41:12,353 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58474
2023-06-26 16:41:12,354 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34247', status: init, memory: 0, processing: 0>
2023-06-26 16:41:12,354 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34247
2023-06-26 16:41:12,354 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58466
2023-06-26 16:41:12,355 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33131', status: init, memory: 0, processing: 0>
2023-06-26 16:41:12,355 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33131
2023-06-26 16:41:12,355 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58450
2023-06-26 16:41:12,362 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46365', status: init, memory: 0, processing: 0>
2023-06-26 16:41:12,362 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46365
2023-06-26 16:41:12,362 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58480
2023-06-26 16:41:15,100 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34643', status: init, memory: 0, processing: 0>
2023-06-26 16:41:15,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34643
2023-06-26 16:41:15,100 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58542
2023-06-26 16:41:15,121 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33543', status: init, memory: 0, processing: 0>
2023-06-26 16:41:15,121 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33543
2023-06-26 16:41:15,121 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58558
2023-06-26 16:41:15,397 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35421', status: init, memory: 0, processing: 0>
2023-06-26 16:41:15,398 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35421
2023-06-26 16:41:15,398 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58562
2023-06-26 16:41:15,501 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34131', status: init, memory: 0, processing: 0>
2023-06-26 16:41:15,502 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34131
2023-06-26 16:41:15,502 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58574
2023-06-26 16:41:15,516 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46317', status: init, memory: 0, processing: 0>
2023-06-26 16:41:15,517 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46317
2023-06-26 16:41:15,517 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58588
2023-06-26 16:41:15,522 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36091', status: init, memory: 0, processing: 0>
2023-06-26 16:41:15,522 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36091
2023-06-26 16:41:15,522 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58580
2023-06-26 16:41:15,526 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42173', status: init, memory: 0, processing: 0>
2023-06-26 16:41:15,526 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42173
2023-06-26 16:41:15,526 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58604
2023-06-26 16:41:15,532 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42947', status: init, memory: 0, processing: 0>
2023-06-26 16:41:15,533 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42947
2023-06-26 16:41:15,533 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58616
2023-06-26 16:41:15,652 - distributed.scheduler - INFO - Restarting finished.
2023-06-26 16:41:25,708 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 16:41:55,464 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 16:41:55,467 - distributed.scheduler - INFO - Remove client Client-1042473c-1440-11ee-93b7-5cff35c1a711
2023-06-26 16:41:55,467 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:43900; closing.
2023-06-26 16:41:55,467 - distributed.scheduler - INFO - Remove client Client-1042473c-1440-11ee-93b7-5cff35c1a711
2023-06-26 16:41:55,468 - distributed.scheduler - INFO - Close client connection: Client-1042473c-1440-11ee-93b7-5cff35c1a711
2023-06-26 16:44:12,449 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 16:44:12,450 - distributed.core - INFO - Connection to tcp://10.120.104.11:58562 has been closed.
2023-06-26 16:44:12,450 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35421', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,451 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35421
2023-06-26 16:44:12,451 - distributed.core - INFO - Connection to tcp://10.120.104.11:58604 has been closed.
2023-06-26 16:44:12,451 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42173', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,451 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42173
2023-06-26 16:44:12,451 - distributed.core - INFO - Connection to tcp://10.120.104.11:58616 has been closed.
2023-06-26 16:44:12,451 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42947', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,451 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42947
2023-06-26 16:44:12,452 - distributed.core - INFO - Connection to tcp://10.120.104.11:58574 has been closed.
2023-06-26 16:44:12,452 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34131', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,452 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34131
2023-06-26 16:44:12,452 - distributed.core - INFO - Connection to tcp://10.120.104.11:58580 has been closed.
2023-06-26 16:44:12,452 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36091', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,452 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36091
2023-06-26 16:44:12,452 - distributed.core - INFO - Connection to tcp://10.120.104.11:58394 has been closed.
2023-06-26 16:44:12,452 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39393', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,452 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39393
2023-06-26 16:44:12,452 - distributed.core - INFO - Connection to tcp://10.120.104.11:58588 has been closed.
2023-06-26 16:44:12,453 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46317', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,453 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46317
2023-06-26 16:44:12,453 - distributed.core - INFO - Connection to tcp://10.120.104.11:58392 has been closed.
2023-06-26 16:44:12,453 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33723', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,453 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33723
2023-06-26 16:44:12,453 - distributed.core - INFO - Connection to tcp://10.120.104.11:58542 has been closed.
2023-06-26 16:44:12,453 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34643', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,453 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34643
2023-06-26 16:44:12,454 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 16:44:12,454 - distributed.core - INFO - Connection to tcp://10.120.104.11:58558 has been closed.
2023-06-26 16:44:12,454 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33543', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,454 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33543
2023-06-26 16:44:12,455 - distributed.core - INFO - Connection to tcp://10.120.104.11:58480 has been closed.
2023-06-26 16:44:12,455 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46365', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,455 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46365
2023-06-26 16:44:12,455 - distributed.core - INFO - Connection to tcp://10.120.104.11:58474 has been closed.
2023-06-26 16:44:12,455 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42819', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,455 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42819
2023-06-26 16:44:12,455 - distributed.core - INFO - Connection to tcp://10.120.104.11:58466 has been closed.
2023-06-26 16:44:12,455 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34247', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,455 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34247
2023-06-26 16:44:12,455 - distributed.core - INFO - Connection to tcp://10.120.104.11:58432 has been closed.
2023-06-26 16:44:12,455 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:45701', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,455 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45701
2023-06-26 16:44:12,456 - distributed.core - INFO - Connection to tcp://10.120.104.11:58434 has been closed.
2023-06-26 16:44:12,456 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43557', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,456 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43557
2023-06-26 16:44:12,456 - distributed.core - INFO - Connection to tcp://10.120.104.11:58450 has been closed.
2023-06-26 16:44:12,456 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33131', status: running, memory: 0, processing: 0>
2023-06-26 16:44:12,456 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33131
2023-06-26 16:44:12,456 - distributed.scheduler - INFO - Lost all workers
2023-06-26 16:44:12,457 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58450>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58450>: Stream is closed
2023-06-26 16:44:12,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58558>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58466>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58474>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58434>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58434>: Stream is closed
2023-06-26 16:44:12,459 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58432>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,459 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58480>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,459 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 16:44:12,462 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 16:44:12,462 - distributed.scheduler - INFO - End scheduler
