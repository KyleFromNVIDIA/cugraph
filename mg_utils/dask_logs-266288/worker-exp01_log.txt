RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 16:39:09,571 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44211'
2023-06-26 16:39:09,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39111'
2023-06-26 16:39:09,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39219'
2023-06-26 16:39:09,579 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36263'
2023-06-26 16:39:09,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46813'
2023-06-26 16:39:09,582 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42783'
2023-06-26 16:39:09,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:32859'
2023-06-26 16:39:09,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35243'
2023-06-26 16:39:09,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39123'
2023-06-26 16:39:09,590 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46181'
2023-06-26 16:39:09,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35573'
2023-06-26 16:39:09,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:32947'
2023-06-26 16:39:09,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40761'
2023-06-26 16:39:09,599 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43285'
2023-06-26 16:39:09,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38253'
2023-06-26 16:39:09,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36847'
2023-06-26 16:39:11,094 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,094 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,175 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,175 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,200 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,215 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,272 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,278 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,278 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:39:11,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:39:11,350 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,380 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,380 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,441 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,441 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,457 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,499 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,499 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,506 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,512 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:11,535 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:39:15,591 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41563
2023-06-26 16:39:15,592 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41563
2023-06-26 16:39:15,592 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35547
2023-06-26 16:39:15,592 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:15,592 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:15,592 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:15,592 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:15,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c7to32kd
2023-06-26 16:39:15,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7244696a-62c3-4cdd-95ce-7693d987e078
2023-06-26 16:39:15,593 - distributed.worker - INFO - Starting Worker plugin RMMSetup-216d8abe-8a4e-46d9-b035-2b66bd648ce0
2023-06-26 16:39:16,520 - distributed.worker - INFO - Starting Worker plugin PreImport-d1462d86-6b28-49b8-84bf-2b4c2c69edf5
2023-06-26 16:39:16,522 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:16,559 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:16,560 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:16,572 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:17,344 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34391
2023-06-26 16:39:17,344 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34391
2023-06-26 16:39:17,344 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37965
2023-06-26 16:39:17,344 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,344 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,345 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_qv2umnp
2023-06-26 16:39:17,345 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d83b9750-b4e9-4c15-93e8-9fa390b5d2bc
2023-06-26 16:39:17,387 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43621
2023-06-26 16:39:17,387 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43621
2023-06-26 16:39:17,387 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40669
2023-06-26 16:39:17,387 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,387 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,387 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,387 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,387 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8jwwfcro
2023-06-26 16:39:17,388 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d93a6e6c-793b-45ce-99c8-eb09cd07a92b
2023-06-26 16:39:17,400 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34883
2023-06-26 16:39:17,401 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34883
2023-06-26 16:39:17,401 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42287
2023-06-26 16:39:17,401 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,401 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,401 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,401 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ux4x84in
2023-06-26 16:39:17,401 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f21ddaba-4c89-4198-b64f-9229411a6647
2023-06-26 16:39:17,415 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33183
2023-06-26 16:39:17,415 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33183
2023-06-26 16:39:17,415 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46313
2023-06-26 16:39:17,416 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,416 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,416 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,416 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,416 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4pejvg7x
2023-06-26 16:39:17,417 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eba293b1-0087-4906-9e87-fc4035e9e07d
2023-06-26 16:39:17,462 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44715
2023-06-26 16:39:17,462 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44715
2023-06-26 16:39:17,462 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36485
2023-06-26 16:39:17,462 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,462 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,462 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,462 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,462 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0z8dyu_4
2023-06-26 16:39:17,463 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dca00797-50cd-4f65-b62f-8e25ab91f943
2023-06-26 16:39:17,536 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41647
2023-06-26 16:39:17,537 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41647
2023-06-26 16:39:17,537 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45281
2023-06-26 16:39:17,537 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,537 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,537 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,537 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,537 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-669ly1fk
2023-06-26 16:39:17,538 - distributed.worker - INFO - Starting Worker plugin PreImport-09c49cb3-3005-48f8-99a8-58dd3754f04c
2023-06-26 16:39:17,538 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3ee7018-1eb6-438c-9b5a-1915dea671f6
2023-06-26 16:39:17,541 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40827
2023-06-26 16:39:17,541 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40827
2023-06-26 16:39:17,541 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45289
2023-06-26 16:39:17,541 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,541 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,541 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,541 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,541 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-avaf2ljo
2023-06-26 16:39:17,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25c96abe-f2c7-4de5-b0aa-e14cae401b38
2023-06-26 16:39:17,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cced4763-d2f9-4a15-b20e-8e4363004d17
2023-06-26 16:39:17,546 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45277
2023-06-26 16:39:17,546 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45277
2023-06-26 16:39:17,546 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33835
2023-06-26 16:39:17,546 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,546 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,547 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,547 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-101ggsga
2023-06-26 16:39:17,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4abbcffc-a396-4b8c-9c6d-75386c6557dd
2023-06-26 16:39:17,557 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37721
2023-06-26 16:39:17,557 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37721
2023-06-26 16:39:17,557 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41779
2023-06-26 16:39:17,557 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,557 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,557 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,558 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qi0em8jy
2023-06-26 16:39:17,558 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70365af5-7654-4ec0-898b-6c98edab8502
2023-06-26 16:39:17,562 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32833
2023-06-26 16:39:17,562 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32833
2023-06-26 16:39:17,562 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41749
2023-06-26 16:39:17,562 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,562 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,562 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,562 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,562 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-548tnsd2
2023-06-26 16:39:17,563 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5a9311f-a820-40bb-88b6-7d6a6b43527a
2023-06-26 16:39:17,564 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a33bb394-d2a2-464b-bab1-ea85958cca5a
2023-06-26 16:39:17,566 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45713
2023-06-26 16:39:17,567 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45713
2023-06-26 16:39:17,567 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46739
2023-06-26 16:39:17,567 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,567 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,567 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,567 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-snq0e2_2
2023-06-26 16:39:17,567 - distributed.worker - INFO - Starting Worker plugin PreImport-bdc35457-e10c-4c35-a8d1-6827b1236c2e
2023-06-26 16:39:17,567 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5451dca-fb85-4057-adc8-1097453428e5
2023-06-26 16:39:17,570 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44973
2023-06-26 16:39:17,570 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44973
2023-06-26 16:39:17,571 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46129
2023-06-26 16:39:17,571 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,571 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,571 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,571 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,571 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yu3mlz5e
2023-06-26 16:39:17,572 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b75a1db4-b1e7-491d-86ea-540644992437
2023-06-26 16:39:17,579 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39355
2023-06-26 16:39:17,579 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39355
2023-06-26 16:39:17,579 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41257
2023-06-26 16:39:17,579 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,579 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,579 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,579 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b1l8rtay
2023-06-26 16:39:17,580 - distributed.worker - INFO - Starting Worker plugin PreImport-d0f45bd9-6ef7-4c32-99ed-12209faa8448
2023-06-26 16:39:17,580 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02a336fc-80ab-4f83-b65f-37dd4a0a159b
2023-06-26 16:39:17,582 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38043
2023-06-26 16:39:17,583 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38043
2023-06-26 16:39:17,583 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36691
2023-06-26 16:39:17,583 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,583 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,583 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,583 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dr_2xebv
2023-06-26 16:39:17,585 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13076467-a69a-4a67-9ce6-a0179339d029
2023-06-26 16:39:17,591 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34199
2023-06-26 16:39:17,592 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34199
2023-06-26 16:39:17,592 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43501
2023-06-26 16:39:17,592 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:39:17,592 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:17,592 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:39:17,592 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:39:17,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pk_cgks3
2023-06-26 16:39:17,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a5771e8-4333-49bf-96d5-4067c4a9a2f3
2023-06-26 16:39:19,281 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:19,285 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:20,864 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:21,039 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d501eb1-f0dd-4075-ba7d-50aa80eb31dc
2023-06-26 16:39:21,040 - distributed.worker - INFO - Starting Worker plugin PreImport-a0f1335a-9ff7-4f45-8e73-b49d77d9676d
2023-06-26 16:39:21,041 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,062 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,071 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6df4f561-5e99-4a59-9e22-2c9183a9a4a7
2023-06-26 16:39:21,191 - distributed.worker - INFO - Starting Worker plugin PreImport-00dd7613-58b1-44b0-bc80-a05abbe90d80
2023-06-26 16:39:21,199 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,223 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,223 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,230 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,266 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35fb593b-a1c3-4acd-8339-7005fbbf3393
2023-06-26 16:39:21,266 - distributed.worker - INFO - Starting Worker plugin PreImport-0c8f53f2-b729-4238-8bbc-b87ef11a7474
2023-06-26 16:39:21,267 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,291 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,292 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,297 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,349 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-854666a7-6e95-4550-8a16-bfbb30813bf6
2023-06-26 16:39:21,349 - distributed.worker - INFO - Starting Worker plugin PreImport-7ebc8407-77ec-44dd-8cc7-a95124da3758
2023-06-26 16:39:21,350 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,350 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82ef6b0f-bd99-46c0-a0b8-00c6593e620b
2023-06-26 16:39:21,351 - distributed.worker - INFO - Starting Worker plugin PreImport-6e40b2a1-ee13-45b6-b62e-a69c69b414ae
2023-06-26 16:39:21,353 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,363 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,363 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,367 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,375 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8367fdb5-1760-4b27-a3d1-6555ffa38174
2023-06-26 16:39:21,376 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,384 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,384 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,387 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,390 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,390 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,395 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,410 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4786e66-de48-4ca1-a492-861eec75972f
2023-06-26 16:39:21,412 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,423 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,423 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-553563ce-b7d2-4494-8177-b63038feaead
2023-06-26 16:39:21,424 - distributed.worker - INFO - Starting Worker plugin PreImport-97aca63f-8999-4b41-a743-88c11d78845e
2023-06-26 16:39:21,425 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,427 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,437 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,437 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,438 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40b99e98-c6a9-44f8-a139-fef96f28028b
2023-06-26 16:39:21,439 - distributed.worker - INFO - Starting Worker plugin PreImport-deba76e5-ac45-434f-820e-287bcb16e4d3
2023-06-26 16:39:21,440 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,441 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,450 - distributed.worker - INFO - Starting Worker plugin PreImport-54ce7add-2169-4ad2-ab6a-be285ada8cd2
2023-06-26 16:39:21,452 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,452 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,452 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,453 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-325df269-343c-4740-8ae8-f9256c8925b5
2023-06-26 16:39:21,453 - distributed.worker - INFO - Starting Worker plugin PreImport-5b7c8761-7de3-4a6e-a4fb-a21d993ed24d
2023-06-26 16:39:21,454 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,457 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,458 - distributed.worker - WARNING - Run Failed
Function: _func_init_all
args:     (b"K[\x10\x1fPJMj\x9d'\x84nu#\x14\xc1", b'\xac\xb9@\x807\x88\xd5\xbb\x02\x00\xc5\xe9\n!\xe4F\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00`\x84e\xbb\xeb\x7f\x00\x00p\xdcIP\xec\x7f\x00\x00\xb0\x89e\xbb\xeb\x7f\x00\x00\xb8\x89e\xbb\xeb\x7f\x00\x00 \xb6\xad\xe6$V\x00\x00pQ\xb7\xcd\xeb\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xad\x9e\xfe\xe4$V\x00\x00 \x92@\xcd\xeb\x7f\x00\x00\x00\x87\xd8Fy\xa0\x1f', True, {'tcp://10.120.104.11:41563': {'rank': 13, 'port': 39011}}, False, 0)
kwargs:   {'dask_worker': <Worker 'tcp://10.120.104.11:41563', status: running, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>}
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 3281, in run
    result = await function(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/raft_dask/common/comms.py", line 446, in _func_init_all
    _func_init_nccl(sessionId, uniqueId, dask_worker=dask_worker)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/raft_dask/common/comms.py", line 511, in _func_init_nccl
    n.init(nWorkers, uniqueId, wid)
  File "nccl.pyx", line 151, in raft_dask.common.nccl.nccl.init
RuntimeError: NCCL_ERROR: b'invalid argument (run with NCCL_DEBUG=WARN for details)'
2023-06-26 16:39:21,466 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,466 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,470 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,470 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,471 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,476 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94c0d154-ceb1-4533-8cf7-f105c1dadad7
2023-06-26 16:39:21,477 - distributed.worker - INFO - Starting Worker plugin PreImport-1cf63e1c-b029-4bc7-a564-306751a85048
2023-06-26 16:39:21,477 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,478 - distributed.worker - INFO - Starting Worker plugin PreImport-954260ad-52c7-4c5d-85bf-aefe038d0ea1
2023-06-26 16:39:21,478 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,479 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,486 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ebe4d56-d6f8-449a-8cba-72f4fdd9f319
2023-06-26 16:39:21,487 - distributed.worker - INFO - Starting Worker plugin PreImport-75417486-f04b-4e7f-9db4-0be6ed8293e3
2023-06-26 16:39:21,488 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61620745-6bd0-4d18-9fd7-83b82f762302
2023-06-26 16:39:21,488 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,489 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,498 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,498 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,498 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,498 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,499 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,499 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,501 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,502 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:39:21,502 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:39:21,503 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,505 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:21,509 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:39:47,852 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,858 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,858 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,858 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,858 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,858 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,858 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,859 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,859 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,860 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,860 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,861 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,864 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,864 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,865 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,867 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,878 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,889 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,890 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,890 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,890 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,890 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,890 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,890 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:47,890 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:39:51,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:02,591 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:02,761 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:02,840 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:02,847 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:02,862 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:02,868 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:02,958 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,021 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,105 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,144 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,165 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,200 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,285 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,286 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,358 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:03,529 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:40:09,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:09,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:48,748 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,749 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,750 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,755 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,755 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,756 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,756 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,756 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,756 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,756 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,756 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,757 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,759 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,760 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,760 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:48,760 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:40:51,898 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,905 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,905 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,905 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,905 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,905 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,905 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,905 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,905 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,906 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,906 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,906 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,906 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:51,907 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:40:52,265 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-69debbf01dc802f51eba5c6cafbfd8f3-1028a8ea3f8e05241bd0f91fd9e698ae', 3)
Function:  subgraph_callable-6cf908b8-a4de-4ec5-b0fe-d929e135
args:      ("('__filter_batches-9e3f1cbb113d5c1295fa52a071797485', 3)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:40:52,265 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-69debbf01dc802f51eba5c6cafbfd8f3-1028a8ea3f8e05241bd0f91fd9e698ae', 0)
Function:  subgraph_callable-6cf908b8-a4de-4ec5-b0fe-d929e135
args:      ("('__filter_batches-9e3f1cbb113d5c1295fa52a071797485', 0)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:40:52,265 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-69debbf01dc802f51eba5c6cafbfd8f3-1028a8ea3f8e05241bd0f91fd9e698ae', 1)
Function:  subgraph_callable-6cf908b8-a4de-4ec5-b0fe-d929e135
args:      ("('__filter_batches-9e3f1cbb113d5c1295fa52a071797485', 1)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:40:52,266 - distributed.worker - WARNING - Compute Failed
Key:       ('len-chunk-69debbf01dc802f51eba5c6cafbfd8f3-1028a8ea3f8e05241bd0f91fd9e698ae', 2)
Function:  subgraph_callable-6cf908b8-a4de-4ec5-b0fe-d929e135
args:      ("('__filter_batches-9e3f1cbb113d5c1295fa52a071797485', 2)", '_BATCH_')
kwargs:    {}
Exception: "TypeError('string indices must be integers')"

2023-06-26 16:40:52,272 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,278 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,279 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,279 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,279 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:52,279 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:40:56,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:40:56,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,708 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32833. Reason: scheduler-restart
2023-06-26 16:40:56,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,708 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33183. Reason: scheduler-restart
2023-06-26 16:40:56,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,709 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,709 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34199. Reason: scheduler-restart
2023-06-26 16:40:56,709 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,709 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,709 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34391. Reason: scheduler-restart
2023-06-26 16:40:56,709 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,710 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34883. Reason: scheduler-restart
2023-06-26 16:40:56,710 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37721. Reason: scheduler-restart
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,711 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32833
2023-06-26 16:40:56,711 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,711 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,711 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38043. Reason: scheduler-restart
2023-06-26 16:40:56,711 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,711 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,712 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,712 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39355. Reason: scheduler-restart
2023-06-26 16:40:56,712 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41563. Reason: scheduler-restart
2023-06-26 16:40:56,712 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40827. Reason: scheduler-restart
2023-06-26 16:40:56,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,713 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41647. Reason: scheduler-restart
2023-06-26 16:40:56,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,713 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43621. Reason: scheduler-restart
2023-06-26 16:40:56,713 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,713 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,713 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,713 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,713 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,714 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,714 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,714 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,714 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,714 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,715 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,715 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,715 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:40:56,717 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45277. Reason: scheduler-restart
2023-06-26 16:40:56,718 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44973. Reason: scheduler-restart
2023-06-26 16:40:56,719 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33183
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34199
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34391
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34883
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37721
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38043
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40827
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41563
2023-06-26 16:40:56,719 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41647
2023-06-26 16:40:56,720 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,720 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44715. Reason: scheduler-restart
2023-06-26 16:40:56,721 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33183
2023-06-26 16:40:56,721 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34199
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34391
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34883
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37721
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38043
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40827
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41563
2023-06-26 16:40:56,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41647
2023-06-26 16:40:56,722 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,725 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33183
2023-06-26 16:40:56,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34199
2023-06-26 16:40:56,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34391
2023-06-26 16:40:56,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34883
2023-06-26 16:40:56,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37721
2023-06-26 16:40:56,733 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38043
2023-06-26 16:40:56,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 16:40:56,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40827
2023-06-26 16:40:56,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41563
2023-06-26 16:40:56,733 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41647
2023-06-26 16:40:56,734 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44973
2023-06-26 16:40:56,734 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43621
2023-06-26 16:40:56,734 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,740 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33183
2023-06-26 16:40:56,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34199
2023-06-26 16:40:56,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34391
2023-06-26 16:40:56,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34883
2023-06-26 16:40:56,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37721
2023-06-26 16:40:56,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38043
2023-06-26 16:40:56,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 16:40:56,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40827
2023-06-26 16:40:56,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41563
2023-06-26 16:40:56,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41647
2023-06-26 16:40:56,743 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44973
2023-06-26 16:40:56,744 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43621
2023-06-26 16:40:56,744 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44715
2023-06-26 16:40:56,744 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,745 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45713. Reason: scheduler-restart
2023-06-26 16:40:56,745 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,746 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:56,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33183
2023-06-26 16:40:56,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34199
2023-06-26 16:40:56,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34391
2023-06-26 16:40:56,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34883
2023-06-26 16:40:56,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37721
2023-06-26 16:40:56,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38043
2023-06-26 16:40:56,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39355
2023-06-26 16:40:56,753 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40827
2023-06-26 16:40:56,754 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41563
2023-06-26 16:40:56,754 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41647
2023-06-26 16:40:56,754 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44973
2023-06-26 16:40:56,754 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43621
2023-06-26 16:40:56,754 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44715
2023-06-26 16:40:56,755 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45277
2023-06-26 16:40:56,755 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:40:56,764 - distributed.nanny - INFO - Worker closed
2023-06-26 16:40:59,351 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:40:59,880 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:01,195 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:01,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:01,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:01,571 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:01,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:02,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:02,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:02,683 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:02,684 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:02,687 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:02,689 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:02,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:04,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:04,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:04,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:04,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:04,782 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:04,806 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:04,807 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:04,807 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:04,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:04,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:04,810 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:04,811 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:04,813 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:04,816 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:04,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:04,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:04,822 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:04,823 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:41:04,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:04,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:04,943 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:04,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:04,977 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:04,991 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:05,000 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:05,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:06,406 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:06,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:06,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:06,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:06,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:06,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:06,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:06,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:06,463 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:06,463 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:06,464 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:06,464 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:06,472 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:06,472 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:06,491 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:41:06,491 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:41:06,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:06,642 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:06,642 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:06,642 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:06,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:06,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:06,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:06,665 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:41:07,044 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33723
2023-06-26 16:41:07,044 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33723
2023-06-26 16:41:07,044 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42563
2023-06-26 16:41:07,044 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:07,044 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:07,044 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:07,045 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:07,045 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1yunr2cw
2023-06-26 16:41:07,045 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9090dd9b-3210-4da4-b476-3104d50e9f86
2023-06-26 16:41:07,215 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39393
2023-06-26 16:41:07,215 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39393
2023-06-26 16:41:07,215 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44467
2023-06-26 16:41:07,215 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:07,215 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:07,215 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:07,216 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:07,216 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f33mkei9
2023-06-26 16:41:07,216 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79fa5654-8115-473c-9b03-953d21c53490
2023-06-26 16:41:07,216 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8a586ea-4c67-4afe-b63b-2044548c7519
2023-06-26 16:41:08,832 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45701
2023-06-26 16:41:08,832 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45701
2023-06-26 16:41:08,832 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35435
2023-06-26 16:41:08,832 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:08,832 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:08,832 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:08,833 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:08,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gpn07q0p
2023-06-26 16:41:08,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efa69595-eba2-4751-9996-41aac5298561
2023-06-26 16:41:08,838 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33131
2023-06-26 16:41:08,838 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33131
2023-06-26 16:41:08,838 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45497
2023-06-26 16:41:08,838 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:08,838 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:08,839 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:08,839 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:08,839 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jbsg6yvn
2023-06-26 16:41:08,839 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4fea790-6c28-4cc5-9677-ecc16a1c3585
2023-06-26 16:41:08,862 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43557
2023-06-26 16:41:08,862 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43557
2023-06-26 16:41:08,862 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39815
2023-06-26 16:41:08,862 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:08,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:08,862 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:08,862 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34247
2023-06-26 16:41:08,863 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:08,863 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34247
2023-06-26 16:41:08,863 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fmp4o0v9
2023-06-26 16:41:08,863 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37845
2023-06-26 16:41:08,863 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:08,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:08,863 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:08,863 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:08,863 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ywpbt5j
2023-06-26 16:41:08,863 - distributed.worker - INFO - Starting Worker plugin PreImport-d15f778b-afd4-4da5-bbac-bcc8a7b56b45
2023-06-26 16:41:08,863 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1dd7fc4-6df1-4be0-8fd2-eb63dd677fac
2023-06-26 16:41:08,863 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a96f935e-021f-44a7-b302-b62305e48519
2023-06-26 16:41:08,895 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46365
2023-06-26 16:41:08,895 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46365
2023-06-26 16:41:08,895 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37323
2023-06-26 16:41:08,895 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:08,895 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:08,895 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:08,895 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:08,895 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j3iyoj2_
2023-06-26 16:41:08,896 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c9a4279f-c1f7-404f-9a73-2ff7766feb1c
2023-06-26 16:41:08,906 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42819
2023-06-26 16:41:08,907 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42819
2023-06-26 16:41:08,907 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41715
2023-06-26 16:41:08,907 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:08,907 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:08,907 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:08,907 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:08,907 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l2ogquy_
2023-06-26 16:41:08,908 - distributed.worker - INFO - Starting Worker plugin PreImport-540a7be2-2a77-4f80-811c-c39da84d64bf
2023-06-26 16:41:08,908 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37ca5f2c-15c5-4c73-8198-24d522dce13b
2023-06-26 16:41:09,607 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-36b144cc-ceff-46ab-8c36-fa3f7108ef9a
2023-06-26 16:41:09,607 - distributed.worker - INFO - Starting Worker plugin PreImport-5ef5e450-3708-4bb3-8536-5cb92c700a04
2023-06-26 16:41:09,608 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:09,620 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:09,620 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:09,625 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:09,674 - distributed.worker - INFO - Starting Worker plugin PreImport-3aff1d7f-d15f-48d8-80d7-6e8120afeb88
2023-06-26 16:41:09,676 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:09,692 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:09,692 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:09,695 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:12,307 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1610b358-5f86-4bc8-93b1-7887915ec2e9
2023-06-26 16:41:12,308 - distributed.worker - INFO - Starting Worker plugin PreImport-4b4a562d-1252-4d6d-87b8-efdb93d7d46c
2023-06-26 16:41:12,310 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,325 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:12,325 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,328 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:12,333 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64ed366b-f2b2-4bc8-93ab-9a1962dac42f
2023-06-26 16:41:12,335 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,340 - distributed.worker - INFO - Starting Worker plugin PreImport-27fcf5fd-9eff-4cf8-918f-b7bcf480a153
2023-06-26 16:41:12,340 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8560d4fc-d39d-4408-9950-079fd489247c
2023-06-26 16:41:12,341 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc3601fc-2853-485e-84b9-cea5817604d6
2023-06-26 16:41:12,343 - distributed.worker - INFO - Starting Worker plugin PreImport-29447297-ad6b-4cec-9864-4f5bcf1f60f4
2023-06-26 16:41:12,343 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-426a9a04-1902-4d69-8720-8d9bf0ed0b93
2023-06-26 16:41:12,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,350 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:12,350 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,352 - distributed.worker - INFO - Starting Worker plugin PreImport-45d5e605-46de-45a1-b89f-001f148d6129
2023-06-26 16:41:12,352 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1121b36a-fc4c-42f8-ace3-2d3a4db8f439
2023-06-26 16:41:12,353 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,353 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:12,353 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:12,353 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,354 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:12,354 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,355 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:12,355 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:12,356 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:12,356 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,359 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:12,362 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:12,362 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:12,364 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:13,237 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33543
2023-06-26 16:41:13,237 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33543
2023-06-26 16:41:13,237 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37425
2023-06-26 16:41:13,237 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:13,237 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:13,237 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:13,237 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:13,237 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yxvjfl18
2023-06-26 16:41:13,238 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20e308fe-95bf-47cb-ac42-f1ae87cb2cfe
2023-06-26 16:41:13,307 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34643
2023-06-26 16:41:13,307 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34643
2023-06-26 16:41:13,307 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35357
2023-06-26 16:41:13,307 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:13,307 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:13,307 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:13,307 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:13,307 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p27eeyau
2023-06-26 16:41:13,308 - distributed.worker - INFO - Starting Worker plugin PreImport-4b0e982a-8bb6-44a1-b4b3-2671b4905645
2023-06-26 16:41:13,309 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1cc4c0e7-ff00-4857-b4db-be681170d8c8
2023-06-26 16:41:13,544 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35421
2023-06-26 16:41:13,544 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35421
2023-06-26 16:41:13,544 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40613
2023-06-26 16:41:13,544 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:13,544 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:13,545 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:13,545 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:13,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p9f9d7kx
2023-06-26 16:41:13,545 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2d5f204-5ad1-41a1-b5ca-afb6a7e5a511
2023-06-26 16:41:13,575 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42947
2023-06-26 16:41:13,575 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42947
2023-06-26 16:41:13,575 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41249
2023-06-26 16:41:13,575 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:13,576 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:13,576 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:13,576 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:13,576 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-87vmepvi
2023-06-26 16:41:13,576 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0d5c46f-81d7-4e7a-94b1-19da2bb7dc56
2023-06-26 16:41:13,590 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46317
2023-06-26 16:41:13,591 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46317
2023-06-26 16:41:13,591 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42565
2023-06-26 16:41:13,591 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:13,591 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:13,591 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:13,591 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:13,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xx8pvta7
2023-06-26 16:41:13,591 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42173
2023-06-26 16:41:13,591 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42173
2023-06-26 16:41:13,591 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46185
2023-06-26 16:41:13,591 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:13,592 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:13,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a82ee9ad-7b7b-4903-ac1a-b26f92034122
2023-06-26 16:41:13,592 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:13,592 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:13,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xlrqamh3
2023-06-26 16:41:13,591 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34131
2023-06-26 16:41:13,592 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34131
2023-06-26 16:41:13,592 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41405
2023-06-26 16:41:13,592 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:13,591 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36091
2023-06-26 16:41:13,592 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:13,592 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:13,592 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36091
2023-06-26 16:41:13,592 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:13,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c6pmx682
2023-06-26 16:41:13,592 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44661
2023-06-26 16:41:13,592 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:41:13,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-848eb607-62df-4f3b-afdc-444dea633f4a
2023-06-26 16:41:13,592 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:13,592 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:41:13,592 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:41:13,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-43iy_ghz
2023-06-26 16:41:13,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d8b57c6-9586-46a8-aac3-69ada5d45f0c
2023-06-26 16:41:13,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94dd2d4a-fd3d-45e5-8499-aba0be2d9346
2023-06-26 16:41:15,083 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bebadc88-f6fc-4ab2-ab31-193cd5b72633
2023-06-26 16:41:15,084 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,101 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:15,101 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,102 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:15,107 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-427d7f40-1376-49c5-9b88-213365308328
2023-06-26 16:41:15,107 - distributed.worker - INFO - Starting Worker plugin PreImport-80ba97e6-49c9-450a-936a-c8881e4c693b
2023-06-26 16:41:15,108 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,121 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:15,121 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,123 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:15,377 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11febcc0-7833-4545-b6e1-982b9ef7f9a3
2023-06-26 16:41:15,377 - distributed.worker - INFO - Starting Worker plugin PreImport-0613355c-6bdc-4156-a8b9-124a49c9911f
2023-06-26 16:41:15,378 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,398 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:15,398 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,401 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:15,481 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-66d788d8-9fa3-468b-a6ae-1dc964f398e9
2023-06-26 16:41:15,482 - distributed.worker - INFO - Starting Worker plugin PreImport-1fce57b4-ff56-4cba-b05c-7c017b0a6f1b
2023-06-26 16:41:15,483 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6ac8d11-e553-403e-89f7-f29cf09cfee1
2023-06-26 16:41:15,500 - distributed.worker - INFO - Starting Worker plugin PreImport-ef5f0fa6-f97e-4200-869a-fc35fcfc335f
2023-06-26 16:41:15,502 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,502 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:15,502 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,505 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55430540-d292-464a-8132-efd1077f8f0d
2023-06-26 16:41:15,505 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:15,505 - distributed.worker - INFO - Starting Worker plugin PreImport-870e0887-a8d4-4316-86af-3b8afda73827
2023-06-26 16:41:15,505 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17b2a19a-e5ae-4a6c-8d73-ec23afaf4a6f
2023-06-26 16:41:15,508 - distributed.worker - INFO - Starting Worker plugin PreImport-c5c9cfa1-753c-48fd-b55b-483b305643bb
2023-06-26 16:41:15,509 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,513 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5dd7a661-7a49-49b3-b44a-703bd3d7f451
2023-06-26 16:41:15,513 - distributed.worker - INFO - Starting Worker plugin PreImport-ea1da888-f77b-4c4d-ac47-c10c83aab427
2023-06-26 16:41:15,514 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,518 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:15,518 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,519 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:15,522 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:15,522 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,525 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:15,526 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:15,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,529 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:15,533 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:41:15,533 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:41:15,536 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:41:24,715 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:24,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:24,865 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:24,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:24,922 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:24,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:24,999 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,129 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,139 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,179 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,225 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,253 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,307 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,310 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,343 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,347 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,377 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,444 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,680 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:41:25,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:25,692 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,692 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,692 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,692 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,693 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:41:25,703 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,703 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687797685.704757] [exp01:269621:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:51445) failed: Address already in use
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,704 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:41:25,716 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,716 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:25,717 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:41:28,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:34,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:38,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:38,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:39,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:43,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:43,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:41:43,203 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,203 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:43,204 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:41:55,030 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,030 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,030 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,030 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,030 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,030 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:41:55,031 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42173. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35421. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42947. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34131. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36091. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39393. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46365. Reason: worker-close
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42819. Reason: worker-close
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34247. Reason: worker-close
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45701. Reason: worker-close
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43557. Reason: worker-close
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33131. Reason: worker-close
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46317. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33723. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34643. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,449 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33543. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:44:12,450 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44211'. Reason: nanny-close
2023-06-26 16:44:12,451 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,451 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39111'. Reason: nanny-close
2023-06-26 16:44:12,450 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:58480 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,450 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:58474 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,450 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:58466 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,450 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:58432 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,452 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,452 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35243'. Reason: nanny-close
2023-06-26 16:44:12,452 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,453 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39123'. Reason: nanny-close
2023-06-26 16:44:12,453 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,453 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46181'. Reason: nanny-close
2023-06-26 16:44:12,453 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,454 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35573'. Reason: nanny-close
2023-06-26 16:44:12,451 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:58450 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,451 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:58434 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:44:12,454 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,454 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:32947'. Reason: nanny-close
2023-06-26 16:44:12,454 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,455 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40761'. Reason: nanny-close
2023-06-26 16:44:12,455 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,455 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43285'. Reason: nanny-close
2023-06-26 16:44:12,455 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,456 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38253'. Reason: nanny-close
2023-06-26 16:44:12,456 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,456 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39219'. Reason: nanny-close
2023-06-26 16:44:12,456 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,457 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36263'. Reason: nanny-close
2023-06-26 16:44:12,457 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,457 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46813'. Reason: nanny-close
2023-06-26 16:44:12,457 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42783'. Reason: nanny-close
2023-06-26 16:44:12,458 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:32859'. Reason: nanny-close
2023-06-26 16:44:12,458 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36847'. Reason: nanny-close
2023-06-26 16:44:12,459 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:44:12,467 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46181 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60496 remote=tcp://10.120.104.11:46181>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46181 after 100 s
2023-06-26 16:44:12,469 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39123 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60106 remote=tcp://10.120.104.11:39123>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39123 after 100 s
2023-06-26 16:44:12,470 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42783 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:38722 remote=tcp://10.120.104.11:42783>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42783 after 100 s
2023-06-26 16:44:12,472 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43285 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46726 remote=tcp://10.120.104.11:43285>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43285 after 100 s
2023-06-26 16:44:12,472 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39111 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55246 remote=tcp://10.120.104.11:39111>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39111 after 100 s
2023-06-26 16:44:12,473 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39219 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:53884 remote=tcp://10.120.104.11:39219>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39219 after 100 s
2023-06-26 16:44:12,474 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35573 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59942 remote=tcp://10.120.104.11:35573>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35573 after 100 s
2023-06-26 16:44:12,474 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40761 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41856 remote=tcp://10.120.104.11:40761>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40761 after 100 s
2023-06-26 16:44:12,474 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:32947 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40932 remote=tcp://10.120.104.11:32947>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:32947 after 100 s
2023-06-26 16:44:12,476 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38253 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46492 remote=tcp://10.120.104.11:38253>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38253 after 100 s
2023-06-26 16:44:12,477 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46813 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:43588 remote=tcp://10.120.104.11:46813>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46813 after 100 s
2023-06-26 16:44:12,477 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36263 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42608 remote=tcp://10.120.104.11:36263>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36263 after 100 s
2023-06-26 16:44:12,477 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36847 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:55612 remote=tcp://10.120.104.11:36847>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36847 after 100 s
2023-06-26 16:44:12,479 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:32859 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52980 remote=tcp://10.120.104.11:32859>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:32859 after 100 s
2023-06-26 16:44:15,660 - distributed.nanny - WARNING - Worker process still alive after 3.1999975585937506 seconds, killing
2023-06-26 16:44:15,662 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:44:15,662 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 16:44:15,663 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,663 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,664 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 16:44:15,665 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,665 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,666 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,666 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,667 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,667 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,668 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,669 - distributed.nanny - WARNING - Worker process still alive after 3.19999984741211 seconds, killing
2023-06-26 16:44:15,670 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:44:15,671 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:44:16,452 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,453 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,454 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,454 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,454 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,454 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,455 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,455 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,455 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,457 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,457 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,457 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,458 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,458 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,459 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,459 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269632 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269629 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269626 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269621 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269618 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269615 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269610 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269605 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269561 parent=266449 started daemon>
2023-06-26 16:44:16,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269558 parent=266449 started daemon>
2023-06-26 16:44:16,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269555 parent=266449 started daemon>
2023-06-26 16:44:16,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269552 parent=266449 started daemon>
2023-06-26 16:44:16,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269522 parent=266449 started daemon>
2023-06-26 16:44:16,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269510 parent=266449 started daemon>
2023-06-26 16:44:16,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269498 parent=266449 started daemon>
2023-06-26 16:44:16,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=269487 parent=266449 started daemon>
2023-06-26 16:44:22,285 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 269629 exit status was already read will report exitcode 255
2023-06-26 16:44:22,533 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 269498 exit status was already read will report exitcode 255
2023-06-26 16:44:23,289 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 269510 exit status was already read will report exitcode 255
