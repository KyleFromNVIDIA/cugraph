RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 17:10:21,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40197'
2023-06-26 17:10:21,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35357'
2023-06-26 17:10:21,671 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36621'
2023-06-26 17:10:21,675 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:32831'
2023-06-26 17:10:21,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42647'
2023-06-26 17:10:21,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34347'
2023-06-26 17:10:21,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46613'
2023-06-26 17:10:21,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40949'
2023-06-26 17:10:21,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42269'
2023-06-26 17:10:21,686 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34593'
2023-06-26 17:10:21,689 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45195'
2023-06-26 17:10:21,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41715'
2023-06-26 17:10:21,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42659'
2023-06-26 17:10:21,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39139'
2023-06-26 17:10:21,698 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33071'
2023-06-26 17:10:21,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38303'
2023-06-26 17:10:23,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,434 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,449 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,449 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,511 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,511 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,553 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,561 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,561 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,566 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,566 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:10:23,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:10:23,611 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,611 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,648 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,688 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,721 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,732 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,743 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,747 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,749 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:23,764 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:10:29,455 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42701
2023-06-26 17:10:29,455 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42701
2023-06-26 17:10:29,455 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45473
2023-06-26 17:10:29,455 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:29,455 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:29,455 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:29,455 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:29,455 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n5f6vt91
2023-06-26 17:10:29,456 - distributed.worker - INFO - Starting Worker plugin RMMSetup-431be4ba-67e5-4751-b8ec-c3264f9d75b3
2023-06-26 17:10:29,798 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39449
2023-06-26 17:10:29,798 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39449
2023-06-26 17:10:29,798 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44025
2023-06-26 17:10:29,798 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:29,798 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:29,798 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:29,798 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:29,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-21nwq6hq
2023-06-26 17:10:29,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd5f564d-2861-4f17-b936-60a203fd0214
2023-06-26 17:10:29,803 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33227
2023-06-26 17:10:29,803 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33227
2023-06-26 17:10:29,803 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33853
2023-06-26 17:10:29,803 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:29,803 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:29,803 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:29,803 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:29,803 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ony4oixb
2023-06-26 17:10:29,803 - distributed.worker - INFO - Starting Worker plugin PreImport-989db405-e92e-42b2-8500-b49e77851550
2023-06-26 17:10:29,804 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9dfe852e-7b65-444c-8f70-901157194d5d
2023-06-26 17:10:29,840 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35079
2023-06-26 17:10:29,840 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35079
2023-06-26 17:10:29,840 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42561
2023-06-26 17:10:29,840 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:29,840 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:29,840 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:29,840 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:29,840 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aasfcqnw
2023-06-26 17:10:29,841 - distributed.worker - INFO - Starting Worker plugin PreImport-db467bf9-8764-476c-b645-e567f797ca1c
2023-06-26 17:10:29,841 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48c25de8-b721-4bf7-9e8e-fd7787a45de9
2023-06-26 17:10:29,988 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39041
2023-06-26 17:10:29,989 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39041
2023-06-26 17:10:29,989 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45987
2023-06-26 17:10:29,989 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:29,989 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:29,989 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:29,989 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:29,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0bkpivqo
2023-06-26 17:10:29,990 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84c3ba61-519a-45af-84eb-6eb9c8dab8c2
2023-06-26 17:10:30,117 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37199
2023-06-26 17:10:30,117 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37199
2023-06-26 17:10:30,117 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43425
2023-06-26 17:10:30,117 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,117 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,117 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,117 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,117 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ioplzqb6
2023-06-26 17:10:30,118 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c74e35ce-395d-4efa-9ed1-719671253786
2023-06-26 17:10:30,118 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2fee0032-145e-4ba7-993b-ba21925d2b22
2023-06-26 17:10:30,122 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40643
2023-06-26 17:10:30,123 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40643
2023-06-26 17:10:30,123 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35433
2023-06-26 17:10:30,123 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,123 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,123 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,123 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,123 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cvhvgdtu
2023-06-26 17:10:30,123 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2afbbc85-7641-4972-8bda-b7f3e3f375aa
2023-06-26 17:10:30,124 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e7c4c6b-9afb-4c40-9955-65dcd2964220
2023-06-26 17:10:30,127 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40523
2023-06-26 17:10:30,127 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40523
2023-06-26 17:10:30,127 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38673
2023-06-26 17:10:30,128 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,128 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,128 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,128 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,128 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a58iud04
2023-06-26 17:10:30,129 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0dfb5590-a2cb-4f81-a831-ec1d5685b82e
2023-06-26 17:10:30,136 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35379
2023-06-26 17:10:30,136 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35379
2023-06-26 17:10:30,136 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44921
2023-06-26 17:10:30,136 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,136 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,136 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,136 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cli3vsmj
2023-06-26 17:10:30,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7063723d-6eb0-4e4b-935b-923b50a580a3
2023-06-26 17:10:30,137 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7b5f476-619a-4cd5-832d-4be43632e0dc
2023-06-26 17:10:30,151 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36153
2023-06-26 17:10:30,151 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36153
2023-06-26 17:10:30,151 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42569
2023-06-26 17:10:30,151 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,151 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,151 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,151 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,151 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nvbr3aie
2023-06-26 17:10:30,152 - distributed.worker - INFO - Starting Worker plugin RMMSetup-466727c1-d805-42e0-b6c1-b2d16740f436
2023-06-26 17:10:30,163 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35615
2023-06-26 17:10:30,163 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35615
2023-06-26 17:10:30,163 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44905
2023-06-26 17:10:30,164 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,164 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,164 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,164 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x2gctp1q
2023-06-26 17:10:30,165 - distributed.worker - INFO - Starting Worker plugin PreImport-86a94208-fa3b-45e4-a616-6e99651914ac
2023-06-26 17:10:30,165 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b7be67fd-0f4b-4fb3-9df9-bd26d82ad77d
2023-06-26 17:10:30,166 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33775
2023-06-26 17:10:30,166 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33775
2023-06-26 17:10:30,166 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46873
2023-06-26 17:10:30,166 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,166 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,166 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,166 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,166 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-az1nggq5
2023-06-26 17:10:30,167 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1febde7b-3bbf-417d-9167-4b7a1f795007
2023-06-26 17:10:30,170 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33349
2023-06-26 17:10:30,170 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33349
2023-06-26 17:10:30,170 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43123
2023-06-26 17:10:30,170 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,170 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,170 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,170 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,170 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sphfa_pp
2023-06-26 17:10:30,171 - distributed.worker - INFO - Starting Worker plugin RMMSetup-207c8c6e-241c-4c15-8e47-dbf76fbd41ce
2023-06-26 17:10:30,174 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41801
2023-06-26 17:10:30,174 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41801
2023-06-26 17:10:30,174 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38019
2023-06-26 17:10:30,174 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,174 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,174 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,174 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,174 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ujv5fpdz
2023-06-26 17:10:30,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd0bcb76-4d26-4196-9321-5227196e9aeb
2023-06-26 17:10:30,183 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36051
2023-06-26 17:10:30,183 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36051
2023-06-26 17:10:30,183 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35157
2023-06-26 17:10:30,183 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,183 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,183 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,183 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,184 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xbblonx8
2023-06-26 17:10:30,184 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b751bbcb-a814-412f-96c6-9b44e145b29d
2023-06-26 17:10:30,188 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41051
2023-06-26 17:10:30,189 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41051
2023-06-26 17:10:30,189 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36899
2023-06-26 17:10:30,189 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:10:30,189 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:30,189 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:10:30,189 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:10:30,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fg5157hv
2023-06-26 17:10:30,189 - distributed.worker - INFO - Starting Worker plugin RMMSetup-edf1e4d0-23dd-401a-aa13-dbed7f04e682
2023-06-26 17:10:33,437 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-463f81b3-c85f-44a9-94e5-54a66d135a0c
2023-06-26 17:10:33,438 - distributed.worker - INFO - Starting Worker plugin PreImport-e788c6c5-784e-41f6-8d09-ec64b123ad29
2023-06-26 17:10:33,439 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,444 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3220bda3-63e3-4a13-92f0-0330f6c93598
2023-06-26 17:10:33,445 - distributed.worker - INFO - Starting Worker plugin PreImport-f4796635-5c08-435e-bfc5-9f2dc2e99043
2023-06-26 17:10:33,446 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,456 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,456 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,457 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,466 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,466 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,474 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,637 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9e3aabe-da1a-4c7e-b4fd-1c35e937a55f
2023-06-26 17:10:33,637 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,655 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,655 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,656 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,706 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30ee5b14-f3ee-4baf-933e-830a35a662d9
2023-06-26 17:10:33,706 - distributed.worker - INFO - Starting Worker plugin PreImport-574d60f1-8b1f-409e-9101-41dcb5c90f43
2023-06-26 17:10:33,707 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,724 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-459e757c-aa93-4f80-86f1-274a4ef984b0
2023-06-26 17:10:33,724 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,724 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,725 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,726 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,741 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,741 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,745 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,841 - distributed.worker - INFO - Starting Worker plugin PreImport-c0e23f63-281a-4534-a7e2-bdc048ad2473
2023-06-26 17:10:33,842 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,875 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,875 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,877 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,880 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b1387fb-2e3b-42cf-adc6-f27be6a11fe0
2023-06-26 17:10:33,881 - distributed.worker - INFO - Starting Worker plugin PreImport-f08e0f87-a807-47b2-bffe-478f22b422b9
2023-06-26 17:10:33,881 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,882 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-92534c2f-8433-4e46-824e-62a83ba61cd9
2023-06-26 17:10:33,882 - distributed.worker - INFO - Starting Worker plugin PreImport-9b5f4ea9-7832-483d-934a-ad39bc4b0f62
2023-06-26 17:10:33,883 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,893 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-533e6f35-5bd6-4d7d-9aa5-1df31ef40696
2023-06-26 17:10:33,893 - distributed.worker - INFO - Starting Worker plugin PreImport-6fbcf68c-e2e4-4d95-9cf7-9b09d061be0b
2023-06-26 17:10:33,894 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,899 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,899 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,901 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,909 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,909 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,909 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,910 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,910 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,911 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,938 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c05eefc0-b83f-4ee7-be25-14d4096dd74a
2023-06-26 17:10:33,939 - distributed.worker - INFO - Starting Worker plugin PreImport-baa637b4-d71e-4eb3-ac5f-6f8793e01a55
2023-06-26 17:10:33,941 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c13d0ed1-ff28-4cfa-85af-0473c0f3b88a
2023-06-26 17:10:33,942 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,944 - distributed.worker - INFO - Starting Worker plugin PreImport-612300d9-ee5a-462f-982e-8d84ef7d4c3f
2023-06-26 17:10:33,944 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,946 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,946 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb28a0b7-f1cf-4f60-abaa-6be51802395b
2023-06-26 17:10:33,947 - distributed.worker - INFO - Starting Worker plugin PreImport-1ed2f89e-c49d-4909-9070-721bf27aeba6
2023-06-26 17:10:33,947 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,950 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4b22f67-94f5-47ef-b795-0608d550472c
2023-06-26 17:10:33,951 - distributed.worker - INFO - Starting Worker plugin PreImport-6d9e2d07-6e14-45fb-a3fd-ff530e9911b5
2023-06-26 17:10:33,952 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,954 - distributed.worker - INFO - Starting Worker plugin PreImport-4c0ed23f-98cf-47f0-adc0-23dda55f809e
2023-06-26 17:10:33,956 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,960 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,960 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,961 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,961 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,961 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,963 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37ad110a-02de-4e63-97d6-844ada624ef3
2023-06-26 17:10:33,964 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,966 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,966 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,964 - distributed.worker - INFO - Starting Worker plugin PreImport-fd525648-4821-4ea1-b285-a810639ff6e7
2023-06-26 17:10:33,968 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,968 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,968 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,970 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,970 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,970 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,970 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,972 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,972 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,972 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,975 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:33,992 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:10:33,992 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:10:33,994 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:10:36,521 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,521 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,521 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,522 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,522 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,522 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,522 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,523 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,524 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,524 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,524 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,525 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,525 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,526 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,528 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,530 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:10:36,539 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,539 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,539 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,539 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,539 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,539 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,539 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,539 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,540 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,540 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,540 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,540 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,540 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,540 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,540 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:36,540 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,225 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:37,225 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:10:40,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:52,258 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:52,570 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:52,732 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:52,764 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:52,823 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:52,826 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:52,857 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,025 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,027 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,031 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,032 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,038 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,101 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,154 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,157 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:53,306 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:10:59,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:10:59,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:39,133 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,133 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,134 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,136 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,137 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,139 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,140 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:39,140 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:11:42,304 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,310 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,311 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,312 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,312 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:11:42,623 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:42,631 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:11:46,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:46,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:46,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:46,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:46,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:46,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:46,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:46,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:11:47,102 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,104 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33227. Reason: scheduler-restart
2023-06-26 17:11:47,104 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,105 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,105 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33349. Reason: scheduler-restart
2023-06-26 17:11:47,105 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,106 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,106 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33775. Reason: scheduler-restart
2023-06-26 17:11:47,106 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,106 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,106 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35079. Reason: scheduler-restart
2023-06-26 17:11:47,107 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,107 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,107 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35379. Reason: scheduler-restart
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,107 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35615. Reason: scheduler-restart
2023-06-26 17:11:47,108 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33227
2023-06-26 17:11:47,108 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,108 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,108 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36051. Reason: scheduler-restart
2023-06-26 17:11:47,108 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,108 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,108 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,108 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,108 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,108 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36153. Reason: scheduler-restart
2023-06-26 17:11:47,109 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,109 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,109 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,109 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37199. Reason: scheduler-restart
2023-06-26 17:11:47,109 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,110 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,110 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39041. Reason: scheduler-restart
2023-06-26 17:11:47,110 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,110 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,110 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,110 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,110 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,111 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,111 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,111 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,112 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,112 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:11:47,113 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40523. Reason: scheduler-restart
2023-06-26 17:11:47,116 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33349
2023-06-26 17:11:47,116 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33775
2023-06-26 17:11:47,116 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35079
2023-06-26 17:11:47,116 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35379
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35615
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36051
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36153
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37199
2023-06-26 17:11:47,117 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39449. Reason: scheduler-restart
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33349
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33775
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35079
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35379
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35615
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36051
2023-06-26 17:11:47,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36153
2023-06-26 17:11:47,118 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37199
2023-06-26 17:11:47,118 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,118 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,118 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41051. Reason: scheduler-restart
2023-06-26 17:11:47,118 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40643. Reason: scheduler-restart
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33349
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39041
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33349
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33775
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33775
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35079
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35079
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35379
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35379
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35615
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35615
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36051
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36051
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36153
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37199
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36153
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37199
2023-06-26 17:11:47,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39041
2023-06-26 17:11:47,119 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,120 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39041
2023-06-26 17:11:47,120 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,120 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,125 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41801. Reason: scheduler-restart
2023-06-26 17:11:47,126 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33349
2023-06-26 17:11:47,126 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33775
2023-06-26 17:11:47,126 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,126 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35079
2023-06-26 17:11:47,126 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35379
2023-06-26 17:11:47,126 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35615
2023-06-26 17:11:47,126 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36051
2023-06-26 17:11:47,127 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36153
2023-06-26 17:11:47,127 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37199
2023-06-26 17:11:47,127 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39041
2023-06-26 17:11:47,127 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,127 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 17:11:47,127 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,127 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39449
2023-06-26 17:11:47,127 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,127 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40643
2023-06-26 17:11:47,128 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7ff05a3cf0c0, tag: 0x30942bc7bc67a918>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7ff05a3cf0c0, tag: 0x30942bc7bc67a918>: 
2023-06-26 17:11:47,130 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33349
2023-06-26 17:11:47,130 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33775
2023-06-26 17:11:47,131 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35079
2023-06-26 17:11:47,131 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35379
2023-06-26 17:11:47,131 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35615
2023-06-26 17:11:47,131 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36051
2023-06-26 17:11:47,131 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36153
2023-06-26 17:11:47,131 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37199
2023-06-26 17:11:47,131 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39041
2023-06-26 17:11:47,134 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 17:11:47,134 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39449
2023-06-26 17:11:47,134 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40643
2023-06-26 17:11:47,135 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41801
2023-06-26 17:11:47,135 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,136 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42701. Reason: scheduler-restart
2023-06-26 17:11:47,136 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33349
2023-06-26 17:11:47,136 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33775
2023-06-26 17:11:47,136 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35079
2023-06-26 17:11:47,137 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35379
2023-06-26 17:11:47,137 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35615
2023-06-26 17:11:47,137 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36051
2023-06-26 17:11:47,137 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36153
2023-06-26 17:11:47,137 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37199
2023-06-26 17:11:47,137 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39041
2023-06-26 17:11:47,138 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40523
2023-06-26 17:11:47,138 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39449
2023-06-26 17:11:47,138 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40643
2023-06-26 17:11:47,138 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41801
2023-06-26 17:11:47,138 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41051
2023-06-26 17:11:47,139 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,142 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:11:47,144 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,144 - distributed.nanny - INFO - Worker closed
2023-06-26 17:11:47,151 - distributed.nanny - INFO - Worker closed
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-6864' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
2023-06-26 17:11:50,268 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:50,842 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:51,810 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:52,358 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:52,359 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:54,160 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:54,160 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:54,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:54,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:54,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:54,428 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:54,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:55,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:55,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:55,183 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,190 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:55,190 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:55,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:55,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:55,208 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,209 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,213 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,214 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,216 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,219 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,226 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,227 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,229 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:11:55,363 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:55,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:55,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:56,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:56,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,883 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,883 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,899 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,900 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,900 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,900 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:56,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:11:56,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:11:57,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,045 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46367
2023-06-26 17:11:57,045 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46367
2023-06-26 17:11:57,045 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42869
2023-06-26 17:11:57,046 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:11:57,046 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:11:57,046 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:11:57,046 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:11:57,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-342ltit5
2023-06-26 17:11:57,046 - distributed.worker - INFO - Starting Worker plugin PreImport-b6d2bad1-ca53-4d74-ab33-83fd1f0b5f39
2023-06-26 17:11:57,046 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc44ce08-a5e4-470c-b47d-9c7790bf9059
2023-06-26 17:11:57,046 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45955
2023-06-26 17:11:57,047 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45955
2023-06-26 17:11:57,047 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36913
2023-06-26 17:11:57,047 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:11:57,047 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:11:57,047 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:11:57,047 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:11:57,047 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l7n4z8a3
2023-06-26 17:11:57,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-741e4ae3-a1c1-407e-a848-3a8e5bad9849
2023-06-26 17:11:57,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,077 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,080 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,080 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,085 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,113 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,116 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:11:57,648 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36355
2023-06-26 17:11:57,648 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36355
2023-06-26 17:11:57,648 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43775
2023-06-26 17:11:57,648 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:11:57,648 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:11:57,648 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:11:57,648 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:11:57,648 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d213aadg
2023-06-26 17:11:57,649 - distributed.worker - INFO - Starting Worker plugin PreImport-6505f886-6235-4831-bab1-8297c85f01b3
2023-06-26 17:11:57,649 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c01e525f-ae82-4d34-9e0f-f409e8268b85
2023-06-26 17:11:57,657 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38669
2023-06-26 17:11:57,657 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38669
2023-06-26 17:11:57,658 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39929
2023-06-26 17:11:57,658 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:11:57,658 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:11:57,658 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:11:57,658 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:11:57,658 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vmwwx54u
2023-06-26 17:11:57,658 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-784f8128-007b-442f-9d2c-c6b1542e3bb1
2023-06-26 17:11:57,658 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b676002d-9589-4cee-8585-e09cacbb020e
2023-06-26 17:11:57,699 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38195
2023-06-26 17:11:57,700 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38195
2023-06-26 17:11:57,700 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44421
2023-06-26 17:11:57,700 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:11:57,700 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:11:57,700 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:11:57,700 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:11:57,700 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-re4o3b_i
2023-06-26 17:11:57,700 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45570f78-3158-4e88-9d39-93f37dba859b
2023-06-26 17:11:58,911 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36775
2023-06-26 17:11:58,911 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36775
2023-06-26 17:11:58,911 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41237
2023-06-26 17:11:58,911 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:11:58,911 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:11:58,911 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:11:58,911 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:11:58,911 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l2tmcw08
2023-06-26 17:11:58,912 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4345ca95-b214-4394-8274-4131656717e3
2023-06-26 17:11:59,953 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3df1e281-7215-42e6-9296-7c076a229308
2023-06-26 17:11:59,954 - distributed.worker - INFO - Starting Worker plugin PreImport-cb5d5a0a-db28-4ad6-96e6-05b0e9735efe
2023-06-26 17:11:59,954 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:11:59,966 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:11:59,966 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:11:59,968 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:00,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ffd40b70-e8df-47b9-8ab1-2994b81af55f
2023-06-26 17:12:00,010 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:00,022 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:00,022 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:00,023 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:00,228 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95fd7e67-eb9c-4733-a426-40020e635d1b
2023-06-26 17:12:00,230 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:00,241 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:00,241 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:00,242 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:00,307 - distributed.worker - INFO - Starting Worker plugin PreImport-09f0934c-9f49-4187-994c-06c163f88652
2023-06-26 17:12:00,309 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:00,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f4036b2-a36e-4d41-9457-42e8ffe29036
2023-06-26 17:12:00,318 - distributed.worker - INFO - Starting Worker plugin PreImport-b791022c-1977-4e72-9705-26860bbfe0ec
2023-06-26 17:12:00,319 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:00,325 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:00,325 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:00,328 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:00,328 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:00,328 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:00,330 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:01,532 - distributed.worker - INFO - Starting Worker plugin PreImport-108dc07d-9544-465d-866a-a7d1d71701c7
2023-06-26 17:12:01,532 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20603131-5f24-4b4d-9bb5-258829bf0fb3
2023-06-26 17:12:01,533 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:01,551 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:01,551 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:01,552 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:03,221 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35165
2023-06-26 17:12:03,221 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35165
2023-06-26 17:12:03,221 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43813
2023-06-26 17:12:03,222 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:03,222 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:03,222 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:03,222 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:03,222 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3dj3fqjd
2023-06-26 17:12:03,223 - distributed.worker - INFO - Starting Worker plugin PreImport-fbee39bf-17da-48ba-9716-393b5f213d72
2023-06-26 17:12:03,223 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40f19d6a-f293-4c27-b0b1-98e882d87930
2023-06-26 17:12:03,224 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37485
2023-06-26 17:12:03,224 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37485
2023-06-26 17:12:03,225 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36547
2023-06-26 17:12:03,225 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:03,225 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:03,225 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:03,225 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:03,225 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sgxc5bh8
2023-06-26 17:12:03,225 - distributed.worker - INFO - Starting Worker plugin PreImport-1abac7e5-b2e8-446f-9ebf-01f7ae93e316
2023-06-26 17:12:03,225 - distributed.worker - INFO - Starting Worker plugin RMMSetup-747cbae6-be78-4224-bd7b-b45eefe6352e
2023-06-26 17:12:03,238 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35571
2023-06-26 17:12:03,238 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35571
2023-06-26 17:12:03,238 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41187
2023-06-26 17:12:03,238 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:03,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:03,238 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:03,238 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:03,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3p41hask
2023-06-26 17:12:03,239 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-927a407f-526b-456c-a5ca-152b3a9d2581
2023-06-26 17:12:03,239 - distributed.worker - INFO - Starting Worker plugin PreImport-5a9a115e-fd2c-42be-9df4-71439b38c9f9
2023-06-26 17:12:03,240 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d2498f3-5cee-402b-adb6-f638dab9621f
2023-06-26 17:12:04,006 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38135
2023-06-26 17:12:04,006 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38135
2023-06-26 17:12:04,006 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43997
2023-06-26 17:12:04,006 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:04,006 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:04,006 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:04,007 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:04,007 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jz9dzga3
2023-06-26 17:12:04,008 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3c8db00-4e63-4f5e-b6a7-57dc0f4ab0e8
2023-06-26 17:12:04,008 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43893
2023-06-26 17:12:04,009 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43893
2023-06-26 17:12:04,009 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41243
2023-06-26 17:12:04,009 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:04,009 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:04,009 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:04,009 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:04,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h7hxog4l
2023-06-26 17:12:04,009 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45c6c0d9-6abb-4866-8404-205bdf369bdb
2023-06-26 17:12:04,034 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41757
2023-06-26 17:12:04,034 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41757
2023-06-26 17:12:04,034 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41637
2023-06-26 17:12:04,034 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:04,034 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:04,035 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:04,035 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:04,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ijf4lhyn
2023-06-26 17:12:04,035 - distributed.worker - INFO - Starting Worker plugin PreImport-686d0bd0-a44a-419a-b410-5a63940d900b
2023-06-26 17:12:04,035 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6145721-4c99-43db-9223-7abee3aea935
2023-06-26 17:12:04,057 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41993
2023-06-26 17:12:04,058 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41993
2023-06-26 17:12:04,058 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41897
2023-06-26 17:12:04,058 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:04,058 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:04,058 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:04,059 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:04,059 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q82uhg07
2023-06-26 17:12:04,060 - distributed.worker - INFO - Starting Worker plugin PreImport-557ff59f-87aa-4ae7-93da-df01d172b266
2023-06-26 17:12:04,060 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8599bd2-68cd-4732-b51e-f965385ae4a3
2023-06-26 17:12:04,063 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37483
2023-06-26 17:12:04,064 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37483
2023-06-26 17:12:04,064 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39097
2023-06-26 17:12:04,064 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:04,064 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:04,064 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:04,064 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:04,064 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3kbd6gwq
2023-06-26 17:12:04,064 - distributed.worker - INFO - Starting Worker plugin PreImport-8ebde90c-cfb5-4715-8320-c80ec08dc685
2023-06-26 17:12:04,064 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1951632-b207-42ce-9914-1962c0e3e824
2023-06-26 17:12:04,080 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45219
2023-06-26 17:12:04,080 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45219
2023-06-26 17:12:04,080 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45835
2023-06-26 17:12:04,080 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:04,081 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:04,081 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:04,081 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:04,081 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-572bo4zj
2023-06-26 17:12:04,082 - distributed.worker - INFO - Starting Worker plugin PreImport-065bf62d-9fe6-45d1-9553-e62003716411
2023-06-26 17:12:04,082 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9d5bc09-106f-465e-b35a-0042e724f3ac
2023-06-26 17:12:04,085 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43833
2023-06-26 17:12:04,085 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43833
2023-06-26 17:12:04,085 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42657
2023-06-26 17:12:04,086 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:12:04,086 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:04,086 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:12:04,086 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:12:04,086 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xpsfaxgi
2023-06-26 17:12:04,086 - distributed.worker - INFO - Starting Worker plugin PreImport-cb65edb4-0c6f-4693-b5a2-e29fa541416e
2023-06-26 17:12:04,086 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d351c55-840d-43ca-b3f1-a7e12503c2ba
2023-06-26 17:12:05,923 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c2a80f78-96ff-4dee-84c6-23fec4f6dcf7
2023-06-26 17:12:05,925 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:05,944 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:05,944 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:05,951 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:05,965 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:05,991 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:05,991 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:05,994 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:06,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-802f14cc-5e2c-4917-96ea-2d0b304eb4aa
2023-06-26 17:12:06,115 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,128 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:06,128 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,129 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:06,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4517adfc-adb3-4c1e-8beb-47d95568afdc
2023-06-26 17:12:06,352 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,365 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:06,365 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,367 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:06,369 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc6f5956-f8ab-4d89-8864-07fa82eabc1f
2023-06-26 17:12:06,371 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,388 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98eb70a0-f2e6-480b-ac1b-a0adf30bbd4e
2023-06-26 17:12:06,390 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:06,391 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,391 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,393 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:06,410 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:06,410 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,413 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:06,420 - distributed.worker - INFO - Starting Worker plugin PreImport-70e4c1d2-7ef3-479e-8210-d9ca13f49496
2023-06-26 17:12:06,420 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35b9a1c7-708c-4f0f-bce1-3ee069b42e84
2023-06-26 17:12:06,422 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56c27396-46c2-42f7-8020-1a9c63799ff6
2023-06-26 17:12:06,428 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a90361b-d684-48ce-a484-04e455177445
2023-06-26 17:12:06,440 - distributed.worker - INFO - Starting Worker plugin PreImport-730f4ba6-baf6-46b2-a071-9b765d26cc01
2023-06-26 17:12:06,442 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,445 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:06,445 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,447 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:06,447 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:06,447 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,450 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:06,452 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88d88c05-c779-455e-9162-a812a84dff61
2023-06-26 17:12:06,455 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,460 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:06,460 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,462 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:06,476 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:12:06,476 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:12:06,479 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:12:15,577 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:15,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:15,793 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:15,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,203 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,313 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,314 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,333 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,342 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,378 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,381 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,387 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,404 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,415 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,446 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,499 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,517 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,605 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:12:16,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:16,616 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,616 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,616 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,617 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687799536.628014] [exp01:306675:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:39006) failed: Address already in use
2023-06-26 17:12:16,627 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,628 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:12:16,628 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687799536.628135] [exp01:306750:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:56310) failed: Address already in use
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:16,639 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:12:19,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:27,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:31,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:12:31,341 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:31,342 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,250 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,251 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,251 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,251 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,251 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:12:43,251 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36355. Reason: worker-close
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36775. Reason: worker-close
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38195. Reason: worker-close
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38669. Reason: worker-close
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35571. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35165. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45219. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38135. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41757. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43893. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45955. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37483. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43833. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37485. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41993. Reason: worker-handle-scheduler-connection-broken
2023-06-26 17:38:22,024 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:32831'. Reason: nanny-close
2023-06-26 17:38:22,025 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42647'. Reason: nanny-close
2023-06-26 17:38:22,026 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,025 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:56164 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:38:22,025 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:56184 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:38:22,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45195'. Reason: nanny-close
2023-06-26 17:38:22,025 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:56230 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:38:22,027 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41715'. Reason: nanny-close
2023-06-26 17:38:22,027 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,025 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:56176 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 17:38:22,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42659'. Reason: nanny-close
2023-06-26 17:38:22,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40197'. Reason: nanny-close
2023-06-26 17:38:22,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35357'. Reason: nanny-close
2023-06-26 17:38:22,029 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36621'. Reason: nanny-close
2023-06-26 17:38:22,029 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34347'. Reason: nanny-close
2023-06-26 17:38:22,030 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,030 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46613'. Reason: nanny-close
2023-06-26 17:38:22,030 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,030 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40949'. Reason: nanny-close
2023-06-26 17:38:22,033 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42269'. Reason: nanny-close
2023-06-26 17:38:22,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34593'. Reason: nanny-close
2023-06-26 17:38:22,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39139'. Reason: nanny-close
2023-06-26 17:38:22,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33071'. Reason: nanny-close
Process Dask Worker process (from Nanny):
2023-06-26 17:38:22,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 17:38:22,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38303'. Reason: nanny-close
2023-06-26 17:38:22,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 202, in _run
    target(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 999, in _run
    asyncio.run(run())
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 47, in run
    _cancel_all_tasks(loop)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 63, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 685, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 919, in _run
    val = self.callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/system_monitor.py", line 160, in update
    net_ioc = psutil.net_io_counters()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/psutil/__init__.py", line 2119, in net_io_counters
    rawdict = _psplatform.net_io_counters()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/psutil/_pslinux.py", line 1028, in net_io_counters
    lines = f.readlines()
  File "/opt/conda/envs/rapids/lib/python3.10/codecs.py", line 319, in decode
    def decode(self, input, final=False):
KeyboardInterrupt
2023-06-26 17:38:22,046 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42647 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45800 remote=tcp://10.120.104.11:42647>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42647 after 100 s
2023-06-26 17:38:22,047 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45195 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50242 remote=tcp://10.120.104.11:45195>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45195 after 100 s
2023-06-26 17:38:22,048 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35357 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60656 remote=tcp://10.120.104.11:35357>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35357 after 100 s
2023-06-26 17:38:22,048 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40197 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59636 remote=tcp://10.120.104.11:40197>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40197 after 100 s
2023-06-26 17:38:22,050 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34347 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:43452 remote=tcp://10.120.104.11:34347>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34347 after 100 s
2023-06-26 17:38:22,050 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46613 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42292 remote=tcp://10.120.104.11:46613>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46613 after 100 s
2023-06-26 17:38:22,054 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39139 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48964 remote=tcp://10.120.104.11:39139>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39139 after 100 s
2023-06-26 17:38:22,055 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33071 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:53186 remote=tcp://10.120.104.11:33071>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33071 after 100 s
2023-06-26 17:38:22,054 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42269 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50990 remote=tcp://10.120.104.11:42269>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42269 after 100 s
2023-06-26 17:38:22,055 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38303 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60590 remote=tcp://10.120.104.11:38303>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38303 after 100 s
2023-06-26 17:38:22,057 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41715 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60828 remote=tcp://10.120.104.11:41715>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41715 after 100 s
2023-06-26 17:38:22,058 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42659 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59618 remote=tcp://10.120.104.11:42659>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42659 after 100 s
2023-06-26 17:38:22,059 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36621 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:57542 remote=tcp://10.120.104.11:36621>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36621 after 100 s
2023-06-26 17:38:22,061 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40949 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:44028 remote=tcp://10.120.104.11:40949>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40949 after 100 s
2023-06-26 17:38:22,541 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46367. Reason: worker-close
2023-06-26 17:38:22,541 - distributed.core - ERROR - Event loop is closed
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 955, in run
    await worker.finished()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1534, in close
    self.status = Status.closing
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1011, in status
    self._send_worker_status_change(stimulus_id)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1022, in _send_worker_status_change
    self.batched_send(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1167, in batched_send
    self.batched_stream.send(msg)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 162, in send
    self.waker.set()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/locks.py", line 222, in set
    fut.set_result(None)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 753, in call_soon
    self._check_closed()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Exception ignored in: <coroutine object WorkerProcess._run.<locals>.run at 0x7f59e6b247b0>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 939, in run
    async with worker:
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 644, in __aexit__
    await self.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1534, in close
    self.status = Status.closing
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1011, in status
    self._send_worker_status_change(stimulus_id)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1022, in _send_worker_status_change
    self.batched_send(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1167, in batched_send
    self.batched_stream.send(msg)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 162, in send
    self.waker.set()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/locks.py", line 222, in set
    fut.set_result(None)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 753, in call_soon
    self._check_closed()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Task was destroyed but it is pending!
task: <Task pending name='Task-11' coro=<Worker.handle_scheduler() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py:209> wait_for=<Future cancelled> cb=[IOLoop.add_future.<locals>.<lambda>() at /opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py:685, gather.<locals>._done_callback() at /opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py:720]>
2023-06-26 17:38:22,544 - distributed.worker - ERROR - <asyncio.locks.Event object at 0x7f59e6b5de40 [unset]> is bound to a different event loop
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1304, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1025, in handle_stream
    await comm.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 210, in wrapper
    future = _create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 147, in _create_future
    future = Future()  # type: Future
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 656, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'MainThread'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1510, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 592, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 211, in wait
    fut = self._get_loop().create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/mixins.py", line 30, in _get_loop
    raise RuntimeError(f'{self!r} is bound to a different event loop')
RuntimeError: <asyncio.locks.Event object at 0x7f59e6b5de40 [unset]> is bound to a different event loop
2023-06-26 17:38:22,544 - distributed.worker - CRITICAL - Error trying close worker in response to broken internal state. Forcibly exiting worker NOW
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1304, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1025, in handle_stream
    await comm.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 210, in wrapper
    future = _create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 147, in _create_future
    future = Future()  # type: Future
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 656, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'MainThread'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 209, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1306, in handle_scheduler
    await self.close(reason="worker-handle-scheduler-connection-broken")
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1510, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 592, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 211, in wait
    fut = self._get_loop().create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/mixins.py", line 30, in _get_loop
    raise RuntimeError(f'{self!r} is bound to a different event loop')
RuntimeError: <asyncio.locks.Event object at 0x7f59e6b5de40 [unset]> is bound to a different event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 241, in _force_close
    await wait_for(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 405, in wait_for
    loop = events.get_running_loop()
RuntimeError: no running event loop
2023-06-26 17:38:23,235 - distributed.nanny - INFO - Worker process 306645 exited with status 1
2023-06-26 17:38:25,237 - distributed.nanny - WARNING - Worker process still alive after 3.199997711181641 seconds, killing
2023-06-26 17:38:25,238 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:25,238 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 17:38:25,238 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:25,239 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:25,239 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:25,240 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 17:38:25,242 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:25,243 - distributed.nanny - WARNING - Worker process still alive after 3.19999984741211 seconds, killing
2023-06-26 17:38:25,244 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:25,244 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-26 17:38:25,244 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 17:38:25,245 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:25,245 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:25,246 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 17:38:26,045 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,047 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,047 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,047 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,047 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,047 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,048 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,048 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,048 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,048 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,048 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,048 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,049 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,049 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,049 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 17:38:26,051 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306772 parent=303918 started daemon>
2023-06-26 17:38:26,051 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306769 parent=303918 started daemon>
2023-06-26 17:38:26,051 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306766 parent=303918 started daemon>
2023-06-26 17:38:26,051 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306763 parent=303918 started daemon>
2023-06-26 17:38:26,051 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306756 parent=303918 started daemon>
2023-06-26 17:38:26,051 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306753 parent=303918 started daemon>
2023-06-26 17:38:26,051 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306750 parent=303918 started daemon>
2023-06-26 17:38:26,052 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306747 parent=303918 started daemon>
2023-06-26 17:38:26,052 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306743 parent=303918 started daemon>
2023-06-26 17:38:26,052 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306731 parent=303918 started daemon>
2023-06-26 17:38:26,052 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306722 parent=303918 started daemon>
2023-06-26 17:38:26,052 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306690 parent=303918 started daemon>
2023-06-26 17:38:26,052 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306687 parent=303918 started daemon>
2023-06-26 17:38:26,052 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306675 parent=303918 started daemon>
2023-06-26 17:38:26,052 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=306657 parent=303918 started daemon>
2023-06-26 17:38:31,380 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 306690 exit status was already read will report exitcode 255
2023-06-26 17:38:32,391 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 306756 exit status was already read will report exitcode 255
