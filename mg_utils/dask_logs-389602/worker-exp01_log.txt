RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=12G
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 18:45:56,391 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39709'
2023-06-26 18:45:56,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36681'
2023-06-26 18:45:56,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33751'
2023-06-26 18:45:56,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36113'
2023-06-26 18:45:56,399 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36537'
2023-06-26 18:45:56,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43309'
2023-06-26 18:45:56,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36201'
2023-06-26 18:45:56,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40645'
2023-06-26 18:45:56,407 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38273'
2023-06-26 18:45:56,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43735'
2023-06-26 18:45:56,412 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43461'
2023-06-26 18:45:56,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43625'
2023-06-26 18:45:56,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39683'
2023-06-26 18:45:56,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34273'
2023-06-26 18:45:56,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33573'
2023-06-26 18:45:56,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46105'
2023-06-26 18:45:57,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:57,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,089 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,109 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,109 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,118 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,118 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,131 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,131 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,131 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,132 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,142 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,188 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:45:58,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:45:58,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,297 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,320 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,323 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,327 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,331 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,362 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,363 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:45:58,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:05,049 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35199
2023-06-26 18:46:05,050 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35199
2023-06-26 18:46:05,050 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44171
2023-06-26 18:46:05,050 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,050 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,050 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,050 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,050 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pxllvti3
2023-06-26 18:46:05,050 - distributed.worker - INFO - Starting Worker plugin PreImport-a4b1267b-7f7c-46a5-84cf-e4c407224f9b
2023-06-26 18:46:05,050 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a57abbd-fa47-441e-b8e9-1f57d31b0ee3
2023-06-26 18:46:05,202 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41851
2023-06-26 18:46:05,203 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41851
2023-06-26 18:46:05,203 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44109
2023-06-26 18:46:05,203 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,203 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,203 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,203 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,203 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jjys6e00
2023-06-26 18:46:05,203 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc48d823-7d22-4979-8e3a-4d14ff90f81f
2023-06-26 18:46:05,206 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33973
2023-06-26 18:46:05,206 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33973
2023-06-26 18:46:05,206 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34879
2023-06-26 18:46:05,206 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,206 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,206 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,206 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,206 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7m1aqabo
2023-06-26 18:46:05,207 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f38575f-09de-436f-9374-ce11a87e84b4
2023-06-26 18:46:05,232 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40665
2023-06-26 18:46:05,233 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40665
2023-06-26 18:46:05,233 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46167
2023-06-26 18:46:05,233 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,233 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,233 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,233 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,233 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w01n8y85
2023-06-26 18:46:05,234 - distributed.worker - INFO - Starting Worker plugin PreImport-a02a8f80-90f7-4cd9-b530-3cf5037bd2a0
2023-06-26 18:46:05,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-adee46c9-a938-4a08-b5f1-4b04b7b50032
2023-06-26 18:46:05,265 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35529
2023-06-26 18:46:05,265 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35529
2023-06-26 18:46:05,265 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43553
2023-06-26 18:46:05,265 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,265 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,265 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,265 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hejg0ijr
2023-06-26 18:46:05,266 - distributed.worker - INFO - Starting Worker plugin PreImport-9e12da9e-cf5e-4f97-bc27-d0a76d1b9350
2023-06-26 18:46:05,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3ca0adb-7ae9-4f2e-847c-bbae48f28f14
2023-06-26 18:46:05,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de61f394-34d9-4fd6-9c73-c297c858beb9
2023-06-26 18:46:05,367 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,383 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,383 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,387 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:05,520 - distributed.worker - INFO - Starting Worker plugin PreImport-1b234f0b-00f2-4efc-a5be-c5044f50e5e9
2023-06-26 18:46:05,520 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b50cd8fc-e588-4fe4-8ef6-9e86dbf04e93
2023-06-26 18:46:05,521 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,559 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,559 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,559 - distributed.worker - INFO - Starting Worker plugin PreImport-ac89b0e9-b86b-44a3-91f8-1aa226218de2
2023-06-26 18:46:05,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-225cfa7d-4baa-4a20-9f57-e665329c159b
2023-06-26 18:46:05,561 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,563 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:05,583 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,583 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,586 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:05,654 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43143
2023-06-26 18:46:05,654 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43143
2023-06-26 18:46:05,654 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38969
2023-06-26 18:46:05,654 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,654 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,654 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g8n14kmf
2023-06-26 18:46:05,655 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0a34bd2-3b26-4a40-8d99-9fcf22640134
2023-06-26 18:46:05,655 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58e468db-c667-45b3-9281-2191e197f658
2023-06-26 18:46:05,655 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46625
2023-06-26 18:46:05,656 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46625
2023-06-26 18:46:05,656 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41169
2023-06-26 18:46:05,656 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,656 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,656 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,656 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,656 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hajb9qnv
2023-06-26 18:46:05,656 - distributed.worker - INFO - Starting Worker plugin RMMSetup-03af2373-f4a5-4b4a-842c-95228ed2a0d4
2023-06-26 18:46:05,657 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34b527d3-3dd8-4d5f-819e-cec900dad697
2023-06-26 18:46:05,658 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,687 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,687 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,689 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:05,710 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35467
2023-06-26 18:46:05,710 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35467
2023-06-26 18:46:05,710 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33929
2023-06-26 18:46:05,710 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,710 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,710 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,710 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,710 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eqdecafj
2023-06-26 18:46:05,711 - distributed.worker - INFO - Starting Worker plugin PreImport-7446672b-36fd-454b-9edc-178fedeeee13
2023-06-26 18:46:05,711 - distributed.worker - INFO - Starting Worker plugin RMMSetup-966e335d-10b7-4b6d-b9cb-0d967f9d305e
2023-06-26 18:46:05,782 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5193031c-6264-43d8-aaae-35b282a3851e
2023-06-26 18:46:05,783 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,808 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,808 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,811 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:05,820 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46743
2023-06-26 18:46:05,820 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46743
2023-06-26 18:46:05,820 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35799
2023-06-26 18:46:05,820 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,820 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,820 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,821 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,821 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ax3v1vj6
2023-06-26 18:46:05,821 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d587f161-19d6-4f25-b94a-d7d5b2986fd9
2023-06-26 18:46:05,841 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41491
2023-06-26 18:46:05,841 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41491
2023-06-26 18:46:05,841 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33797
2023-06-26 18:46:05,841 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,841 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,841 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,841 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zjgk77e_
2023-06-26 18:46:05,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5efc92b7-75da-4972-b018-85e0f6fc516c
2023-06-26 18:46:05,842 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35839
2023-06-26 18:46:05,842 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35839
2023-06-26 18:46:05,842 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46803
2023-06-26 18:46:05,842 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,842 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,842 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,842 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4vlpau76
2023-06-26 18:46:05,844 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b796dc91-9bab-41d2-9902-8c04e40e910f
2023-06-26 18:46:05,855 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33633
2023-06-26 18:46:05,855 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33633
2023-06-26 18:46:05,855 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39941
2023-06-26 18:46:05,855 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,855 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,855 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,855 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,855 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i5_qbfee
2023-06-26 18:46:05,856 - distributed.worker - INFO - Starting Worker plugin PreImport-c9cc3792-23f2-427c-a08b-92677f3879f0
2023-06-26 18:46:05,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a82e1a1-7e38-443e-80cb-82a3fd9232a4
2023-06-26 18:46:05,876 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45225
2023-06-26 18:46:05,876 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45225
2023-06-26 18:46:05,876 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34527
2023-06-26 18:46:05,877 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,877 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,877 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,877 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,877 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9xz37dxc
2023-06-26 18:46:05,878 - distributed.worker - INFO - Starting Worker plugin PreImport-ea3b60e5-21a2-480c-af0b-5001d3bb1032
2023-06-26 18:46:05,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc770427-5647-43bd-ab65-ee7842109f19
2023-06-26 18:46:05,883 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42985
2023-06-26 18:46:05,883 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42985
2023-06-26 18:46:05,883 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37691
2023-06-26 18:46:05,883 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32915
2023-06-26 18:46:05,883 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37691
2023-06-26 18:46:05,883 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,883 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,883 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41123
2023-06-26 18:46:05,883 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,883 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,883 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,883 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,883 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y6eosepe
2023-06-26 18:46:05,883 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,883 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,883 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cgb78mnw
2023-06-26 18:46:05,884 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e29a4a1-d218-4231-9962-79568adcfb16
2023-06-26 18:46:05,884 - distributed.worker - INFO - Starting Worker plugin PreImport-e6e08635-0fda-4371-a517-0eaadd9bc8fd
2023-06-26 18:46:05,884 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8ca9362-4787-4cbf-874a-d614cdc00404
2023-06-26 18:46:05,899 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35125
2023-06-26 18:46:05,900 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35125
2023-06-26 18:46:05,900 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38283
2023-06-26 18:46:05,900 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:05,900 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:05,900 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:05,900 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:05,900 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4cci3dfd
2023-06-26 18:46:05,900 - distributed.worker - INFO - Starting Worker plugin PreImport-50b5e57c-4a6c-4aec-9481-9fb0ac81bd30
2023-06-26 18:46:05,900 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5eea54d7-cc8b-499d-9b3a-83ad343afead
2023-06-26 18:46:06,120 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45a0cfb0-fae3-46ad-998d-d4334af22ad2
2023-06-26 18:46:06,120 - distributed.worker - INFO - Starting Worker plugin PreImport-04473637-a01b-428e-a113-f576ec3d2398
2023-06-26 18:46:06,121 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,122 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,189 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c89b0b54-717c-41af-bf80-ee281b5e9a9e
2023-06-26 18:46:06,189 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbcb3678-482f-4ee6-9d78-b7a747709d33
2023-06-26 18:46:06,190 - distributed.worker - INFO - Starting Worker plugin PreImport-25e82766-43ac-4b61-a5ab-3d484c0133a7
2023-06-26 18:46:06,190 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,190 - distributed.worker - INFO - Starting Worker plugin PreImport-bd626574-9be5-411e-a40a-0132729bcb0a
2023-06-26 18:46:06,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d991d717-425a-4764-b386-55eda1e94a50
2023-06-26 18:46:06,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d67293c8-f311-40d0-83d5-67b0696b54f9
2023-06-26 18:46:06,190 - distributed.worker - INFO - Starting Worker plugin PreImport-4f3b6b87-d267-4827-881c-348110513b96
2023-06-26 18:46:06,191 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,191 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,191 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2bff9e23-4225-4a49-bbdc-be665cabe1a1
2023-06-26 18:46:06,191 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,192 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,192 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,192 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,194 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,194 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,194 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,195 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,202 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,202 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,203 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,246 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d870ef79-4e10-4e9d-b7df-20f027c4cedc
2023-06-26 18:46:06,246 - distributed.worker - INFO - Starting Worker plugin PreImport-1f9034b7-418e-43d3-9763-6ab3412815ad
2023-06-26 18:46:06,246 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-504e8c19-9a64-4fb7-adc1-73e4fb62c720
2023-06-26 18:46:06,246 - distributed.worker - INFO - Starting Worker plugin PreImport-3e818438-9bdb-4cdd-82b2-856e8c25d8be
2023-06-26 18:46:06,246 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ace3a830-27fb-4f71-adf1-24e2622eee84
2023-06-26 18:46:06,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,247 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c01c6129-e507-48dd-a01f-67442209e020
2023-06-26 18:46:06,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,248 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,249 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,249 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,250 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,251 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,251 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,251 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,251 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,252 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,253 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,254 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,261 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,261 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,262 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,262 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,263 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,263 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,267 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,267 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,268 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:06,268 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:06,269 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:06,271 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:12,431 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,431 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,431 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,431 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,431 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,433 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,433 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,435 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,435 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,435 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,436 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,437 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,437 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,438 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,439 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,439 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,448 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,449 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:12,449 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,130 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,131 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,131 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,131 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:13,131 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 18:46:27,980 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,102 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,127 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,160 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,183 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,262 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,268 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,275 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,324 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,412 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,418 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,454 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,457 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,511 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:28,587 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:29,028 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 18:46:35,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:46:35,086 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,086 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,086 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,086 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,086 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,087 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,087 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,087 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,087 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,087 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,087 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,087 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,088 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,089 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,089 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:35,089 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 18:46:39,976 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,976 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,976 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,976 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,976 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,976 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:39,977 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:46:40,417 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,420 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33633. Reason: scheduler-restart
2023-06-26 18:46:40,421 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,421 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33973. Reason: scheduler-restart
2023-06-26 18:46:40,421 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,422 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35125. Reason: scheduler-restart
2023-06-26 18:46:40,422 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35199. Reason: scheduler-restart
2023-06-26 18:46:40,422 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,422 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,423 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35467. Reason: scheduler-restart
2023-06-26 18:46:40,423 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,423 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,423 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35529. Reason: scheduler-restart
2023-06-26 18:46:40,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,424 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35839. Reason: scheduler-restart
2023-06-26 18:46:40,424 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,424 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,424 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,424 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,425 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,425 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,425 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,425 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37691. Reason: scheduler-restart
2023-06-26 18:46:40,425 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,425 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33633
2023-06-26 18:46:40,425 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40665. Reason: scheduler-restart
2023-06-26 18:46:40,425 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,425 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41491. Reason: scheduler-restart
2023-06-26 18:46:40,425 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,426 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,426 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,426 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,426 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41851. Reason: scheduler-restart
2023-06-26 18:46:40,426 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,427 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,427 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42985. Reason: scheduler-restart
2023-06-26 18:46:40,427 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,427 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,427 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,427 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,428 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43143. Reason: scheduler-restart
2023-06-26 18:46:40,428 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,428 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 18:46:40,430 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45225. Reason: scheduler-restart
2023-06-26 18:46:40,430 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,430 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,431 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,431 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,431 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,434 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33973
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35125
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35199
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35467
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35529
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35839
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37691
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41491
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40665
2023-06-26 18:46:40,435 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41851
2023-06-26 18:46:40,435 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,437 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,437 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46625. Reason: scheduler-restart
2023-06-26 18:46:40,437 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,438 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33973
2023-06-26 18:46:40,438 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35125
2023-06-26 18:46:40,438 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35199
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35467
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35529
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35839
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37691
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41491
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40665
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41851
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42985
2023-06-26 18:46:40,439 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45225
2023-06-26 18:46:40,453 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,456 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,457 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33973
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35125
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35199
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35467
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35529
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35839
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37691
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41491
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40665
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41851
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42985
2023-06-26 18:46:40,458 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45225
2023-06-26 18:46:40,459 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43143
2023-06-26 18:46:40,459 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,466 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:40,484 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46743. Reason: scheduler-restart
2023-06-26 18:46:40,485 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33973
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35125
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35199
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35467
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35529
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35839
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37691
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41491
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40665
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41851
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42985
2023-06-26 18:46:40,486 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45225
2023-06-26 18:46:40,487 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43143
2023-06-26 18:46:40,487 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46625
2023-06-26 18:46:40,487 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 18:46:40,489 - distributed.nanny - INFO - Worker closed
2023-06-26 18:46:42,115 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:44,365 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:44,366 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:44,368 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:45,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:45,393 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,395 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,419 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,425 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,426 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,431 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,432 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,434 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,436 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,438 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,440 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,442 - distributed.nanny - WARNING - Restarting worker
2023-06-26 18:46:45,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:45,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:45,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:45,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:45,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:46,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:46,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:46,076 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:46,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:46,203 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:46,953 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39615
2023-06-26 18:46:46,953 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39615
2023-06-26 18:46:46,953 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43365
2023-06-26 18:46:46,953 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:46,953 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:46,953 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:46,954 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:46,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uvi2km_h
2023-06-26 18:46:46,954 - distributed.worker - INFO - Starting Worker plugin PreImport-c4c01aef-6f56-4972-96a0-53c25c0438e7
2023-06-26 18:46:46,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b303e70-f0bf-483f-9b5c-632b923b4bf5
2023-06-26 18:46:47,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,118 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,118 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,122 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,157 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32af1256-43a8-4aaf-b3fb-3cde7d5aa7bb
2023-06-26 18:46:47,157 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:47,167 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:47,168 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:47,169 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:47,177 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,177 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,180 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,180 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 18:46:47,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 18:46:47,279 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,280 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,298 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,301 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,364 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,364 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 18:46:47,585 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42033
2023-06-26 18:46:47,586 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42033
2023-06-26 18:46:47,586 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37989
2023-06-26 18:46:47,586 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:47,586 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:47,586 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:47,586 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:47,586 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c6hxs9lr
2023-06-26 18:46:47,587 - distributed.worker - INFO - Starting Worker plugin PreImport-95111b9d-59ab-47dd-8acd-dfb3c6e68106
2023-06-26 18:46:47,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-feda63a7-bf38-480b-898a-d46438038bfe
2023-06-26 18:46:47,955 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a681673-e643-4bce-9b6d-57cb8eac2da2
2023-06-26 18:46:47,956 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:47,971 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:47,971 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:47,973 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:48,034 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36721
2023-06-26 18:46:48,034 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36721
2023-06-26 18:46:48,034 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36441
2023-06-26 18:46:48,034 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:48,034 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:48,034 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:48,034 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:48,034 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-59kb_y1g
2023-06-26 18:46:48,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e07b71b-dc7d-43bc-9aee-90aa11641d4c
2023-06-26 18:46:48,071 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38919
2023-06-26 18:46:48,071 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38919
2023-06-26 18:46:48,071 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45355
2023-06-26 18:46:48,071 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:48,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:48,071 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:48,071 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:48,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ul4mqn06
2023-06-26 18:46:48,071 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13ef0871-6c73-4a5b-bd40-f4077bccb94a
2023-06-26 18:46:48,422 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94a50aaf-6236-4446-858d-4ffcd14faaca
2023-06-26 18:46:48,422 - distributed.worker - INFO - Starting Worker plugin PreImport-7ccb44c6-e9d4-46b2-a622-0f78c4cc8c19
2023-06-26 18:46:48,423 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:48,423 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c11da35d-a33c-45ee-8bfb-153485c47358
2023-06-26 18:46:48,423 - distributed.worker - INFO - Starting Worker plugin PreImport-278df6a5-2247-4220-b103-5c492840058b
2023-06-26 18:46:48,423 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:48,434 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:48,435 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:48,435 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:48,435 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:48,436 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:48,436 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:52,479 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46649
2023-06-26 18:46:52,479 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46649
2023-06-26 18:46:52,479 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36865
2023-06-26 18:46:52,479 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,479 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,479 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,479 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,479 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9rlh9q09
2023-06-26 18:46:52,480 - distributed.worker - INFO - Starting Worker plugin PreImport-9ccba589-7d0b-4620-9720-763e2e173a6f
2023-06-26 18:46:52,480 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef8da650-a68e-4d1e-9ffd-2ae25a276450
2023-06-26 18:46:52,495 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34265
2023-06-26 18:46:52,495 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34265
2023-06-26 18:46:52,495 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44075
2023-06-26 18:46:52,495 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,495 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,495 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,495 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k80lbv5x
2023-06-26 18:46:52,495 - distributed.worker - INFO - Starting Worker plugin PreImport-8af1a0d6-0c09-475f-80e6-384318647e6f
2023-06-26 18:46:52,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9423d4b-9440-4755-b6b3-fc477684fedd
2023-06-26 18:46:52,497 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37263
2023-06-26 18:46:52,497 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37263
2023-06-26 18:46:52,497 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39469
2023-06-26 18:46:52,497 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,497 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,497 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,497 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45093
2023-06-26 18:46:52,497 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,497 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45093
2023-06-26 18:46:52,497 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bhg6voag
2023-06-26 18:46:52,497 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36847
2023-06-26 18:46:52,498 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,498 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,498 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,498 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,498 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h0fdy0zj
2023-06-26 18:46:52,498 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0a5ca2f-28d1-4508-be9f-a34c0a1330c7
2023-06-26 18:46:52,498 - distributed.worker - INFO - Starting Worker plugin PreImport-9e757a51-db34-4b03-864c-352ac225e9c5
2023-06-26 18:46:52,498 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf91a997-ae21-414c-b504-cc7563865173
2023-06-26 18:46:52,498 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9ebca37-6837-42d7-9ca6-8944776c172f
2023-06-26 18:46:52,543 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37491
2023-06-26 18:46:52,543 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37491
2023-06-26 18:46:52,543 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37821
2023-06-26 18:46:52,543 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,543 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,543 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,543 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,543 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-os5uq4h7
2023-06-26 18:46:52,544 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f3a671ca-073d-46c9-94dd-abeda672fc80
2023-06-26 18:46:52,546 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37321
2023-06-26 18:46:52,546 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37321
2023-06-26 18:46:52,546 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39541
2023-06-26 18:46:52,546 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,546 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,546 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,546 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bjjbtco2
2023-06-26 18:46:52,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bacaf155-3bad-4d0e-9e60-e883da048711
2023-06-26 18:46:52,547 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39967
2023-06-26 18:46:52,547 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39967
2023-06-26 18:46:52,547 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44471
2023-06-26 18:46:52,547 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,547 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,547 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,547 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1n7odhrv
2023-06-26 18:46:52,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e43101b-285f-4b27-96bb-a1b41e05e653
2023-06-26 18:46:52,550 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37329
2023-06-26 18:46:52,550 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37329
2023-06-26 18:46:52,550 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46365
2023-06-26 18:46:52,550 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,550 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,550 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,550 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,550 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5t50qhb9
2023-06-26 18:46:52,551 - distributed.worker - INFO - Starting Worker plugin PreImport-b629f1b8-82a4-4e5f-a570-e9a79d158d7a
2023-06-26 18:46:52,552 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79a5a382-6f01-4b0f-a290-ec618d6c293f
2023-06-26 18:46:52,554 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37815
2023-06-26 18:46:52,554 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37815
2023-06-26 18:46:52,554 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45891
2023-06-26 18:46:52,554 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,554 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,555 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,555 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,555 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bd6lz5xz
2023-06-26 18:46:52,556 - distributed.worker - INFO - Starting Worker plugin RMMSetup-788d17a7-81bd-4dc8-996e-641f6521ac2b
2023-06-26 18:46:52,560 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40271
2023-06-26 18:46:52,560 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40271
2023-06-26 18:46:52,560 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40795
2023-06-26 18:46:52,560 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,560 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,560 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,560 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,560 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-97lhmtx8
2023-06-26 18:46:52,561 - distributed.worker - INFO - Starting Worker plugin PreImport-f8d9cc7e-2203-4a70-bf65-77cdd6cfb2b8
2023-06-26 18:46:52,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a1fed93-4606-4b5b-bbda-684bf50c4d78
2023-06-26 18:46:52,561 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46801
2023-06-26 18:46:52,561 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46801
2023-06-26 18:46:52,561 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45167
2023-06-26 18:46:52,561 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,561 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,561 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,561 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,561 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3jvuoy1d
2023-06-26 18:46:52,562 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ec92c8c-8dbe-49d1-96f1-dae66c58c4ff
2023-06-26 18:46:52,566 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42843
2023-06-26 18:46:52,567 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42843
2023-06-26 18:46:52,567 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45335
2023-06-26 18:46:52,567 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,567 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,567 - distributed.worker - INFO -               Threads:                          1
2023-06-26 18:46:52,567 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 18:46:52,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w6eoak1i
2023-06-26 18:46:52,568 - distributed.worker - INFO - Starting Worker plugin PreImport-f2f937ed-a5d1-4903-af1a-efda63219838
2023-06-26 18:46:52,568 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24eec515-7368-41cd-928f-c92d5790dfe9
2023-06-26 18:46:52,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-defa370e-f9be-4a5a-ac27-a747da2caafd
2023-06-26 18:46:52,874 - distributed.worker - INFO - Starting Worker plugin PreImport-a84b3832-e920-4ffb-8133-3626939cd008
2023-06-26 18:46:52,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0da3ccf9-a5de-4609-a82a-d529c5bc5738
2023-06-26 18:46:52,874 - distributed.worker - INFO - Starting Worker plugin PreImport-47c8856d-a713-444b-8b95-4ae2467c27ce
2023-06-26 18:46:52,875 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,875 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4649f27e-2bb7-4216-b0cd-92afc592881e
2023-06-26 18:46:52,875 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,876 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,877 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,884 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,884 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,886 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,886 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,888 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:52,892 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:52,964 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e83622e-9eb0-4d59-8883-f55da4d53607
2023-06-26 18:46:52,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-276184f0-30f1-4f0c-a621-c91efabaac92
2023-06-26 18:46:52,966 - distributed.worker - INFO - Starting Worker plugin PreImport-11cc1c47-eb33-4101-8bda-ca58e2b219ab
2023-06-26 18:46:52,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a44b0d14-0b1e-42ad-b770-680dbac71f5b
2023-06-26 18:46:52,966 - distributed.worker - INFO - Starting Worker plugin PreImport-996f6959-5b74-494a-8e44-8202805ccf61
2023-06-26 18:46:52,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf621fa2-b5ea-4f2c-af31-0b5b22b05d38
2023-06-26 18:46:52,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dadd4744-0fd6-43b5-ad0e-b6a0e680fe94
2023-06-26 18:46:52,967 - distributed.worker - INFO - Starting Worker plugin PreImport-3b1da42b-8be6-4c56-8d26-883ef0a239f1
2023-06-26 18:46:52,967 - distributed.worker - INFO - Starting Worker plugin PreImport-c114284a-af16-4d7b-a281-f0de0f0b4851
2023-06-26 18:46:52,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8287f32-c7cb-440a-88d0-405867a4b84d
2023-06-26 18:46:52,967 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,967 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,967 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,967 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8492c1ff-59e8-402b-9dc7-4f9e618592c1
2023-06-26 18:46:52,968 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,968 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,968 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,969 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,970 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,972 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,973 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,975 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,975 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,977 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,977 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,978 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:52,978 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,978 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,979 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:52,979 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:52,980 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:52,981 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:52,991 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0a4dc5b-e845-49ca-b62b-1c8c6d02543c
2023-06-26 18:46:52,992 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,993 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,993 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,994 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,994 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,995 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,995 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,995 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:52,996 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:52,999 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:53,000 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:53,004 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:53,005 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:46:53,006 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 18:46:53,006 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 18:46:53,014 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45093. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46801. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36721. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37815. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37321. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39967. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38919. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39615. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34265. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37329. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37491. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36201'. Reason: nanny-close
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42843. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40271. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37263. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46649. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,862 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,862 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42033. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:47:43,863 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40645'. Reason: nanny-close
2023-06-26 18:47:43,864 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38273'. Reason: nanny-close
2023-06-26 18:47:43,864 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39709'. Reason: nanny-close
2023-06-26 18:47:43,865 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,865 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36681'. Reason: nanny-close
2023-06-26 18:47:43,865 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33751'. Reason: nanny-close
2023-06-26 18:47:43,866 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36113'. Reason: nanny-close
2023-06-26 18:47:43,866 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,867 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36537'. Reason: nanny-close
2023-06-26 18:47:43,867 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,867 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43309'. Reason: nanny-close
2023-06-26 18:47:43,867 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43735'. Reason: nanny-close
2023-06-26 18:47:43,868 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43461'. Reason: nanny-close
2023-06-26 18:47:43,869 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43625'. Reason: nanny-close
2023-06-26 18:47:43,869 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39683'. Reason: nanny-close
2023-06-26 18:47:43,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34273'. Reason: nanny-close
2023-06-26 18:47:43,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,871 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33573'. Reason: nanny-close
2023-06-26 18:47:43,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:43,871 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46105'. Reason: nanny-close
2023-06-26 18:47:43,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:47:44,228 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36681 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45090 remote=tcp://10.120.104.11:36681>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36681 after 100 s
2023-06-26 18:47:44,229 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36113 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39162 remote=tcp://10.120.104.11:36113>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36113 after 100 s
2023-06-26 18:47:44,236 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34273 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56124 remote=tcp://10.120.104.11:34273>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34273 after 100 s
2023-06-26 18:47:44,237 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38273 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33420 remote=tcp://10.120.104.11:38273>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38273 after 100 s
2023-06-26 18:47:44,238 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36537 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48038 remote=tcp://10.120.104.11:36537>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36537 after 100 s
2023-06-26 18:47:44,237 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39683 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39124 remote=tcp://10.120.104.11:39683>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39683 after 100 s
2023-06-26 18:47:44,242 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33751 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:54662 remote=tcp://10.120.104.11:33751>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33751 after 100 s
2023-06-26 18:47:44,246 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43625 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56766 remote=tcp://10.120.104.11:43625>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43625 after 100 s
2023-06-26 18:47:44,249 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43735 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42620 remote=tcp://10.120.104.11:43735>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43735 after 100 s
2023-06-26 18:47:44,249 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43461 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39670 remote=tcp://10.120.104.11:43461>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43461 after 100 s
2023-06-26 18:47:44,250 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40645 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50402 remote=tcp://10.120.104.11:40645>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40645 after 100 s
2023-06-26 18:47:44,251 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43309 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35078 remote=tcp://10.120.104.11:43309>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43309 after 100 s
2023-06-26 18:47:44,251 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39709 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:43244 remote=tcp://10.120.104.11:39709>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39709 after 100 s
2023-06-26 18:47:44,265 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46105 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52952 remote=tcp://10.120.104.11:46105>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46105 after 100 s
2023-06-26 18:47:44,267 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33573 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47454 remote=tcp://10.120.104.11:33573>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33573 after 100 s
2023-06-26 18:47:47,073 - distributed.nanny - WARNING - Worker process still alive after 3.199994506835938 seconds, killing
2023-06-26 18:47:47,074 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:47:47,074 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:47:47,074 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:47:47,074 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:47:47,075 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:47:47,075 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,075 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,075 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,075 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,076 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,076 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,076 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,077 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,077 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,077 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:47:47,441 - distributed.nanny - INFO - Worker process 391901 was killed by signal 9
2023-06-26 18:47:47,502 - distributed.nanny - INFO - Worker process 391841 was killed by signal 9
2023-06-26 18:47:47,647 - distributed.nanny - INFO - Worker process 391889 was killed by signal 9
2023-06-26 18:47:47,948 - distributed.nanny - INFO - Worker process 391874 was killed by signal 9
2023-06-26 18:47:47,948 - distributed.nanny - INFO - Worker process 391844 was killed by signal 9
2023-06-26 18:47:47,949 - distributed.nanny - INFO - Worker process 391880 was killed by signal 9
2023-06-26 18:47:47,949 - distributed.nanny - INFO - Worker process 391809 was killed by signal 9
2023-06-26 18:47:48,452 - distributed.nanny - INFO - Worker process 391886 was killed by signal 9
2023-06-26 18:47:48,452 - distributed.nanny - INFO - Worker process 391883 was killed by signal 9
2023-06-26 18:47:48,453 - distributed.nanny - INFO - Worker process 391896 was killed by signal 9
2023-06-26 18:47:48,453 - distributed.nanny - INFO - Worker process 391847 was killed by signal 9
2023-06-26 18:47:48,454 - distributed.nanny - INFO - Worker process 391899 was killed by signal 9
2023-06-26 18:47:48,454 - distributed.nanny - INFO - Worker process 391868 was killed by signal 9
2023-06-26 18:47:48,455 - distributed.nanny - INFO - Worker process 391871 was killed by signal 9
2023-06-26 18:47:48,455 - distributed.nanny - INFO - Worker process 391877 was killed by signal 9
2023-06-26 18:47:48,456 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,457 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,457 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,457 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,458 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,458 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,458 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,458 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,458 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:47:48,468 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=391892 parent=389744 started daemon>
2023-06-26 18:47:49,045 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 391892 exit status was already read will report exitcode 255
