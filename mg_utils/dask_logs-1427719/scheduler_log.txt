RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 21:32:58,229 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 21:32:58,720 - distributed.scheduler - INFO - State start
2023-06-22 21:32:58,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-59xws8at', purging
2023-06-22 21:32:58,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-u56dsn65', purging
2023-06-22 21:32:58,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rwc312m7', purging
2023-06-22 21:32:58,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3e09vvzp', purging
2023-06-22 21:32:58,723 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ancgwhir', purging
2023-06-22 21:32:58,723 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-go6325n6', purging
2023-06-22 21:32:58,723 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-frbh3_db', purging
2023-06-22 21:32:58,733 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 21:32:58,734 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 21:32:58,734 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 21:33:09,924 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:43297', status: init, memory: 0, processing: 0>
2023-06-22 21:33:10,179 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:43297
2023-06-22 21:33:10,179 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59060
2023-06-22 21:33:10,180 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:44209', status: init, memory: 0, processing: 0>
2023-06-22 21:33:10,180 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:44209
2023-06-22 21:33:10,180 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59076
2023-06-22 21:33:10,181 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:45021', status: init, memory: 0, processing: 0>
2023-06-22 21:33:10,181 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:45021
2023-06-22 21:33:10,181 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59062
2023-06-22 21:33:10,182 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:42631', status: init, memory: 0, processing: 0>
2023-06-22 21:33:10,182 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:42631
2023-06-22 21:33:10,182 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59084
2023-06-22 21:33:10,260 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:34291', status: init, memory: 0, processing: 0>
2023-06-22 21:33:10,260 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:34291
2023-06-22 21:33:10,260 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59104
2023-06-22 21:33:10,261 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:36329', status: init, memory: 0, processing: 0>
2023-06-22 21:33:10,261 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:36329
2023-06-22 21:33:10,261 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59106
2023-06-22 21:33:10,262 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:42601', status: init, memory: 0, processing: 0>
2023-06-22 21:33:10,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:42601
2023-06-22 21:33:10,262 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59092
2023-06-22 21:33:10,262 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:36465', status: init, memory: 0, processing: 0>
2023-06-22 21:33:10,263 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:36465
2023-06-22 21:33:10,263 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59120
2023-06-22 21:33:16,680 - distributed.scheduler - INFO - Receive client connection: Client-664b3ccf-1144-11ee-89f0-d8c49778ced7
2023-06-22 21:33:16,680 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:59204
2023-06-22 21:33:16,792 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 21:34:06,749 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:34:08,939 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 21:34:12,884 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 21:34:17,548 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-22 21:34:17,549 - distributed.scheduler - INFO - Remove client Client-664b3ccf-1144-11ee-89f0-d8c49778ced7
2023-06-22 21:34:17,556 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:59204; closing.
2023-06-22 21:34:17,556 - distributed.scheduler - INFO - Remove client Client-664b3ccf-1144-11ee-89f0-d8c49778ced7
2023-06-22 21:34:17,557 - distributed.scheduler - INFO - Close client connection: Client-664b3ccf-1144-11ee-89f0-d8c49778ced7
2023-06-22 21:38:02,241 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 21:38:02,242 - distributed.core - INFO - Connection to tcp://10.33.227.169:59084 has been closed.
2023-06-22 21:38:02,243 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:42631', status: running, memory: 0, processing: 0>
2023-06-22 21:38:02,244 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:42631
2023-06-22 21:38:02,244 - distributed.core - INFO - Connection to tcp://10.33.227.169:59106 has been closed.
2023-06-22 21:38:02,244 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:36329', status: running, memory: 0, processing: 0>
2023-06-22 21:38:02,245 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:36329
2023-06-22 21:38:02,245 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 21:38:02,246 - distributed.core - INFO - Connection to tcp://10.33.227.169:59120 has been closed.
2023-06-22 21:38:02,246 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:36465', status: running, memory: 0, processing: 0>
2023-06-22 21:38:02,246 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:36465
2023-06-22 21:38:02,247 - distributed.core - INFO - Connection to tcp://10.33.227.169:59062 has been closed.
2023-06-22 21:38:02,247 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:45021', status: running, memory: 0, processing: 0>
2023-06-22 21:38:02,247 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:45021
2023-06-22 21:38:02,248 - distributed.core - INFO - Connection to tcp://10.33.227.169:59076 has been closed.
2023-06-22 21:38:02,248 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:44209', status: running, memory: 0, processing: 0>
2023-06-22 21:38:02,248 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:44209
2023-06-22 21:38:02,248 - distributed.core - INFO - Connection to tcp://10.33.227.169:59060 has been closed.
2023-06-22 21:38:02,249 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:43297', status: running, memory: 0, processing: 0>
2023-06-22 21:38:02,249 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:43297
2023-06-22 21:38:02,249 - distributed.core - INFO - Connection to tcp://10.33.227.169:59104 has been closed.
2023-06-22 21:38:02,249 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:34291', status: running, memory: 0, processing: 0>
2023-06-22 21:38:02,249 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:34291
2023-06-22 21:38:02,249 - distributed.core - INFO - Connection to tcp://10.33.227.169:59092 has been closed.
2023-06-22 21:38:02,250 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:42601', status: running, memory: 0, processing: 0>
2023-06-22 21:38:02,250 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:42601
2023-06-22 21:38:02,250 - distributed.scheduler - INFO - Lost all workers
2023-06-22 21:38:02,250 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59104>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59104>: Stream is closed
2023-06-22 21:38:02,251 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59120>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:38:02,252 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59092>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59092>: Stream is closed
2023-06-22 21:38:02,252 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59060>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59060>: Stream is closed
2023-06-22 21:38:02,252 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59076>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59076>: Stream is closed
2023-06-22 21:38:02,252 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:59062>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 21:38:02,253 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 21:38:02,255 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 21:38:02,256 - distributed.scheduler - INFO - End scheduler
