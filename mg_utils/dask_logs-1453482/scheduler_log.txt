RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 22:09:01,653 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 22:09:02,144 - distributed.scheduler - INFO - State start
2023-06-22 22:09:02,145 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-65cj4ld5', purging
2023-06-22 22:09:02,145 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-djckg4j5', purging
2023-06-22 22:09:02,145 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-c8wtipx2', purging
2023-06-22 22:09:02,145 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ncpmk9cj', purging
2023-06-22 22:09:02,146 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vssmjc3d', purging
2023-06-22 22:09:02,146 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ckh4653j', purging
2023-06-22 22:09:02,146 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yaxufvil', purging
2023-06-22 22:09:02,146 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-l8ib56a2', purging
2023-06-22 22:09:02,156 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 22:09:02,156 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 22:09:02,157 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 22:09:13,101 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:35123', status: init, memory: 0, processing: 0>
2023-06-22 22:09:13,104 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:35123
2023-06-22 22:09:13,105 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44562
2023-06-22 22:09:13,271 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:34817', status: init, memory: 0, processing: 0>
2023-06-22 22:09:13,271 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:34817
2023-06-22 22:09:13,271 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44566
2023-06-22 22:09:13,273 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:46551', status: init, memory: 0, processing: 0>
2023-06-22 22:09:13,274 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:46551
2023-06-22 22:09:13,274 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44564
2023-06-22 22:09:13,276 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:43727', status: init, memory: 0, processing: 0>
2023-06-22 22:09:13,276 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:43727
2023-06-22 22:09:13,276 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44578
2023-06-22 22:09:13,277 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:37101', status: init, memory: 0, processing: 0>
2023-06-22 22:09:13,277 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:37101
2023-06-22 22:09:13,277 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44586
2023-06-22 22:09:13,318 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:35351', status: init, memory: 0, processing: 0>
2023-06-22 22:09:13,318 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:35351
2023-06-22 22:09:13,318 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44592
2023-06-22 22:09:13,319 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:43173', status: init, memory: 0, processing: 0>
2023-06-22 22:09:13,319 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:43173
2023-06-22 22:09:13,319 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44608
2023-06-22 22:09:13,320 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:39823', status: init, memory: 0, processing: 0>
2023-06-22 22:09:13,320 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:39823
2023-06-22 22:09:13,320 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44622
2023-06-22 22:09:19,621 - distributed.scheduler - INFO - Receive client connection: Client-6f81d274-1149-11ee-ae76-d8c49778ced7
2023-06-22 22:09:19,621 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:44318
2023-06-22 22:09:19,726 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 22:10:10,791 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 22:10:12,997 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 22:10:14,117 - distributed.scheduler - INFO - Remove client Client-6f81d274-1149-11ee-ae76-d8c49778ced7
2023-06-22 22:10:14,119 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:44318; closing.
2023-06-22 22:10:14,120 - distributed.scheduler - INFO - Remove client Client-6f81d274-1149-11ee-ae76-d8c49778ced7
2023-06-22 22:10:14,121 - distributed.scheduler - INFO - Close client connection: Client-6f81d274-1149-11ee-ae76-d8c49778ced7
2023-06-22 22:12:43,269 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 22:12:43,271 - distributed.core - INFO - Connection to tcp://10.33.227.169:44608 has been closed.
2023-06-22 22:12:43,272 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:43173', status: running, memory: 0, processing: 0>
2023-06-22 22:12:43,273 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:43173
2023-06-22 22:12:43,274 - distributed.core - INFO - Connection to tcp://10.33.227.169:44586 has been closed.
2023-06-22 22:12:43,274 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:37101', status: running, memory: 0, processing: 0>
2023-06-22 22:12:43,274 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:37101
2023-06-22 22:12:43,275 - distributed.core - INFO - Connection to tcp://10.33.227.169:44578 has been closed.
2023-06-22 22:12:43,275 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:43727', status: running, memory: 0, processing: 0>
2023-06-22 22:12:43,275 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:43727
2023-06-22 22:12:43,275 - distributed.core - INFO - Connection to tcp://10.33.227.169:44562 has been closed.
2023-06-22 22:12:43,276 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:35123', status: running, memory: 0, processing: 0>
2023-06-22 22:12:43,276 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:35123
2023-06-22 22:12:43,278 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 22:12:43,279 - distributed.core - INFO - Connection to tcp://10.33.227.169:44622 has been closed.
2023-06-22 22:12:43,279 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:39823', status: running, memory: 0, processing: 0>
2023-06-22 22:12:43,279 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:39823
2023-06-22 22:12:43,280 - distributed.core - INFO - Connection to tcp://10.33.227.169:44592 has been closed.
2023-06-22 22:12:43,280 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:35351', status: running, memory: 0, processing: 0>
2023-06-22 22:12:43,280 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:35351
2023-06-22 22:12:43,280 - distributed.core - INFO - Connection to tcp://10.33.227.169:44566 has been closed.
2023-06-22 22:12:43,280 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:34817', status: running, memory: 0, processing: 0>
2023-06-22 22:12:43,280 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:34817
2023-06-22 22:12:43,282 - distributed.core - INFO - Connection to tcp://10.33.227.169:44564 has been closed.
2023-06-22 22:12:43,282 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:46551', status: running, memory: 0, processing: 0>
2023-06-22 22:12:43,282 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:46551
2023-06-22 22:12:43,282 - distributed.scheduler - INFO - Lost all workers
2023-06-22 22:12:43,283 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:44566>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 22:12:43,284 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:44592>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 22:12:43,284 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:44622>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 22:12:43,285 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:44564>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:44564>: Stream is closed
2023-06-22 22:12:43,286 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 22:12:43,289 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 22:12:43,290 - distributed.scheduler - INFO - End scheduler
