RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 20:21:50,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42623'
2023-06-26 20:21:50,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43755'
2023-06-26 20:21:50,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44159'
2023-06-26 20:21:50,769 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36421'
2023-06-26 20:21:50,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33891'
2023-06-26 20:21:50,773 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36815'
2023-06-26 20:21:50,775 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41285'
2023-06-26 20:21:50,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42519'
2023-06-26 20:21:50,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33831'
2023-06-26 20:21:50,781 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34039'
2023-06-26 20:21:50,782 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43209'
2023-06-26 20:21:50,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46843'
2023-06-26 20:21:50,787 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44431'
2023-06-26 20:21:50,790 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43809'
2023-06-26 20:21:50,791 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36323'
2023-06-26 20:21:50,794 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36399'
2023-06-26 20:21:52,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,445 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,445 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,457 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,457 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,458 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,458 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,460 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,460 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,463 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,463 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,476 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,487 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,531 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,531 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,538 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:21:52,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:21:52,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,636 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,636 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,639 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,640 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,661 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,696 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,709 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,711 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,711 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,713 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:52,718 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:21:59,672 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38067
2023-06-26 20:21:59,672 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38067
2023-06-26 20:21:59,672 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34699
2023-06-26 20:21:59,672 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,672 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,672 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,672 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,672 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qobju66m
2023-06-26 20:21:59,673 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c8377d8-1161-4b5e-9221-3200aba8012d
2023-06-26 20:21:59,703 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37467
2023-06-26 20:21:59,703 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37467
2023-06-26 20:21:59,703 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46585
2023-06-26 20:21:59,703 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,703 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,703 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,703 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,703 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cn8eestj
2023-06-26 20:21:59,704 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b7e13fc5-f2c1-4b0e-ac44-b0c4046f813e
2023-06-26 20:21:59,721 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36701
2023-06-26 20:21:59,721 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36701
2023-06-26 20:21:59,721 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44511
2023-06-26 20:21:59,721 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,721 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,721 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,721 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1zu5819u
2023-06-26 20:21:59,722 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1dc8b43-84d0-422c-9922-5670a9f2d1f1
2023-06-26 20:21:59,753 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45079
2023-06-26 20:21:59,753 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45079
2023-06-26 20:21:59,753 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43059
2023-06-26 20:21:59,753 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,753 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,753 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,753 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g3tsymb6
2023-06-26 20:21:59,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cde2987-063c-41cb-b93a-561bb998d45d
2023-06-26 20:21:59,761 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37813
2023-06-26 20:21:59,761 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37813
2023-06-26 20:21:59,762 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43459
2023-06-26 20:21:59,762 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,762 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,762 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,762 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,762 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yymygnqq
2023-06-26 20:21:59,762 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-245d666b-7d5e-4ce8-8406-6af92ef26f9c
2023-06-26 20:21:59,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f2aadc3-bcff-4f8d-b8e8-a2c611346c76
2023-06-26 20:21:59,777 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42803
2023-06-26 20:21:59,777 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42803
2023-06-26 20:21:59,777 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35885
2023-06-26 20:21:59,777 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,777 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,777 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,777 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x_kpb4ok
2023-06-26 20:21:59,778 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-170c0c06-eed2-4a71-b3ce-5efa9688f93f
2023-06-26 20:21:59,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6021746-ae6b-4347-a8ca-ca952e046b2e
2023-06-26 20:21:59,787 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45957
2023-06-26 20:21:59,787 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45957
2023-06-26 20:21:59,787 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42701
2023-06-26 20:21:59,787 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,787 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42115
2023-06-26 20:21:59,787 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,787 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,787 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42115
2023-06-26 20:21:59,787 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dl3vym9v
2023-06-26 20:21:59,787 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33697
2023-06-26 20:21:59,787 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,787 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,787 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,787 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_h0pesha
2023-06-26 20:21:59,788 - distributed.worker - INFO - Starting Worker plugin PreImport-1e7754e6-b331-4fc2-b818-15b0677e0bc3
2023-06-26 20:21:59,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6ad4cf0-1f5e-447d-af08-1714e7a2e12d
2023-06-26 20:21:59,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aaa74e0b-245b-408b-ad1d-ac0b9c8e0c36
2023-06-26 20:21:59,807 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39107
2023-06-26 20:21:59,807 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39107
2023-06-26 20:21:59,807 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43699
2023-06-26 20:21:59,807 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,807 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,807 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,808 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-426z4m72
2023-06-26 20:21:59,808 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6049921c-cd34-4e6b-b600-b6f21bd4d788
2023-06-26 20:21:59,815 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39767
2023-06-26 20:21:59,815 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39767
2023-06-26 20:21:59,815 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34371
2023-06-26 20:21:59,815 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,816 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,816 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,816 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,816 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m5n2__ex
2023-06-26 20:21:59,817 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4164650b-fb7d-4aaf-90d8-b39770b1f1dd
2023-06-26 20:21:59,817 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46639
2023-06-26 20:21:59,818 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46639
2023-06-26 20:21:59,818 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42355
2023-06-26 20:21:59,818 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,818 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,818 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,818 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,818 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f9ai4r0x
2023-06-26 20:21:59,818 - distributed.worker - INFO - Starting Worker plugin PreImport-10c961aa-1b71-4d88-b5a7-48df81d8645a
2023-06-26 20:21:59,818 - distributed.worker - INFO - Starting Worker plugin RMMSetup-761e429e-0384-40f6-b642-b1d6d459eaaa
2023-06-26 20:21:59,824 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45307
2023-06-26 20:21:59,825 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45307
2023-06-26 20:21:59,825 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42199
2023-06-26 20:21:59,825 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,825 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,825 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fm4lg62o
2023-06-26 20:21:59,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d6a4c95-bcde-46c8-b995-5c166ae0f2d4
2023-06-26 20:21:59,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64c09c0c-ab74-49ee-9bc1-01e24f1e17b4
2023-06-26 20:21:59,832 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41153
2023-06-26 20:21:59,832 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41153
2023-06-26 20:21:59,832 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34007
2023-06-26 20:21:59,832 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,832 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,832 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,832 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bwm6rxsg
2023-06-26 20:21:59,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1d2e5b0-2c74-4698-a2a8-9cb5e9630259
2023-06-26 20:21:59,862 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42139
2023-06-26 20:21:59,862 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42139
2023-06-26 20:21:59,862 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33625
2023-06-26 20:21:59,862 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,862 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,862 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,862 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e0ysmg4l
2023-06-26 20:21:59,863 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0442c2b6-6f10-449a-84cc-60ee837299bc
2023-06-26 20:21:59,863 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42953
2023-06-26 20:21:59,863 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42953
2023-06-26 20:21:59,863 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33977
2023-06-26 20:21:59,863 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,863 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,863 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,863 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-el5qtqxd
2023-06-26 20:21:59,864 - distributed.worker - INFO - Starting Worker plugin PreImport-f148b404-45e5-4215-8b4d-51c91f246395
2023-06-26 20:21:59,864 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f97d276-bd42-4711-8e6b-8b2588ebd07c
2023-06-26 20:21:59,879 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46027
2023-06-26 20:21:59,879 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46027
2023-06-26 20:21:59,879 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34915
2023-06-26 20:21:59,879 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:21:59,879 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:21:59,879 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:21:59,880 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:21:59,880 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jaz_mu6x
2023-06-26 20:21:59,881 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4a27b12-618c-48b6-b16f-c440c98fdeba
2023-06-26 20:22:03,426 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-501727d2-95fa-487b-b1ca-32e62e472376
2023-06-26 20:22:03,427 - distributed.worker - INFO - Starting Worker plugin PreImport-97dd9c14-7524-4737-beea-ba9744485a9c
2023-06-26 20:22:03,429 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,458 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,458 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,461 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,481 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-841e9805-df70-48b2-a283-c27ac44aabe1
2023-06-26 20:22:03,484 - distributed.worker - INFO - Starting Worker plugin PreImport-e40f677b-72e4-452e-8b41-444ea1551257
2023-06-26 20:22:03,487 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,515 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,515 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,518 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40d59539-f8c1-43ac-af53-7813dd26bd24
2023-06-26 20:22:03,560 - distributed.worker - INFO - Starting Worker plugin PreImport-998000e5-9cfb-439a-8f6b-e0c156257423
2023-06-26 20:22:03,562 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,588 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8fafb16b-8318-421b-bc55-2a988d013f56
2023-06-26 20:22:03,588 - distributed.worker - INFO - Starting Worker plugin PreImport-a6829924-1238-444b-b76c-8c36767b5757
2023-06-26 20:22:03,588 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,602 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,602 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,603 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,603 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,604 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,606 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,662 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e36dfd9e-7599-4f5f-ad69-54f8c0de2e4b
2023-06-26 20:22:03,662 - distributed.worker - INFO - Starting Worker plugin PreImport-58a0b75f-f90d-49cf-b023-9451fc60a36e
2023-06-26 20:22:03,664 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,666 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af0d2921-ecb3-4ad5-a012-ff791fd2574b
2023-06-26 20:22:03,667 - distributed.worker - INFO - Starting Worker plugin PreImport-c7de2a92-f361-415a-9700-9ef51e985c83
2023-06-26 20:22:03,668 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,692 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2cb47ce2-64a2-425e-bada-c3bc98bd808f
2023-06-26 20:22:03,692 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24e8f319-dd55-45ff-a1d2-bbf2924e0958
2023-06-26 20:22:03,693 - distributed.worker - INFO - Starting Worker plugin PreImport-e3d3adc1-6044-49ed-a11c-efab64f743c8
2023-06-26 20:22:03,693 - distributed.worker - INFO - Starting Worker plugin PreImport-f4740b3c-9c95-419e-a702-c34a002a019d
2023-06-26 20:22:03,693 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,695 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,695 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,695 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,696 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,696 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,697 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,697 - distributed.worker - INFO - Starting Worker plugin PreImport-d29ead60-d9fa-416e-956a-948f63fa1a94
2023-06-26 20:22:03,699 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,699 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,709 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,709 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,711 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,717 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0d3f6f9-b201-42d6-8c1d-62d3e72ef084
2023-06-26 20:22:03,718 - distributed.worker - INFO - Starting Worker plugin PreImport-93e98305-7d92-47d0-9902-cc251bc67207
2023-06-26 20:22:03,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,722 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,722 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,724 - distributed.worker - INFO - Starting Worker plugin PreImport-c87497cd-1f61-46c2-9b57-48169b6fa174
2023-06-26 20:22:03,724 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,726 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,729 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,729 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,732 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,732 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8baa7a26-7121-4730-a080-dc8000341855
2023-06-26 20:22:03,733 - distributed.worker - INFO - Starting Worker plugin PreImport-d806c39f-27df-46a1-8248-73d080a64f54
2023-06-26 20:22:03,733 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,735 - distributed.worker - INFO - Starting Worker plugin PreImport-a30454ec-5389-4525-8257-44d0a12f8fe5
2023-06-26 20:22:03,736 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,737 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,737 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,739 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,748 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,748 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,750 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,757 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,757 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,758 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,758 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7accdf2-3839-405d-9574-f01958093873
2023-06-26 20:22:03,758 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,759 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,760 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,762 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ced9279-834b-4e20-85d6-2c8863295656
2023-06-26 20:22:03,764 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01749135-a063-4d3b-98f6-07afb4401a5e
2023-06-26 20:22:03,765 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,770 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,770 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,772 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,777 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,777 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,779 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:03,792 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:22:03,792 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:22:03,795 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:22:07,606 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,606 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,606 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,606 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,606 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,607 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,607 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,607 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,607 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,608 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,608 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,609 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,609 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,609 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,610 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,610 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,619 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:07,620 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:22:08,301 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,301 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,301 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,302 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:08,303 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:22:11,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:11,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:23,338 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,563 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,595 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,608 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,677 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,696 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,810 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,867 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,909 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,917 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,957 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:23,975 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:24,043 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:24,072 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:24,075 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:26,988 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:22:32,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:22:32,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:38,290 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,290 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,291 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,294 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,296 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,296 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,296 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,296 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,296 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,296 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,296 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,296 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,297 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,297 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,297 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:38,301 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:23:45,184 - distributed.worker - WARNING - Compute Failed
Key:       ('_replicate_df-dea506e4504148dceafa7aa442ec635c', 1)
Function:  subgraph_callable-91ec917f-03c4-49a6-92f6-7d04e34d
args:      ({'number': 1, 'division': None}, 4, {'src': 111059956, 'dst': 111059956}, 'assign-d9c0c9223e080bc9969a0b272804e322', 'dst', 0,                  src       dst
100980368  108886467   9374596
100980369   36743203   9374596
100980370   64292340   9374596
100980371   12716162   9374596
100980372   25478427   9374596
...              ...       ...
201960731  108565589  14463884
201960732   18176435  14463885
201960733   85995975  14463886
201960734   80797653  14463886
201960735   71487912  14463886

[100980368 rows x 2 columns], 'src')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/rapids/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory')"

2023-06-26 20:23:45,237 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,238 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,238 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,238 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,239 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,239 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,240 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,240 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,240 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,240 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,241 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,242 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,242 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,243 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,246 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:45,247 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:23:49,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:23:49,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,716 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36701. Reason: scheduler-restart
2023-06-26 20:23:49,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,717 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,718 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,718 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,718 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37813. Reason: scheduler-restart
2023-06-26 20:23:49,719 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,719 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,719 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,720 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,720 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39107. Reason: scheduler-restart
2023-06-26 20:23:49,720 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,720 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37467. Reason: scheduler-restart
2023-06-26 20:23:49,721 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42115. Reason: scheduler-restart
2023-06-26 20:23:49,721 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42139. Reason: scheduler-restart
2023-06-26 20:23:49,721 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,721 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,721 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,721 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,722 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,722 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,722 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41153. Reason: scheduler-restart
2023-06-26 20:23:49,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,722 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,722 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39767. Reason: scheduler-restart
2023-06-26 20:23:49,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,723 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38067. Reason: scheduler-restart
2023-06-26 20:23:49,723 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,723 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,724 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,724 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,725 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45307. Reason: scheduler-restart
2023-06-26 20:23:49,725 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 20:23:49,726 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42803. Reason: scheduler-restart
2023-06-26 20:23:49,726 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,727 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46027. Reason: scheduler-restart
2023-06-26 20:23:49,727 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,727 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,727 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,728 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,728 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42953. Reason: scheduler-restart
2023-06-26 20:23:49,728 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,728 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,728 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,728 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,728 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,728 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,728 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,729 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,729 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,729 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,729 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45079. Reason: scheduler-restart
2023-06-26 20:23:49,729 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,729 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45957. Reason: scheduler-restart
2023-06-26 20:23:49,729 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,730 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,730 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36701
2023-06-26 20:23:49,730 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,730 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,730 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,730 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,730 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,731 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,731 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,731 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,731 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,731 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,731 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,731 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,731 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,731 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,731 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,731 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46639. Reason: scheduler-restart
2023-06-26 20:23:49,731 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,732 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,734 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,735 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,735 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,735 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41153
2023-06-26 20:23:49,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46027
2023-06-26 20:23:49,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45307
2023-06-26 20:23:49,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37467
2023-06-26 20:23:49,736 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38067
2023-06-26 20:23:49,736 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41153
2023-06-26 20:23:49,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37813
2023-06-26 20:23:49,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46027
2023-06-26 20:23:49,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39107
2023-06-26 20:23:49,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45307
2023-06-26 20:23:49,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42139
2023-06-26 20:23:49,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37467
2023-06-26 20:23:49,737 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38067
2023-06-26 20:23:49,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41153
2023-06-26 20:23:49,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46027
2023-06-26 20:23:49,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45307
2023-06-26 20:23:49,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37467
2023-06-26 20:23:49,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38067
2023-06-26 20:23:49,738 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,738 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42953
2023-06-26 20:23:49,738 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,739 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41153
2023-06-26 20:23:49,739 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46027
2023-06-26 20:23:49,739 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45307
2023-06-26 20:23:49,739 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37467
2023-06-26 20:23:49,739 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38067
2023-06-26 20:23:49,740 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42953
2023-06-26 20:23:49,740 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42115
2023-06-26 20:23:49,740 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,740 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41153
2023-06-26 20:23:49,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46027
2023-06-26 20:23:49,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45307
2023-06-26 20:23:49,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37467
2023-06-26 20:23:49,741 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38067
2023-06-26 20:23:49,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42953
2023-06-26 20:23:49,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42115
2023-06-26 20:23:49,742 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42803
2023-06-26 20:23:49,742 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,742 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,744 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42115
2023-06-26 20:23:49,744 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,744 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42803
2023-06-26 20:23:49,744 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39767
2023-06-26 20:23:49,744 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45079
2023-06-26 20:23:49,745 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,746 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,752 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,756 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,757 - distributed.worker - ERROR - failed during get data with tcp://10.120.104.11:46027 -> tcp://10.120.104.11:46639
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1780, in get_data
    response = await comm.read(deserializers=serializers)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46027 remote=tcp://10.120.104.11:58896>: ConnectionResetError: [Errno 104] Connection reset by peer
2023-06-26 20:23:49,758 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41153
2023-06-26 20:23:49,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46027
2023-06-26 20:23:49,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45307
2023-06-26 20:23:49,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37467
2023-06-26 20:23:49,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38067
2023-06-26 20:23:49,759 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42953
2023-06-26 20:23:49,761 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,761 - distributed.worker - ERROR - failed during get data with tcp://10.120.104.11:42953 -> tcp://10.120.104.11:45079
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1124, in write_to_fd
    return self.socket.send(data)  # type: ignore
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1780, in get_data
    response = await comm.read(deserializers=serializers)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42953 remote=tcp://10.120.104.11:38414>: ConnectionResetError: [Errno 104] Connection reset by peer
2023-06-26 20:23:49,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42115
2023-06-26 20:23:49,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42803
2023-06-26 20:23:49,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39767
2023-06-26 20:23:49,761 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45079
2023-06-26 20:23:49,761 - distributed.worker - ERROR - failed during get data with tcp://10.120.104.11:41153 -> tcp://10.120.104.11:42115
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1124, in write_to_fd
    return self.socket.send(data)  # type: ignore
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1780, in get_data
    response = await comm.read(deserializers=serializers)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41153 remote=tcp://10.120.104.11:58242>: ConnectionResetError: [Errno 104] Connection reset by peer
2023-06-26 20:23:49,762 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,762 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45957
2023-06-26 20:23:49,762 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 20:23:49,763 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,763 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,767 - distributed.nanny - INFO - Worker closed
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f98382510c0, tag: 0xcea3db74ce85f7f3>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f98382510c0, tag: 0xcea3db74ce85f7f3>: 
2023-06-26 20:23:49,784 - distributed.nanny - INFO - Worker closed
2023-06-26 20:23:49,813 - distributed.nanny - INFO - Worker closed
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-11098' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
2023-06-26 20:23:53,414 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:56,719 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:56,720 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,475 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:57,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:57,482 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,483 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,485 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:57,722 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,723 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,725 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,738 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,739 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,741 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,755 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,756 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,758 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:57,763 - distributed.nanny - WARNING - Restarting worker
2023-06-26 20:23:58,225 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:58,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:58,247 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:58,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:58,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:58,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:58,948 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45471
2023-06-26 20:23:58,949 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45471
2023-06-26 20:23:58,949 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42153
2023-06-26 20:23:58,949 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:23:58,949 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:23:58,949 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:23:58,949 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:23:58,949 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1p9uia_r
2023-06-26 20:23:58,949 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09539f82-14b8-4f08-af89-a0733314f32d
2023-06-26 20:23:59,093 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,093 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,101 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,101 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,276 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,281 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,282 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,427 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,456 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,465 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,465 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,491 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,491 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,494 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,499 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,510 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:23:59,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:23:59,574 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,577 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6397afa9-61da-4b94-b6b5-fe4059445b90
2023-06-26 20:23:59,577 - distributed.worker - INFO - Starting Worker plugin PreImport-f7bfd8f1-6375-47ef-b67f-b33ef28bf015
2023-06-26 20:23:59,578 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:23:59,591 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:23:59,591 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:23:59,593 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:23:59,637 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,672 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,839 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:23:59,905 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36875
2023-06-26 20:23:59,906 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36875
2023-06-26 20:23:59,906 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33845
2023-06-26 20:23:59,906 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:23:59,906 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:23:59,906 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:23:59,906 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:23:59,906 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f8shw1us
2023-06-26 20:23:59,906 - distributed.worker - INFO - Starting Worker plugin PreImport-c138d11d-1c26-4431-861a-e34605e83434
2023-06-26 20:23:59,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31f2a21e-b0ba-4b2b-adcb-4d6b4f930a81
2023-06-26 20:23:59,930 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33557
2023-06-26 20:23:59,930 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33557
2023-06-26 20:23:59,931 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42287
2023-06-26 20:23:59,931 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:23:59,931 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:23:59,931 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:23:59,931 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:23:59,931 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h1roj8h3
2023-06-26 20:23:59,931 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d373bcb8-197d-46e8-b606-8af575fbc745
2023-06-26 20:23:59,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b7d3484-3f3b-4899-a48e-b66a8fceb3fc
2023-06-26 20:24:02,184 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8516eb86-f46b-4dc3-a8f6-fcaa0ac38739
2023-06-26 20:24:02,185 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:02,196 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:02,196 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:02,197 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:02,202 - distributed.worker - INFO - Starting Worker plugin PreImport-acf167a9-2691-4a1f-b29c-1f6edc22a28e
2023-06-26 20:24:02,204 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:02,215 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:02,215 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:02,217 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:05,195 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42721
2023-06-26 20:24:05,196 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42721
2023-06-26 20:24:05,196 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44859
2023-06-26 20:24:05,196 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,196 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,196 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,196 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,196 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jeminlbw
2023-06-26 20:24:05,196 - distributed.worker - INFO - Starting Worker plugin PreImport-6c0dd7d6-4556-4d84-b7dc-fe0c51c527fb
2023-06-26 20:24:05,197 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f4c42a6-848b-4a71-a3bc-e1baae77ed3a
2023-06-26 20:24:05,343 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36401
2023-06-26 20:24:05,344 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36401
2023-06-26 20:24:05,344 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37111
2023-06-26 20:24:05,344 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,344 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,344 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y7o37rjg
2023-06-26 20:24:05,344 - distributed.worker - INFO - Starting Worker plugin PreImport-338bff21-5818-495f-8b18-e4e7b4565abd
2023-06-26 20:24:05,344 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38afcf68-c9b8-45e5-bbd9-fe09d9128a20
2023-06-26 20:24:05,371 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44763
2023-06-26 20:24:05,371 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44763
2023-06-26 20:24:05,371 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34701
2023-06-26 20:24:05,371 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,371 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,371 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,371 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,372 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aqkn0mmx
2023-06-26 20:24:05,372 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82b9b4b7-c027-42e0-b069-adfeb3368153
2023-06-26 20:24:05,372 - distributed.worker - INFO - Starting Worker plugin PreImport-a6e5f55b-a2cf-4600-9661-eddcb750a77d
2023-06-26 20:24:05,372 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f4dfb23-4683-4023-a4ff-c248cf2a9d85
2023-06-26 20:24:05,394 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43493
2023-06-26 20:24:05,394 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43493
2023-06-26 20:24:05,394 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38815
2023-06-26 20:24:05,394 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,394 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,394 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,395 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,395 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v2p_pbed
2023-06-26 20:24:05,395 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e74b92a-ae77-4288-9a2f-cfffb9681b27
2023-06-26 20:24:05,395 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd5762fe-a89c-43b7-b4fd-0651de9ff4ef
2023-06-26 20:24:05,404 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42297
2023-06-26 20:24:05,405 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42297
2023-06-26 20:24:05,405 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40735
2023-06-26 20:24:05,405 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,405 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,405 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,405 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7a00ppw8
2023-06-26 20:24:05,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb7c3f27-b81e-4ac0-bee5-b8363c3fef78
2023-06-26 20:24:05,486 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33673
2023-06-26 20:24:05,486 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33673
2023-06-26 20:24:05,486 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33011
2023-06-26 20:24:05,486 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,486 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,487 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,487 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,487 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-34ir4xiy
2023-06-26 20:24:05,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67088d2a-09e5-4892-81b2-a64be1c394ad
2023-06-26 20:24:05,505 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35619
2023-06-26 20:24:05,505 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35619
2023-06-26 20:24:05,505 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34783
2023-06-26 20:24:05,505 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,505 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,505 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,505 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,505 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rflbypwo
2023-06-26 20:24:05,506 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f693e3c6-ae8d-405f-a9dc-88d0c28de347
2023-06-26 20:24:05,541 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45119
2023-06-26 20:24:05,541 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45119
2023-06-26 20:24:05,541 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39209
2023-06-26 20:24:05,541 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,541 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,541 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,541 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,541 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r55v8kl_
2023-06-26 20:24:05,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1583b87c-df11-4592-9d77-0b66dc53b752
2023-06-26 20:24:05,632 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46589
2023-06-26 20:24:05,632 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46589
2023-06-26 20:24:05,632 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34091
2023-06-26 20:24:05,632 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,632 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,632 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,632 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gad983f7
2023-06-26 20:24:05,633 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa507314-3403-498e-8069-dade5f7ee2b6
2023-06-26 20:24:05,795 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42999
2023-06-26 20:24:05,795 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42999
2023-06-26 20:24:05,795 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43613
2023-06-26 20:24:05,795 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,795 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,795 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,795 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ogwp1vf
2023-06-26 20:24:05,796 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f446578-f389-452e-bb48-c23a2ed75dc2
2023-06-26 20:24:05,832 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42241
2023-06-26 20:24:05,832 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42241
2023-06-26 20:24:05,833 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39821
2023-06-26 20:24:05,833 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,833 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,833 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_l7hdin4
2023-06-26 20:24:05,834 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a36094ba-892e-442b-83d0-3265f8133e9a
2023-06-26 20:24:05,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e42afb6-2189-4939-871e-6cd035a94dd4
2023-06-26 20:24:05,854 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46161
2023-06-26 20:24:05,854 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46161
2023-06-26 20:24:05,854 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42343
2023-06-26 20:24:05,854 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,854 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,854 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,854 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38363
2023-06-26 20:24:05,854 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,854 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38363
2023-06-26 20:24:05,854 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1ci7hbbn
2023-06-26 20:24:05,854 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34953
2023-06-26 20:24:05,854 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:24:05,854 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:05,854 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:24:05,854 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:24:05,854 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-89erdmfx
2023-06-26 20:24:05,855 - distributed.worker - INFO - Starting Worker plugin PreImport-783c464a-8e01-450b-86a4-418c0da3a4b0
2023-06-26 20:24:05,855 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96167d9b-9c11-45e8-b4e4-66d481c78149
2023-06-26 20:24:05,855 - distributed.worker - INFO - Starting Worker plugin PreImport-6a0e7604-ba97-4e69-9103-3875272e6541
2023-06-26 20:24:05,855 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ae1475a-2303-4319-ab47-598503693f49
2023-06-26 20:24:08,806 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dcabf672-637b-4fe9-a290-a856cbf43e29
2023-06-26 20:24:08,807 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,830 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:08,830 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,832 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:08,864 - distributed.worker - INFO - Starting Worker plugin PreImport-f74099a7-7cfa-4957-b0f8-b735d15eaa54
2023-06-26 20:24:08,867 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,875 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ece5cada-7ff0-4287-9543-43a229cd93d2
2023-06-26 20:24:08,876 - distributed.worker - INFO - Starting Worker plugin PreImport-dbdd89c7-367b-43f9-b223-d2c1df375a64
2023-06-26 20:24:08,876 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,888 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:08,888 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,894 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:08,894 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:08,894 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,903 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:08,922 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84056e8c-3434-466c-9953-32244b07ff8c
2023-06-26 20:24:08,924 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,943 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:08,943 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,950 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:08,957 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-575b45ac-1fbb-4a61-bdb2-5c3f4f2ca8f1
2023-06-26 20:24:08,958 - distributed.worker - INFO - Starting Worker plugin PreImport-b207241e-c7d3-4b0b-b91a-4ae16e7946b2
2023-06-26 20:24:08,960 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,972 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,979 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:08,979 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,980 - distributed.worker - INFO - Starting Worker plugin PreImport-a0587cd3-253d-4c86-97f7-69223ce2fefe
2023-06-26 20:24:08,981 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,985 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54090f95-6303-4440-8f05-51a83aea2107
2023-06-26 20:24:08,985 - distributed.worker - INFO - Starting Worker plugin PreImport-15ea50a9-8334-440a-a56b-91e96f66b4d2
2023-06-26 20:24:08,986 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,986 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:08,986 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,987 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:08,988 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:08,995 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:08,995 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:08,996 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:08,996 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,000 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:09,002 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:09,032 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f40b6ca-904b-4994-a1ed-7ad512f77b2f
2023-06-26 20:24:09,033 - distributed.worker - INFO - Starting Worker plugin PreImport-720deaed-13cf-48df-913f-b488be48d1fe
2023-06-26 20:24:09,034 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,046 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2001249e-7480-4a73-9c4e-99bf8d990bf8
2023-06-26 20:24:09,047 - distributed.worker - INFO - Starting Worker plugin PreImport-fd7c9cdd-6868-4476-b4bd-e12a82747a7e
2023-06-26 20:24:09,048 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:09,048 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,049 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,049 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:09,070 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3083bfa5-be11-4c31-a098-cf3a972ff26c
2023-06-26 20:24:09,071 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:09,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,073 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:09,079 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e13cd0d3-d959-447a-be00-a0c489e3740e
2023-06-26 20:24:09,082 - distributed.worker - INFO - Starting Worker plugin PreImport-ad9aa367-a94b-4829-8221-b460293686cd
2023-06-26 20:24:09,084 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0eec9b8d-532d-4617-816d-cb16e615bc76
2023-06-26 20:24:09,090 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,100 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:09,100 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,102 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:09,102 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,102 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:24:09,102 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:24:09,103 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:09,105 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:09,109 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:24:18,246 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,468 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,619 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,668 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,702 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,748 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,799 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,845 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,850 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,860 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,871 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,894 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,911 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:18,938 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:18,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:19,007 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:19,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:19,072 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 20:24:19,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:19,084 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,084 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,084 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,084 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,084 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,084 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,084 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,085 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,094 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687811059.095079] [exp01:486085:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:37319) failed: Address already in use
2023-06-26 20:24:19,095 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,095 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,095 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,095 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,095 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,095 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,095 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,106 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:19,107 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:24:22,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:29,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:30,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:30,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:30,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:24:30,085 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,085 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,085 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,085 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,085 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,085 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,085 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:30,086 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:24:41,883 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 20:25:46,701 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42241. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,701 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42999. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,701 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45471. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,701 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38363. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,701 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43493. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,701 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33673. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,701 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42721. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,701 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46589. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36401. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42297. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33557. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36875. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35619. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45119. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44763. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46161. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:25:46,702 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42623'. Reason: nanny-close
2023-06-26 20:25:46,703 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43755'. Reason: nanny-close
2023-06-26 20:25:46,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,705 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42519'. Reason: nanny-close
2023-06-26 20:25:46,705 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33831'. Reason: nanny-close
2023-06-26 20:25:46,706 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34039'. Reason: nanny-close
2023-06-26 20:25:46,706 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43209'. Reason: nanny-close
2023-06-26 20:25:46,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43809'. Reason: nanny-close
2023-06-26 20:25:46,707 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36323'. Reason: nanny-close
2023-06-26 20:25:46,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36399'. Reason: nanny-close
2023-06-26 20:25:46,708 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,709 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44159'. Reason: nanny-close
2023-06-26 20:25:46,709 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,709 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36421'. Reason: nanny-close
2023-06-26 20:25:46,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,710 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33891'. Reason: nanny-close
2023-06-26 20:25:46,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36815'. Reason: nanny-close
2023-06-26 20:25:46,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41285'. Reason: nanny-close
2023-06-26 20:25:46,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,712 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46843'. Reason: nanny-close
2023-06-26 20:25:46,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44431'. Reason: nanny-close
2023-06-26 20:25:46,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:25:46,714 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42623 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:37086 remote=tcp://10.120.104.11:42623>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42623 after 100 s
2023-06-26 20:25:46,723 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34039 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:44210 remote=tcp://10.120.104.11:34039>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34039 after 100 s
2023-06-26 20:25:46,723 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43755 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40300 remote=tcp://10.120.104.11:43755>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43755 after 100 s
2023-06-26 20:25:46,725 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44159 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60742 remote=tcp://10.120.104.11:44159>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44159 after 100 s
2023-06-26 20:25:46,725 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42519 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42904 remote=tcp://10.120.104.11:42519>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42519 after 100 s
2023-06-26 20:25:46,727 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43209 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34876 remote=tcp://10.120.104.11:43209>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43209 after 100 s
2023-06-26 20:25:46,727 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36323 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50768 remote=tcp://10.120.104.11:36323>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36323 after 100 s
2023-06-26 20:25:46,728 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36815 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36650 remote=tcp://10.120.104.11:36815>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36815 after 100 s
2023-06-26 20:25:46,729 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36421 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35878 remote=tcp://10.120.104.11:36421>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36421 after 100 s
2023-06-26 20:25:46,730 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33891 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39574 remote=tcp://10.120.104.11:33891>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33891 after 100 s
2023-06-26 20:25:46,732 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33831 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45728 remote=tcp://10.120.104.11:33831>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33831 after 100 s
2023-06-26 20:25:46,732 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41285 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47052 remote=tcp://10.120.104.11:41285>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41285 after 100 s
2023-06-26 20:25:46,736 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43809 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41168 remote=tcp://10.120.104.11:43809>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43809 after 100 s
2023-06-26 20:25:46,736 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44431 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33548 remote=tcp://10.120.104.11:44431>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44431 after 100 s
2023-06-26 20:25:46,738 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46843 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41080 remote=tcp://10.120.104.11:46843>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46843 after 100 s
2023-06-26 20:25:46,738 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36399 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:42450 remote=tcp://10.120.104.11:36399>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36399 after 100 s
2023-06-26 20:25:49,915 - distributed.nanny - WARNING - Worker process still alive after 3.1999955749511724 seconds, killing
2023-06-26 20:25:49,915 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:25:49,916 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:25:49,916 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 20:25:49,916 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:25:49,917 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:25:49,919 - distributed.nanny - WARNING - Worker process still alive after 3.19999984741211 seconds, killing
2023-06-26 20:25:49,920 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:25:49,920 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:25:49,921 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:25:49,922 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:25:49,922 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:25:49,922 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-26 20:25:49,922 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-26 20:25:49,923 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 20:25:49,924 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:25:50,704 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,706 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,706 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,706 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,707 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,707 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,707 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,709 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,709 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,709 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,711 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,711 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,711 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,712 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,713 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,714 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486112 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486109 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486106 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486103 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486100 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486097 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486094 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486091 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486088 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486085 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486082 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486079 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486076 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486062 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486059 parent=483034 started daemon>
2023-06-26 20:25:50,716 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=486024 parent=483034 started daemon>
2023-06-26 20:25:54,431 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 486079 exit status was already read will report exitcode 255
2023-06-26 20:25:55,958 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 486097 exit status was already read will report exitcode 255
2023-06-26 20:25:56,216 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 486112 exit status was already read will report exitcode 255
