RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 18:36:04,125 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 18:36:04,645 - distributed.scheduler - INFO - State start
2023-06-26 18:36:04,646 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yn7gfpu9', purging
2023-06-26 18:36:04,646 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-21mp0oxi', purging
2023-06-26 18:36:04,646 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4vvkx31h', purging
2023-06-26 18:36:04,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3pk6dijv', purging
2023-06-26 18:36:04,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-u3m0an4_', purging
2023-06-26 18:36:04,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o1whew_9', purging
2023-06-26 18:36:04,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rqtn0x_j', purging
2023-06-26 18:36:04,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-tozm6wub', purging
2023-06-26 18:36:04,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-w60u5scy', purging
2023-06-26 18:36:04,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-tidjinwd', purging
2023-06-26 18:36:04,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-sx1u4tt_', purging
2023-06-26 18:36:04,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-h2bb24jt', purging
2023-06-26 18:36:04,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2a0y5i5y', purging
2023-06-26 18:36:04,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-zt8u4d0t', purging
2023-06-26 18:36:04,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-a9dnk7uw', purging
2023-06-26 18:36:04,649 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-j2v8ty_m', purging
2023-06-26 18:36:04,661 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 18:36:04,662 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 18:36:04,662 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 18:36:23,193 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:45153', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,195 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:45153
2023-06-26 18:36:23,195 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60948
2023-06-26 18:36:23,378 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39459', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,379 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39459
2023-06-26 18:36:23,379 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60960
2023-06-26 18:36:23,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33149', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,478 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33149
2023-06-26 18:36:23,478 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60962
2023-06-26 18:36:23,513 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36317', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,513 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36317
2023-06-26 18:36:23,513 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60978
2023-06-26 18:36:23,537 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39209', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39209
2023-06-26 18:36:23,537 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60980
2023-06-26 18:36:23,557 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36073', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,557 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36073
2023-06-26 18:36:23,557 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60992
2023-06-26 18:36:23,560 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38695', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,560 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38695
2023-06-26 18:36:23,560 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60988
2023-06-26 18:36:23,568 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40621', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40621
2023-06-26 18:36:23,569 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32772
2023-06-26 18:36:23,614 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43965', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,615 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43965
2023-06-26 18:36:23,615 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32774
2023-06-26 18:36:23,682 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33315', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,683 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33315
2023-06-26 18:36:23,683 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32784
2023-06-26 18:36:23,704 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40041', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,705 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40041
2023-06-26 18:36:23,705 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32800
2023-06-26 18:36:23,709 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34727', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,710 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34727
2023-06-26 18:36:23,710 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32804
2023-06-26 18:36:23,711 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41549', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,711 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41549
2023-06-26 18:36:23,711 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32806
2023-06-26 18:36:23,734 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41837', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41837
2023-06-26 18:36:23,734 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32820
2023-06-26 18:36:23,736 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46801', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,736 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46801
2023-06-26 18:36:23,736 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32810
2023-06-26 18:36:23,750 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33719', status: init, memory: 0, processing: 0>
2023-06-26 18:36:23,751 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33719
2023-06-26 18:36:23,751 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:32832
2023-06-26 18:36:32,791 - distributed.scheduler - INFO - Receive client connection: Client-5f88eb60-1450-11ee-8cbf-5cff35c1a711
2023-06-26 18:36:32,791 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:43270
2023-06-26 18:36:33,491 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 18:37:25,436 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 18:38:18,093 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 18:38:18,095 - distributed.scheduler - INFO - Restarting workers and releasing all keys.
2023-06-26 18:38:18,116 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:60962; closing.
2023-06-26 18:38:18,117 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33149', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,117 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33149
2023-06-26 18:38:18,118 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32784; closing.
2023-06-26 18:38:18,118 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32832; closing.
2023-06-26 18:38:18,119 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33315', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33315
2023-06-26 18:38:18,119 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33719', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,119 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33719
2023-06-26 18:38:18,120 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32804; closing.
2023-06-26 18:38:18,120 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34727', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,120 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34727
2023-06-26 18:38:18,121 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:60992; closing.
2023-06-26 18:38:18,121 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:60978; closing.
2023-06-26 18:38:18,121 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:60988; closing.
2023-06-26 18:38:18,121 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36073', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,121 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36073
2023-06-26 18:38:18,121 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36317', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,122 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36317
2023-06-26 18:38:18,122 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38695', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,122 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38695
2023-06-26 18:38:18,122 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:60980; closing.
2023-06-26 18:38:18,123 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39209', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,123 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39209
2023-06-26 18:38:18,123 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:60960; closing.
2023-06-26 18:38:18,124 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39459', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,124 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39459
2023-06-26 18:38:18,124 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32800; closing.
2023-06-26 18:38:18,124 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40041', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,124 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40041
2023-06-26 18:38:18,125 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:32800>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:38:18,126 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32772; closing.
2023-06-26 18:38:18,126 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40621', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,127 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40621
2023-06-26 18:38:18,128 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32774; closing.
2023-06-26 18:38:18,128 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43965', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,128 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43965
2023-06-26 18:38:18,130 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32820; closing.
2023-06-26 18:38:18,130 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41837', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,130 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41837
2023-06-26 18:38:18,142 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:60948; closing.
2023-06-26 18:38:18,142 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:45153', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,142 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45153
2023-06-26 18:38:18,144 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32810; closing.
2023-06-26 18:38:18,145 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46801', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,145 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46801
2023-06-26 18:38:18,153 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:32806; closing.
2023-06-26 18:38:18,153 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41549', status: closing, memory: 0, processing: 0>
2023-06-26 18:38:18,153 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41549
2023-06-26 18:38:18,153 - distributed.scheduler - INFO - Lost all workers
2023-06-26 18:38:29,167 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36263', status: init, memory: 0, processing: 0>
2023-06-26 18:38:29,168 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36263
2023-06-26 18:38:29,168 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:47984
2023-06-26 18:38:29,184 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:32821', status: init, memory: 0, processing: 0>
2023-06-26 18:38:29,184 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:32821
2023-06-26 18:38:29,184 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:47998
2023-06-26 18:38:29,202 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36461', status: init, memory: 0, processing: 0>
2023-06-26 18:38:29,203 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36461
2023-06-26 18:38:29,203 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48012
2023-06-26 18:38:32,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40245', status: init, memory: 0, processing: 0>
2023-06-26 18:38:32,110 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40245
2023-06-26 18:38:32,110 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48052
2023-06-26 18:38:33,401 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33671', status: init, memory: 0, processing: 0>
2023-06-26 18:38:33,401 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33671
2023-06-26 18:38:33,401 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48064
2023-06-26 18:38:35,818 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37819', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37819
2023-06-26 18:38:35,819 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48094
2023-06-26 18:38:35,819 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40971', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,820 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40971
2023-06-26 18:38:35,820 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48084
2023-06-26 18:38:35,844 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40401', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,844 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40401
2023-06-26 18:38:35,844 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48098
2023-06-26 18:38:35,881 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39155', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,881 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39155
2023-06-26 18:38:35,881 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48112
2023-06-26 18:38:35,902 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35857', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,902 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35857
2023-06-26 18:38:35,902 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48138
2023-06-26 18:38:35,903 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42581', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,903 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42581
2023-06-26 18:38:35,903 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48124
2023-06-26 18:38:35,915 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:45881', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,915 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:45881
2023-06-26 18:38:35,916 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48158
2023-06-26 18:38:35,920 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35889', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,921 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35889
2023-06-26 18:38:35,921 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48146
2023-06-26 18:38:35,926 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40437', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,937 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40437
2023-06-26 18:38:35,937 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48180
2023-06-26 18:38:35,937 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33455', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,938 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33455
2023-06-26 18:38:35,938 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48174
2023-06-26 18:38:35,938 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:34899', status: init, memory: 0, processing: 0>
2023-06-26 18:38:35,938 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:34899
2023-06-26 18:38:35,938 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:48192
2023-06-26 18:38:36,077 - distributed.scheduler - INFO - Restarting finished.
2023-06-26 18:38:46,087 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 18:39:06,130 - distributed.scheduler - INFO - Remove client Client-5f88eb60-1450-11ee-8cbf-5cff35c1a711
2023-06-26 18:39:06,131 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:43270; closing.
2023-06-26 18:39:06,131 - distributed.scheduler - INFO - Remove client Client-5f88eb60-1450-11ee-8cbf-5cff35c1a711
2023-06-26 18:39:06,132 - distributed.scheduler - INFO - Close client connection: Client-5f88eb60-1450-11ee-8cbf-5cff35c1a711
2023-06-26 18:42:10,103 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 18:42:10,103 - distributed.core - INFO - Connection to tcp://10.120.104.11:48064 has been closed.
2023-06-26 18:42:10,104 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33671', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,104 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33671
2023-06-26 18:42:10,105 - distributed.core - INFO - Connection to tcp://10.120.104.11:48146 has been closed.
2023-06-26 18:42:10,105 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35889', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,105 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35889
2023-06-26 18:42:10,107 - distributed.core - INFO - Connection to tcp://10.120.104.11:48098 has been closed.
2023-06-26 18:42:10,107 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40401', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40401
2023-06-26 18:42:10,107 - distributed.core - INFO - Connection to tcp://10.120.104.11:48174 has been closed.
2023-06-26 18:42:10,107 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33455', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,107 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33455
2023-06-26 18:42:10,108 - distributed.core - INFO - Connection to tcp://10.120.104.11:48158 has been closed.
2023-06-26 18:42:10,108 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:45881', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,108 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:45881
2023-06-26 18:42:10,108 - distributed.core - INFO - Connection to tcp://10.120.104.11:48192 has been closed.
2023-06-26 18:42:10,108 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:34899', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,108 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34899
2023-06-26 18:42:10,108 - distributed.core - INFO - Connection to tcp://10.120.104.11:48124 has been closed.
2023-06-26 18:42:10,108 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42581', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,108 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42581
2023-06-26 18:42:10,108 - distributed.core - INFO - Connection to tcp://10.120.104.11:47998 has been closed.
2023-06-26 18:42:10,109 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:32821', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,109 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32821
2023-06-26 18:42:10,109 - distributed.core - INFO - Connection to tcp://10.120.104.11:47984 has been closed.
2023-06-26 18:42:10,109 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36263', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,109 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36263
2023-06-26 18:42:10,109 - distributed.core - INFO - Connection to tcp://10.120.104.11:48094 has been closed.
2023-06-26 18:42:10,109 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37819', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,109 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37819
2023-06-26 18:42:10,109 - distributed.core - INFO - Connection to tcp://10.120.104.11:48180 has been closed.
2023-06-26 18:42:10,110 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40437', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,110 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40437
2023-06-26 18:42:10,110 - distributed.core - INFO - Connection to tcp://10.120.104.11:48112 has been closed.
2023-06-26 18:42:10,110 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39155', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,110 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39155
2023-06-26 18:42:10,110 - distributed.core - INFO - Connection to tcp://10.120.104.11:48138 has been closed.
2023-06-26 18:42:10,110 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35857', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,110 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35857
2023-06-26 18:42:10,110 - distributed.core - INFO - Connection to tcp://10.120.104.11:48012 has been closed.
2023-06-26 18:42:10,110 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36461', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,110 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36461
2023-06-26 18:42:10,112 - distributed.core - INFO - Connection to tcp://10.120.104.11:48084 has been closed.
2023-06-26 18:42:10,112 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40971', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,112 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40971
2023-06-26 18:42:10,112 - distributed.core - INFO - Connection to tcp://10.120.104.11:48052 has been closed.
2023-06-26 18:42:10,112 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40245', status: running, memory: 0, processing: 0>
2023-06-26 18:42:10,112 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40245
2023-06-26 18:42:10,112 - distributed.scheduler - INFO - Lost all workers
2023-06-26 18:42:10,113 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 18:42:10,113 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:47998>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,113 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48174>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,113 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48192>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,113 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48138>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,113 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:47984>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,113 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48012>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,114 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48094>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,114 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48112>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,114 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48052>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48052>: Stream is closed
2023-06-26 18:42:10,114 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48098>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,114 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48180>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,114 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48084>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48084>: Stream is closed
2023-06-26 18:42:10,114 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48124>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,114 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:48158>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:42:10,115 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 18:42:10,118 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 18:42:10,119 - distributed.scheduler - INFO - End scheduler
