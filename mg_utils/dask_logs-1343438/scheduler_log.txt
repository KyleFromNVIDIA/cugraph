RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/work/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-22 19:36:42,816 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 19:36:43,341 - distributed.scheduler - INFO - State start
2023-06-22 19:36:43,342 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-hh9k7mho', purging
2023-06-22 19:36:43,343 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4j6eiev7', purging
2023-06-22 19:36:43,343 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qf35ejpj', purging
2023-06-22 19:36:43,343 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-d2ed1fw2', purging
2023-06-22 19:36:43,343 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-l8r08kk7', purging
2023-06-22 19:36:43,344 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vxw7l2db', purging
2023-06-22 19:36:43,654 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-22 19:36:43,655 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.169:8786
2023-06-22 19:36:43,656 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.169:8787/status
2023-06-22 19:36:54,193 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:34985', status: init, memory: 0, processing: 0>
2023-06-22 19:36:54,445 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:34985
2023-06-22 19:36:54,445 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:56958
2023-06-22 19:36:54,470 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:37853', status: init, memory: 0, processing: 0>
2023-06-22 19:36:54,471 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:37853
2023-06-22 19:36:54,471 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:56966
2023-06-22 19:36:54,471 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:36793', status: init, memory: 0, processing: 0>
2023-06-22 19:36:54,471 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:36793
2023-06-22 19:36:54,472 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:56982
2023-06-22 19:36:54,472 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:42219', status: init, memory: 0, processing: 0>
2023-06-22 19:36:54,472 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:42219
2023-06-22 19:36:54,473 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:57008
2023-06-22 19:36:54,474 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:37555', status: init, memory: 0, processing: 0>
2023-06-22 19:36:54,474 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:37555
2023-06-22 19:36:54,475 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:57020
2023-06-22 19:36:54,475 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:37647', status: init, memory: 0, processing: 0>
2023-06-22 19:36:54,476 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:37647
2023-06-22 19:36:54,476 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:56992
2023-06-22 19:36:54,476 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:45779', status: init, memory: 0, processing: 0>
2023-06-22 19:36:54,476 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:45779
2023-06-22 19:36:54,476 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:56986
2023-06-22 19:36:54,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.33.227.169:35229', status: init, memory: 0, processing: 0>
2023-06-22 19:36:54,477 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.227.169:35229
2023-06-22 19:36:54,477 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:57018
2023-06-22 19:37:14,864 - distributed.scheduler - INFO - Receive client connection: Client-30ba683f-1134-11ee-81b4-d8c49778ced7
2023-06-22 19:37:14,864 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:43638
2023-06-22 19:37:14,951 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-22 19:38:04,540 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 19:38:06,909 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-22 19:38:10,756 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 19:38:10,834 - distributed.scheduler - INFO - Receive client connection: Client-worker-52179640-1134-11ee-8074-d8c49778ced7
2023-06-22 19:38:10,835 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37608
2023-06-22 19:38:10,856 - distributed.scheduler - INFO - Receive client connection: Client-worker-521ac2f7-1134-11ee-8080-d8c49778ced7
2023-06-22 19:38:10,857 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37612
2023-06-22 19:38:10,861 - distributed.scheduler - INFO - Receive client connection: Client-worker-521ad160-1134-11ee-8083-d8c49778ced7
2023-06-22 19:38:10,861 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37628
2023-06-22 19:38:10,861 - distributed.scheduler - INFO - Receive client connection: Client-worker-521ad566-1134-11ee-8077-d8c49778ced7
2023-06-22 19:38:10,862 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37634
2023-06-22 19:38:10,862 - distributed.scheduler - INFO - Receive client connection: Client-worker-521ad393-1134-11ee-8089-d8c49778ced7
2023-06-22 19:38:10,862 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37636
2023-06-22 19:38:10,863 - distributed.scheduler - INFO - Receive client connection: Client-worker-521ad6eb-1134-11ee-807d-d8c49778ced7
2023-06-22 19:38:10,864 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37652
2023-06-22 19:38:10,865 - distributed.scheduler - INFO - Receive client connection: Client-worker-521ada00-1134-11ee-807a-d8c49778ced7
2023-06-22 19:38:10,865 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37658
2023-06-22 19:38:10,866 - distributed.scheduler - INFO - Receive client connection: Client-worker-521ad28d-1134-11ee-8087-d8c49778ced7
2023-06-22 19:38:10,866 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:37662
2023-06-22 19:40:38,957 - distributed.scheduler - INFO - Remove client Client-30ba683f-1134-11ee-81b4-d8c49778ced7
2023-06-22 19:40:38,961 - distributed.core - INFO - Received 'close-stream' from tcp://10.33.227.169:43638; closing.
2023-06-22 19:40:38,961 - distributed.scheduler - INFO - Remove client Client-30ba683f-1134-11ee-81b4-d8c49778ced7
2023-06-22 19:40:38,963 - distributed.scheduler - INFO - Close client connection: Client-30ba683f-1134-11ee-81b4-d8c49778ced7
2023-06-22 19:40:46,558 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-22 19:40:46,559 - distributed.scheduler - INFO - Scheduler closing...
2023-06-22 19:40:46,559 - distributed.core - INFO - Connection to tcp://10.33.227.169:37634 has been closed.
2023-06-22 19:40:46,560 - distributed.core - INFO - Connection to tcp://10.33.227.169:37658 has been closed.
2023-06-22 19:40:46,560 - distributed.core - INFO - Connection to tcp://10.33.227.169:57008 has been closed.
2023-06-22 19:40:46,560 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:42219', status: running, memory: 0, processing: 0>
2023-06-22 19:40:46,561 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:42219
2023-06-22 19:40:46,563 - distributed.core - INFO - Connection to tcp://10.33.227.169:56986 has been closed.
2023-06-22 19:40:46,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:45779', status: running, memory: 0, processing: 0>
2023-06-22 19:40:46,563 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:45779
2023-06-22 19:40:46,563 - distributed.core - INFO - Connection to tcp://10.33.227.169:56982 has been closed.
2023-06-22 19:40:46,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:36793', status: running, memory: 0, processing: 0>
2023-06-22 19:40:46,563 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:36793
2023-06-22 19:40:46,564 - distributed.core - INFO - Connection to tcp://10.33.227.169:56966 has been closed.
2023-06-22 19:40:46,564 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:37853', status: running, memory: 0, processing: 0>
2023-06-22 19:40:46,564 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:37853
2023-06-22 19:40:46,564 - distributed.core - INFO - Connection to tcp://10.33.227.169:37662 has been closed.
2023-06-22 19:40:46,565 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-22 19:40:46,566 - distributed.core - INFO - Connection to tcp://10.33.227.169:37652 has been closed.
2023-06-22 19:40:46,566 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:56982>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 19:40:46,567 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:56966>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 19:40:46,567 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:56986>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 19:40:46,568 - distributed.core - INFO - Connection to tcp://10.33.227.169:56958 has been closed.
2023-06-22 19:40:46,568 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:34985', status: running, memory: 0, processing: 0>
2023-06-22 19:40:46,569 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:34985
2023-06-22 19:40:46,569 - distributed.core - INFO - Connection to tcp://10.33.227.169:57018 has been closed.
2023-06-22 19:40:46,569 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:35229', status: running, memory: 0, processing: 0>
2023-06-22 19:40:46,569 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:35229
2023-06-22 19:40:46,569 - distributed.core - INFO - Connection to tcp://10.33.227.169:57020 has been closed.
2023-06-22 19:40:46,569 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:37555', status: running, memory: 0, processing: 0>
2023-06-22 19:40:46,569 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:37555
2023-06-22 19:40:46,570 - distributed.core - INFO - Connection to tcp://10.33.227.169:56992 has been closed.
2023-06-22 19:40:46,570 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.33.227.169:37647', status: running, memory: 1, processing: 0>
2023-06-22 19:40:46,570 - distributed.core - INFO - Removing comms to tcp://10.33.227.169:37647
2023-06-22 19:40:46,570 - distributed.scheduler - INFO - Lost all workers
2023-06-22 19:40:46,571 - distributed.core - INFO - Connection to tcp://10.33.227.169:37608 has been closed.
2023-06-22 19:40:46,572 - distributed.core - INFO - Connection to tcp://10.33.227.169:37612 has been closed.
2023-06-22 19:40:46,572 - distributed.core - INFO - Connection to tcp://10.33.227.169:37628 has been closed.
2023-06-22 19:40:46,572 - distributed.core - INFO - Connection to tcp://10.33.227.169:37636 has been closed.
2023-06-22 19:40:46,573 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:37608>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 19:40:46,573 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:37612>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 19:40:46,573 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:37636>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 19:40:46,573 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler->Client local=tcp://10.33.227.169:8786 remote=tcp://10.33.227.169:37628>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 19:40:46,573 - distributed.scheduler - ERROR - broadcast to tcp://10.33.227.169:34985 failed: CommClosedError: Address removed.
2023-06-22 19:40:46,574 - distributed.scheduler - ERROR - Couldn't gather keys {'Series-9cbcb8ff-b9db-42b0-9c4e-ca2a600f89de': ['tcp://10.33.227.169:37647']} state: [None] workers: ['tcp://10.33.227.169:37647']
NoneType: None
2023-06-22 19:40:46,574 - distributed.scheduler - ERROR - Couldn't gather keys {'Series-9cbcb8ff-b9db-42b0-9c4e-ca2a600f89de': ['tcp://10.33.227.169:37647']} state: [None] workers: ['tcp://10.33.227.169:37647']
NoneType: None
2023-06-22 19:40:46,574 - distributed.scheduler - ERROR - Couldn't gather keys {'Series-9cbcb8ff-b9db-42b0-9c4e-ca2a600f89de': ['tcp://10.33.227.169:37647']} state: [None] workers: ['tcp://10.33.227.169:37647']
NoneType: None
2023-06-22 19:40:46,574 - distributed.scheduler - ERROR - Couldn't gather keys {'Series-9cbcb8ff-b9db-42b0-9c4e-ca2a600f89de': ['tcp://10.33.227.169:37647']} state: [None] workers: ['tcp://10.33.227.169:37647']
NoneType: None
2023-06-22 19:40:46,574 - tornado.application - ERROR - Uncaught exception GET /info/logs/tcp%3A%2F%2F10.33.227.169%3A34985.html (10.20.237.237)
HTTPServerRequest(protocol='http', host='10.33.227.169:8787', method='GET', uri='/info/logs/tcp%3A%2F%2F10.33.227.169%3A34985.html', version='HTTP/1.1', remote_ip='10.20.237.237')
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 432, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 441, in wait_for
    await _cancel_and_wait(fut, loop=loop)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 518, in _cancel_and_wait
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/web.py", line 1786, in _execute
    result = await result
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/http/scheduler/info.py", line 127, in get
    logs = await self.server.get_worker_logs(workers=[worker])
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 7878, in get_worker_logs
    results = await self.broadcast(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 6124, in broadcast
    results = await All(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 249, in All
    result = await tasks.next()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/scheduler.py", line 6099, in send_message
    comm = await self.rpc.connect(addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1550, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-06-22 19:40:46,579 - distributed.scheduler - ERROR - Shut down workers that don't have promised key: ['tcp://10.33.227.169:37647'], Series-9cbcb8ff-b9db-42b0-9c4e-ca2a600f89de
NoneType: None
2023-06-22 19:40:46,579 - distributed.scheduler - ERROR - Shut down workers that don't have promised key: ['tcp://10.33.227.169:37647'], Series-9cbcb8ff-b9db-42b0-9c4e-ca2a600f89de
NoneType: None
2023-06-22 19:40:46,579 - distributed.scheduler - ERROR - Shut down workers that don't have promised key: ['tcp://10.33.227.169:37647'], Series-9cbcb8ff-b9db-42b0-9c4e-ca2a600f89de
NoneType: None
2023-06-22 19:40:46,580 - distributed.scheduler - ERROR - Shut down workers that don't have promised key: ['tcp://10.33.227.169:37647'], Series-9cbcb8ff-b9db-42b0-9c4e-ca2a600f89de
NoneType: None
2023-06-22 19:40:47,447 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.169:8786'
2023-06-22 19:40:47,448 - distributed.scheduler - INFO - End scheduler
