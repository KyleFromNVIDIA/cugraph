RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 20:46:34,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43415'
2023-06-26 20:46:34,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46185'
2023-06-26 20:46:34,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44755'
2023-06-26 20:46:34,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39841'
2023-06-26 20:46:34,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39859'
2023-06-26 20:46:34,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35623'
2023-06-26 20:46:34,357 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40405'
2023-06-26 20:46:34,358 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33905'
2023-06-26 20:46:34,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34093'
2023-06-26 20:46:34,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45591'
2023-06-26 20:46:34,364 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:39971'
2023-06-26 20:46:34,367 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45917'
2023-06-26 20:46:34,369 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38805'
2023-06-26 20:46:34,371 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33761'
2023-06-26 20:46:34,374 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37829'
2023-06-26 20:46:34,376 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40825'
2023-06-26 20:46:36,015 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,015 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,064 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,064 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,067 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,067 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,068 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,082 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,082 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,093 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,093 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,124 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,145 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 20:46:36,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 20:46:36,194 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,244 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,244 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,246 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,249 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,250 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,261 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,266 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,272 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,280 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,304 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:36,357 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 20:46:42,679 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34589
2023-06-26 20:46:42,679 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34589
2023-06-26 20:46:42,679 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46791
2023-06-26 20:46:42,679 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,679 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,679 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,679 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,679 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x_ve27ev
2023-06-26 20:46:42,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfc5bffd-979a-4bbf-8bbe-ff4694a32388
2023-06-26 20:46:42,704 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46549
2023-06-26 20:46:42,704 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46549
2023-06-26 20:46:42,704 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40753
2023-06-26 20:46:42,704 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,704 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,705 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,705 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,705 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-djp6ioc6
2023-06-26 20:46:42,706 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9db420c8-0145-4aa6-a8c8-3b486c2cea55
2023-06-26 20:46:42,718 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38647
2023-06-26 20:46:42,718 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38647
2023-06-26 20:46:42,718 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34673
2023-06-26 20:46:42,718 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,718 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,718 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,718 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,718 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l_7sh3tb
2023-06-26 20:46:42,719 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5001569d-e064-4027-83ca-5bfd03f24057
2023-06-26 20:46:42,719 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90596b9c-0225-4fe1-93b0-d7efbf5a879c
2023-06-26 20:46:42,786 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33029
2023-06-26 20:46:42,787 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33029
2023-06-26 20:46:42,787 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42907
2023-06-26 20:46:42,787 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,787 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,787 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,787 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8yyqoabo
2023-06-26 20:46:42,788 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29ff54cf-cc59-4fab-8852-645cfbcbecd8
2023-06-26 20:46:42,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3bcef468-f70e-45f2-be5f-992ffb8b68c0
2023-06-26 20:46:42,799 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34927
2023-06-26 20:46:42,799 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34927
2023-06-26 20:46:42,799 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38797
2023-06-26 20:46:42,799 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,799 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,799 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,800 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,800 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-itl2vk0f
2023-06-26 20:46:42,801 - distributed.worker - INFO - Starting Worker plugin PreImport-c1e6d2d3-7230-4dee-a2f0-c3e1aafa7570
2023-06-26 20:46:42,801 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8a3449c1-fda6-4c6f-b934-f18a45ca9403
2023-06-26 20:46:42,825 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34383
2023-06-26 20:46:42,826 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34383
2023-06-26 20:46:42,826 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36459
2023-06-26 20:46:42,826 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,826 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,826 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,826 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-toq8xqnh
2023-06-26 20:46:42,826 - distributed.worker - INFO - Starting Worker plugin PreImport-6a2d68f7-bc1d-434c-96f5-abb83e0eff8c
2023-06-26 20:46:42,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-15133460-ba4c-4d4e-8695-6fa3aa3c6186
2023-06-26 20:46:42,898 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40785
2023-06-26 20:46:42,898 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40785
2023-06-26 20:46:42,898 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40319
2023-06-26 20:46:42,898 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,898 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,899 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,899 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,899 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nvu8qa_n
2023-06-26 20:46:42,899 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4617cf4-1b54-47c9-87c3-ed12c2d9b504
2023-06-26 20:46:42,905 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34909
2023-06-26 20:46:42,905 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34909
2023-06-26 20:46:42,905 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36463
2023-06-26 20:46:42,906 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,906 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,906 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,906 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,906 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f5gc8irf
2023-06-26 20:46:42,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62de9d09-11c2-4d30-b044-f7c172d95b9d
2023-06-26 20:46:42,912 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38349
2023-06-26 20:46:42,912 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38349
2023-06-26 20:46:42,913 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37979
2023-06-26 20:46:42,913 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,913 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,913 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,913 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,913 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9kgns5sa
2023-06-26 20:46:42,913 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9ba14da-ba75-4ad6-8343-fc70c7803e3a
2023-06-26 20:46:42,922 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40733
2023-06-26 20:46:42,922 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40733
2023-06-26 20:46:42,922 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38857
2023-06-26 20:46:42,922 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,922 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,923 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,923 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,923 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i2yokw8z
2023-06-26 20:46:42,924 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b036b6af-23c6-41e3-965a-30a2c43768fe
2023-06-26 20:46:42,925 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35141
2023-06-26 20:46:42,926 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35141
2023-06-26 20:46:42,926 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44933
2023-06-26 20:46:42,926 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,926 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,926 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,926 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x9tbvh0a
2023-06-26 20:46:42,926 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cd6b192-6c7d-4a46-a49b-d6237d470143
2023-06-26 20:46:42,926 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40144ea3-721f-4a0f-8e1c-5c2a83b1aecd
2023-06-26 20:46:42,946 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35367
2023-06-26 20:46:42,947 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35367
2023-06-26 20:46:42,947 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40643
2023-06-26 20:46:42,947 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,947 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,947 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,947 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,947 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-adaz7gic
2023-06-26 20:46:42,947 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35527
2023-06-26 20:46:42,947 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35527
2023-06-26 20:46:42,947 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41671
2023-06-26 20:46:42,947 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,947 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,947 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,947 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,947 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l7s3d1hv
2023-06-26 20:46:42,948 - distributed.worker - INFO - Starting Worker plugin RMMSetup-494c334a-db6e-44f2-93a7-164e076aa646
2023-06-26 20:46:42,948 - distributed.worker - INFO - Starting Worker plugin PreImport-6ab7b8ab-4678-4613-8eaf-36770d9db967
2023-06-26 20:46:42,948 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84cbeebb-f7f7-4aa0-b612-4e58a87133e8
2023-06-26 20:46:42,953 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39355
2023-06-26 20:46:42,953 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39355
2023-06-26 20:46:42,953 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40917
2023-06-26 20:46:42,953 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,953 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,954 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,954 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ik_x5b86
2023-06-26 20:46:42,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-030c15e6-e5ab-4f11-9696-57bce0dea9a1
2023-06-26 20:46:42,958 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36477
2023-06-26 20:46:42,958 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36477
2023-06-26 20:46:42,958 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39505
2023-06-26 20:46:42,958 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,958 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,958 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,958 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,958 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y6m674uv
2023-06-26 20:46:42,958 - distributed.worker - INFO - Starting Worker plugin RMMSetup-373f7478-cc40-4266-b0a3-0c248c54c26c
2023-06-26 20:46:42,963 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32871
2023-06-26 20:46:42,964 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32871
2023-06-26 20:46:42,964 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35003
2023-06-26 20:46:42,964 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 20:46:42,964 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:42,964 - distributed.worker - INFO -               Threads:                          1
2023-06-26 20:46:42,964 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 20:46:42,964 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g43jp9kt
2023-06-26 20:46:42,964 - distributed.worker - INFO - Starting Worker plugin RMMSetup-469aa1f1-c908-431e-9fde-1f57013ffe69
2023-06-26 20:46:46,491 - distributed.worker - INFO - Starting Worker plugin PreImport-5d38311f-2815-43e8-adcf-9079c7101a10
2023-06-26 20:46:46,492 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,508 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2f9cf03-5a58-47ea-b793-7a0028cc1ce6
2023-06-26 20:46:46,510 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,511 - distributed.worker - INFO - Starting Worker plugin PreImport-91a131f6-41db-448c-917d-06a46a2f12c4
2023-06-26 20:46:46,513 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,528 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5134e3e1-91ec-4387-aac2-2cf8acdad11f
2023-06-26 20:46:46,529 - distributed.worker - INFO - Starting Worker plugin PreImport-61eb9a95-49a1-481f-a819-40035163daa3
2023-06-26 20:46:46,529 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,531 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,531 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,533 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,539 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,539 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,540 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,540 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,542 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,542 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,547 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,547 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,548 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,572 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2c493db-204e-4ff6-81e4-9b2ffbd2977b
2023-06-26 20:46:46,573 - distributed.worker - INFO - Starting Worker plugin PreImport-79df722e-9206-45e3-9f97-598f3a6c746e
2023-06-26 20:46:46,574 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,577 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-223f3471-b02e-4763-a055-6b801f4a1f85
2023-06-26 20:46:46,578 - distributed.worker - INFO - Starting Worker plugin PreImport-b190ab44-c9e6-4170-a18e-e112f3b67296
2023-06-26 20:46:46,580 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,599 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,599 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,601 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,608 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,608 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,610 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,638 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6bcfd37f-e9f3-4570-921c-012f1b613691
2023-06-26 20:46:46,639 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,654 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,655 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,656 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,676 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7405328e-4685-4bec-815b-b0730296c1d0
2023-06-26 20:46:46,676 - distributed.worker - INFO - Starting Worker plugin PreImport-8382146a-2206-428a-91b4-c78dae342c2f
2023-06-26 20:46:46,677 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,695 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,695 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,696 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,704 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bafe252b-d8e7-4b23-8441-356d0b7acf67
2023-06-26 20:46:46,705 - distributed.worker - INFO - Starting Worker plugin PreImport-463d055a-862f-4229-bc4c-5495f447a561
2023-06-26 20:46:46,707 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,709 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20f35cee-ff10-472c-b251-26d7d5e1d585
2023-06-26 20:46:46,709 - distributed.worker - INFO - Starting Worker plugin PreImport-aae7a219-e882-4143-ab36-d276660a08b9
2023-06-26 20:46:46,710 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,723 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,723 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,724 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,730 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,731 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,733 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,738 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04afb713-3afa-4fc8-b658-f41f49264e9d
2023-06-26 20:46:46,739 - distributed.worker - INFO - Starting Worker plugin PreImport-af1768fa-ff4b-474a-b340-af379dfcb7d4
2023-06-26 20:46:46,745 - distributed.worker - INFO - Starting Worker plugin PreImport-dde8f50a-eeb5-4749-a4fd-51d4cc66beac
2023-06-26 20:46:46,745 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,746 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-118c8f9d-f29f-4deb-83d6-c0e3cf38fbb5
2023-06-26 20:46:46,751 - distributed.worker - INFO - Starting Worker plugin PreImport-915001c2-1519-472e-ac17-0e6304e4a61b
2023-06-26 20:46:46,756 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8263717-3f43-47e2-93ef-25db7a31a7d6
2023-06-26 20:46:46,759 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,759 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,759 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,761 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,763 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-803ac4b1-f84e-40da-85dd-c14ab94e4835
2023-06-26 20:46:46,764 - distributed.worker - INFO - Starting Worker plugin PreImport-a530bb8e-141c-46d9-a423-0d9c8393d590
2023-06-26 20:46:46,766 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,770 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,770 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,772 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,775 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41bdbef0-9c7b-4c88-8df1-e58cacf8ad7e
2023-06-26 20:46:46,776 - distributed.worker - INFO - Starting Worker plugin PreImport-3f968daf-a43b-4d72-a655-ce6a397ad4a2
2023-06-26 20:46:46,778 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,784 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,784 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,786 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,788 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,788 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,791 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,797 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,797 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,800 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:46,800 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 20:46:46,800 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 20:46:46,803 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 20:46:49,256 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,256 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,256 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,257 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,258 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,259 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,259 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,260 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,260 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,260 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,260 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,262 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,263 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,264 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,264 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,267 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 20:46:49,276 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,276 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,276 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,276 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,277 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 20:46:49,963 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,963 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,963 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,963 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,963 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,963 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,963 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,963 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,964 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,964 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,964 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,964 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,964 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,964 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,964 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:49,964 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 20:46:53,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:05,817 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:05,880 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:05,900 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:05,902 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:05,996 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,067 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,071 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,115 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,131 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,171 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,185 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,223 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,239 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,415 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,548 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:06,618 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 20:47:13,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:13,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:47:51,843 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,844 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,844 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,846 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,849 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:51,850 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 20:47:54,941 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,947 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,948 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,948 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,949 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:47:54,950 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 20:48:02,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:02,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 20:48:15,250 - distributed.worker - ERROR - Exception during execution of task ('split-simple-shuffle-091972cead9eb71d5c8474628f29d9ea', 0, 8).
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 2390, in _prepare_args_for_execution
    data[k] = self.data[k]
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda/device_host_file.py", line 269, in __getitem__
    raise KeyError(key)
KeyError: "('group-simple-shuffle-091972cead9eb71d5c8474628f29d9ea', 8)"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 2254, in execute
    args2, kwargs2 = self._prepare_args_for_execution(ts, args, kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 2394, in _prepare_args_for_execution
    data[k] = Actor(type(self.state.actors[k]), self.address, k, self)
KeyError: "('group-simple-shuffle-091972cead9eb71d5c8474628f29d9ea', 8)"
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36477. Reason: worker-close
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32871. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40785. Reason: worker-close
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34383. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38647. Reason: worker-close
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33029. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46549. Reason: worker-close
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35141. Reason: worker-close
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34589. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35527. Reason: worker-handle-scheduler-connection-broken
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38349. Reason: worker-close
2023-06-26 20:49:26,862 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40733. Reason: worker-close
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34909. Reason: worker-close
2023-06-26 20:49:26,861 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34927. Reason: worker-close
2023-06-26 20:49:26,862 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39355. Reason: worker-close
2023-06-26 20:49:26,862 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35367. Reason: worker-close
2023-06-26 20:49:26,862 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43415'. Reason: nanny-close
2023-06-26 20:49:26,864 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,863 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60186 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,863 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60162 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46185'. Reason: nanny-close
2023-06-26 20:49:26,863 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60144 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,863 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60232 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,864 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60236 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,864 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60208 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,864 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60138 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,864 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60262 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,865 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,864 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60176 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44755'. Reason: nanny-close
2023-06-26 20:49:26,866 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,864 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60216 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39841'. Reason: nanny-close
2023-06-26 20:49:26,866 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,867 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39859'. Reason: nanny-close
2023-06-26 20:49:26,867 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,867 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35623'. Reason: nanny-close
2023-06-26 20:49:26,865 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:60248 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 20:49:26,867 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40405'. Reason: nanny-close
2023-06-26 20:49:26,868 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33905'. Reason: nanny-close
2023-06-26 20:49:26,868 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34093'. Reason: nanny-close
2023-06-26 20:49:26,869 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45591'. Reason: nanny-close
2023-06-26 20:49:26,869 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:39971'. Reason: nanny-close
2023-06-26 20:49:26,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45917'. Reason: nanny-close
2023-06-26 20:49:26,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38805'. Reason: nanny-close
2023-06-26 20:49:26,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,871 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33761'. Reason: nanny-close
2023-06-26 20:49:26,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,871 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37829'. Reason: nanny-close
2023-06-26 20:49:26,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,872 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40825'. Reason: nanny-close
2023-06-26 20:49:26,872 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 20:49:26,876 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39841 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40848 remote=tcp://10.120.104.11:39841>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39841 after 100 s
2023-06-26 20:49:26,882 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43415 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:51072 remote=tcp://10.120.104.11:43415>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43415 after 100 s
2023-06-26 20:49:26,884 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40405 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:60574 remote=tcp://10.120.104.11:40405>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40405 after 100 s
2023-06-26 20:49:26,885 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44755 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:48038 remote=tcp://10.120.104.11:44755>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44755 after 100 s
2023-06-26 20:49:26,885 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46185 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59396 remote=tcp://10.120.104.11:46185>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46185 after 100 s
2023-06-26 20:49:26,886 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39859 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33698 remote=tcp://10.120.104.11:39859>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39859 after 100 s
2023-06-26 20:49:26,886 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33905 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46834 remote=tcp://10.120.104.11:33905>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33905 after 100 s
2023-06-26 20:49:26,887 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35623 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:43970 remote=tcp://10.120.104.11:35623>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35623 after 100 s
2023-06-26 20:49:26,888 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34093 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:56776 remote=tcp://10.120.104.11:34093>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34093 after 100 s
2023-06-26 20:49:26,888 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:38805 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40166 remote=tcp://10.120.104.11:38805>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:38805 after 100 s
2023-06-26 20:49:26,889 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45591 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41988 remote=tcp://10.120.104.11:45591>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45591 after 100 s
2023-06-26 20:49:26,889 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:39971 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46968 remote=tcp://10.120.104.11:39971>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:39971 after 100 s
2023-06-26 20:49:26,890 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45917 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45604 remote=tcp://10.120.104.11:45917>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45917 after 100 s
2023-06-26 20:49:26,891 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40825 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40462 remote=tcp://10.120.104.11:40825>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40825 after 100 s
2023-06-26 20:49:26,900 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33761 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:58598 remote=tcp://10.120.104.11:33761>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33761 after 100 s
2023-06-26 20:49:26,902 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37829 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35946 remote=tcp://10.120.104.11:37829>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37829 after 100 s
2023-06-26 20:49:30,074 - distributed.nanny - WARNING - Worker process still alive after 3.199991455078125 seconds, killing
2023-06-26 20:49:30,074 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:49:30,075 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:49:30,075 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:49:30,076 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:49:30,076 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:49:30,077 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:49:30,077 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:49:30,077 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 20:49:30,078 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:49:30,078 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:49:30,079 - distributed.nanny - WARNING - Worker process still alive after 3.19999984741211 seconds, killing
2023-06-26 20:49:30,079 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:49:30,079 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:49:30,079 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 20:49:30,081 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 20:49:30,864 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,866 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,866 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,867 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,867 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,868 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,869 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,869 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,869 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,869 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,870 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,871 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,871 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,871 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,872 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,872 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508545 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508543 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508540 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508537 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508534 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508531 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508528 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508525 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508521 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508518 parent=508447 started daemon>
2023-06-26 20:49:30,873 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508515 parent=508447 started daemon>
2023-06-26 20:49:30,874 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508512 parent=508447 started daemon>
2023-06-26 20:49:30,874 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508509 parent=508447 started daemon>
2023-06-26 20:49:30,874 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508506 parent=508447 started daemon>
2023-06-26 20:49:30,874 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508503 parent=508447 started daemon>
2023-06-26 20:49:30,874 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=508500 parent=508447 started daemon>
2023-06-26 20:49:37,942 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 508525 exit status was already read will report exitcode 255
2023-06-26 20:49:38,843 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 508534 exit status was already read will report exitcode 255
2023-06-26 20:49:39,678 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 508500 exit status was already read will report exitcode 255
