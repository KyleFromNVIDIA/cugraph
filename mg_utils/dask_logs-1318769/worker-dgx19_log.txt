RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=12G
             --local-directory=/tmp/
             --scheduler-file=/root/work/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-22 18:57:11,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:33167'
2023-06-22 18:57:11,071 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:38431'
2023-06-22 18:57:11,072 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:46109'
2023-06-22 18:57:11,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:45625'
2023-06-22 18:57:11,077 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:45353'
2023-06-22 18:57:11,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:40891'
2023-06-22 18:57:11,081 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:34619'
2023-06-22 18:57:11,084 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:38689'
2023-06-22 18:57:12,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 18:57:12,448 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 18:57:12,579 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 18:57:12,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 18:57:12,580 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 18:57:12,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 18:57:12,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 18:57:12,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 18:57:12,584 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 18:57:12,584 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 18:57:12,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 18:57:12,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 18:57:12,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 18:57:12,605 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 18:57:12,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 18:57:12,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 18:57:12,852 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 18:57:13,006 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 18:57:13,016 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 18:57:13,027 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 18:57:13,027 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 18:57:13,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 18:57:13,047 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 18:57:13,048 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 18:57:15,487 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:39873
2023-06-22 18:57:15,487 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:39873
2023-06-22 18:57:15,488 - distributed.worker - INFO -          dashboard at:        10.33.227.169:38099
2023-06-22 18:57:15,488 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,488 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,488 - distributed.worker - INFO -               Threads:                          1
2023-06-22 18:57:15,488 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 18:57:15,488 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ohitemiq
2023-06-22 18:57:15,488 - distributed.worker - INFO - Starting Worker plugin PreImport-6d2544ad-1fbd-42f7-954f-2bee19529109
2023-06-22 18:57:15,488 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a38e57d-7987-4ba1-8a75-1de23e97fa61
2023-06-22 18:57:15,491 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:34945
2023-06-22 18:57:15,491 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:34945
2023-06-22 18:57:15,491 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44551
2023-06-22 18:57:15,491 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,491 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,491 - distributed.worker - INFO -               Threads:                          1
2023-06-22 18:57:15,491 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 18:57:15,491 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cs0mlhhd
2023-06-22 18:57:15,492 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aeeb7d19-ee56-4b86-93d8-f66d918d5a7a
2023-06-22 18:57:15,492 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a64c0726-50fc-4d00-9d77-ec91d3825309
2023-06-22 18:57:15,493 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:33279
2023-06-22 18:57:15,494 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:33279
2023-06-22 18:57:15,494 - distributed.worker - INFO -          dashboard at:        10.33.227.169:43029
2023-06-22 18:57:15,494 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,494 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,494 - distributed.worker - INFO -               Threads:                          1
2023-06-22 18:57:15,494 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 18:57:15,494 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4l2a9nvu
2023-06-22 18:57:15,495 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47764a8c-05d0-4cb0-885d-3d59c5d4ce0f
2023-06-22 18:57:15,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-abae8271-a9be-4a9e-9dab-a71328a1aca7
2023-06-22 18:57:15,495 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:46389
2023-06-22 18:57:15,495 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:46389
2023-06-22 18:57:15,495 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:38865
2023-06-22 18:57:15,495 - distributed.worker - INFO -          dashboard at:        10.33.227.169:38197
2023-06-22 18:57:15,495 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:38865
2023-06-22 18:57:15,495 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,495 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,495 - distributed.worker - INFO -          dashboard at:        10.33.227.169:34489
2023-06-22 18:57:15,495 - distributed.worker - INFO -               Threads:                          1
2023-06-22 18:57:15,495 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,495 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,495 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 18:57:15,496 - distributed.worker - INFO -               Threads:                          1
2023-06-22 18:57:15,496 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7cfne_ge
2023-06-22 18:57:15,496 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 18:57:15,496 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y1d8h609
2023-06-22 18:57:15,496 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-288f3c86-3faa-414d-bf99-c209b58bac0e
2023-06-22 18:57:15,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-639ddc5a-ab82-43f8-84a4-f5e3c210532b
2023-06-22 18:57:15,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c2fb074-8d41-478c-bd22-eded4a195839
2023-06-22 18:57:15,509 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:45599
2023-06-22 18:57:15,509 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:45599
2023-06-22 18:57:15,509 - distributed.worker - INFO -          dashboard at:        10.33.227.169:37893
2023-06-22 18:57:15,509 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,509 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,509 - distributed.worker - INFO -               Threads:                          1
2023-06-22 18:57:15,509 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 18:57:15,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dr8l_5eq
2023-06-22 18:57:15,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8862a6b4-bb9b-4f7f-968c-f6e2007a9b58
2023-06-22 18:57:15,510 - distributed.worker - INFO - Starting Worker plugin RMMSetup-320b65b9-8942-42c5-9fe8-fd01907b3f51
2023-06-22 18:57:15,510 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:36263
2023-06-22 18:57:15,511 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:36263
2023-06-22 18:57:15,511 - distributed.worker - INFO -          dashboard at:        10.33.227.169:42489
2023-06-22 18:57:15,511 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,511 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,511 - distributed.worker - INFO -               Threads:                          1
2023-06-22 18:57:15,511 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 18:57:15,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pokeo_iq
2023-06-22 18:57:15,512 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cf79808-7e0e-43de-8e0c-9e078023d2b5
2023-06-22 18:57:15,512 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:45959
2023-06-22 18:57:15,513 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:45959
2023-06-22 18:57:15,513 - distributed.worker - INFO -          dashboard at:        10.33.227.169:35151
2023-06-22 18:57:15,513 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,513 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,513 - distributed.worker - INFO -               Threads:                          1
2023-06-22 18:57:15,513 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 18:57:15,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fx8aa58o
2023-06-22 18:57:15,514 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-937d2b78-1642-407e-adad-d6c593f6e325
2023-06-22 18:57:15,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-83467e85-9ac0-48ab-9754-3ffd3ae9b295
2023-06-22 18:57:15,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-82c6ade8-1f6f-4834-b303-9c7e5465c22a
2023-06-22 18:57:15,693 - distributed.worker - INFO - Starting Worker plugin PreImport-9f169377-481a-4212-b802-0dce6f6a6117
2023-06-22 18:57:15,693 - distributed.worker - INFO - Starting Worker plugin PreImport-360227d4-d5a2-45c0-b599-9ca4f5df388a
2023-06-22 18:57:15,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bad11170-b6e5-4853-9e1d-9659fa25a84f
2023-06-22 18:57:15,693 - distributed.worker - INFO - Starting Worker plugin PreImport-29bc4337-ff63-4381-84a9-ced094613b14
2023-06-22 18:57:15,694 - distributed.worker - INFO - Starting Worker plugin PreImport-ce952cc6-cf48-4a55-9920-80510c8ece0b
2023-06-22 18:57:15,694 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-929137c8-0d03-4696-831f-c58cc35fa11b
2023-06-22 18:57:15,694 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,694 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,694 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,694 - distributed.worker - INFO - Starting Worker plugin PreImport-3f571cb7-058e-4fa4-a2c0-8aca8bf6d32c
2023-06-22 18:57:15,695 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,696 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,696 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,739 - distributed.worker - INFO - Starting Worker plugin PreImport-e4002db9-59b0-4ee2-9036-11f734c79102
2023-06-22 18:57:15,740 - distributed.worker - INFO - Starting Worker plugin PreImport-39122c7a-561b-44ed-b7d3-c8d05a893603
2023-06-22 18:57:15,740 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,741 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,946 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,946 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,947 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 18:57:15,948 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,948 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,948 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,948 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,949 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,949 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,950 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 18:57:15,950 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,950 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,951 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 18:57:15,951 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,951 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,952 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 18:57:15,953 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 18:57:15,954 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 18:57:15,958 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,958 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,959 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 18:57:15,959 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 18:57:15,960 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 18:57:15,961 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 18:57:24,117 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:57:24,117 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:57:24,117 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:57:24,117 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:57:24,118 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:57:24,118 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:57:24,121 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:57:24,121 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:57:24,191 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 18:57:24,191 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 18:57:24,191 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 18:57:24,191 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 18:57:24,191 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 18:57:24,191 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 18:57:24,191 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 18:57:24,191 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 18:57:35,141 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 18:57:35,150 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 18:57:35,221 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 18:57:35,233 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 18:57:35,297 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 18:57:35,408 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 18:57:35,407 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 18:57:35,439 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 18:57:41,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 18:57:41,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 18:57:41,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 18:57:41,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 18:57:41,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 18:57:41,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 18:57:41,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 18:57:41,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 18:58:16,341 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 18:58:16,342 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 18:58:16,346 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 18:58:16,346 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 18:58:16,346 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 18:58:16,346 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 18:58:16,346 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 18:58:16,346 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 18:58:20,416 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:58:20,416 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:58:20,416 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:58:20,416 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:58:20,416 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:58:20,416 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:58:20,416 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:58:20,416 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 18:59:47,474 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:36263. Reason: worker-close
2023-06-22 18:59:47,474 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:39873. Reason: worker-handle-scheduler-connection-broken
2023-06-22 18:59:47,475 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:34945. Reason: worker-close
2023-06-22 18:59:47,475 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:33167'. Reason: nanny-close
2023-06-22 18:59:47,476 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:50132 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 18:59:47,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 18:59:47,476 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.33.227.169:50078 remote=tcp://10.33.227.169:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-22 18:59:47,478 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:38431'. Reason: nanny-close
2023-06-22 18:59:47,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 18:59:47,480 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:46109'. Reason: nanny-close
2023-06-22 18:59:47,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 18:59:47,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:45625'. Reason: nanny-close
2023-06-22 18:59:47,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 18:59:47,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:45353'. Reason: nanny-close
2023-06-22 18:59:47,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 18:59:47,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:40891'. Reason: nanny-close
2023-06-22 18:59:47,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 18:59:47,483 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:34619'. Reason: nanny-close
2023-06-22 18:59:47,483 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 18:59:47,483 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:38689'. Reason: nanny-close
2023-06-22 18:59:47,484 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 18:59:47,488 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:33167 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:56132 remote=tcp://10.33.227.169:33167>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:33167 after 100 s
2023-06-22 18:59:47,493 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:46109 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:32994 remote=tcp://10.33.227.169:46109>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:46109 after 100 s
2023-06-22 18:59:47,501 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:38689 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:40852 remote=tcp://10.33.227.169:38689>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:38689 after 100 s
2023-06-22 18:59:50,685 - distributed.nanny - WARNING - Worker process still alive after 3.199972076416016 seconds, killing
2023-06-22 18:59:50,686 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 18:59:50,686 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 18:59:50,687 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 18:59:50,687 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 18:59:50,688 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 18:59:50,688 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-22 18:59:50,689 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 18:59:51,478 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 18:59:51,480 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 18:59:51,481 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 18:59:51,482 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 18:59:51,483 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 18:59:51,483 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 18:59:51,483 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 18:59:51,484 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 18:59:51,486 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1318947 parent=1318905 started daemon>
2023-06-22 18:59:51,486 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1318944 parent=1318905 started daemon>
2023-06-22 18:59:51,486 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1318941 parent=1318905 started daemon>
2023-06-22 18:59:51,486 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1318938 parent=1318905 started daemon>
2023-06-22 18:59:51,486 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1318935 parent=1318905 started daemon>
2023-06-22 18:59:51,486 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1318932 parent=1318905 started daemon>
2023-06-22 18:59:51,486 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1318929 parent=1318905 started daemon>
2023-06-22 18:59:51,486 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1318926 parent=1318905 started daemon>
2023-06-22 18:59:51,525 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1318929 exit status was already read will report exitcode 255
2023-06-22 18:59:51,730 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1318941 exit status was already read will report exitcode 255
2023-06-22 18:59:52,179 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1318932 exit status was already read will report exitcode 255
2023-06-22 18:59:52,435 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1318935 exit status was already read will report exitcode 255
