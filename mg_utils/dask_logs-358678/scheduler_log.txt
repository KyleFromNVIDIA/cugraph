RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 18:17:17,461 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 18:17:17,973 - distributed.scheduler - INFO - State start
2023-06-26 18:17:17,974 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-crs92ypl', purging
2023-06-26 18:17:17,975 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7j57htpv', purging
2023-06-26 18:17:17,975 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4q_v5awl', purging
2023-06-26 18:17:17,975 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x28dbcxo', purging
2023-06-26 18:17:17,975 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4ovhbeww', purging
2023-06-26 18:17:17,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-shox3wvs', purging
2023-06-26 18:17:17,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-uy9vu6ay', purging
2023-06-26 18:17:17,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5vmq3kfv', purging
2023-06-26 18:17:17,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-q_lsyu4e', purging
2023-06-26 18:17:17,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ag3pbiju', purging
2023-06-26 18:17:17,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_tvwqx7d', purging
2023-06-26 18:17:17,977 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1idt2af4', purging
2023-06-26 18:17:17,977 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3bg2dnr3', purging
2023-06-26 18:17:17,977 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-czd_jhp0', purging
2023-06-26 18:17:17,977 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5hvlexef', purging
2023-06-26 18:17:17,977 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pct8_wh3', purging
2023-06-26 18:17:17,990 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 18:17:17,990 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 18:17:17,991 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 18:17:36,657 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39311', status: init, memory: 0, processing: 0>
2023-06-26 18:17:36,660 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39311
2023-06-26 18:17:36,660 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46048
2023-06-26 18:17:36,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46155', status: init, memory: 0, processing: 0>
2023-06-26 18:17:36,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46155
2023-06-26 18:17:36,899 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46056
2023-06-26 18:17:36,922 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41597', status: init, memory: 0, processing: 0>
2023-06-26 18:17:36,922 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41597
2023-06-26 18:17:36,922 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46068
2023-06-26 18:17:36,951 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41775', status: init, memory: 0, processing: 0>
2023-06-26 18:17:36,951 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41775
2023-06-26 18:17:36,951 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46084
2023-06-26 18:17:36,988 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36759', status: init, memory: 0, processing: 0>
2023-06-26 18:17:36,988 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36759
2023-06-26 18:17:36,988 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46092
2023-06-26 18:17:37,018 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46591', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,018 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46591
2023-06-26 18:17:37,018 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46098
2023-06-26 18:17:37,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41345', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,025 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41345
2023-06-26 18:17:37,025 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46112
2023-06-26 18:17:37,026 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46209', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46209
2023-06-26 18:17:37,026 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46102
2023-06-26 18:17:37,038 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36093', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36093
2023-06-26 18:17:37,039 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46128
2023-06-26 18:17:37,086 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46135', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,086 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46135
2023-06-26 18:17:37,087 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46138
2023-06-26 18:17:37,109 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:37137', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,110 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:37137
2023-06-26 18:17:37,110 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46146
2023-06-26 18:17:37,131 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39335', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,131 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39335
2023-06-26 18:17:37,131 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46158
2023-06-26 18:17:37,156 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36151', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36151
2023-06-26 18:17:37,156 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46172
2023-06-26 18:17:37,206 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41121', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41121
2023-06-26 18:17:37,207 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46184
2023-06-26 18:17:37,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41221', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,208 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41221
2023-06-26 18:17:37,208 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46194
2023-06-26 18:17:37,210 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43507', status: init, memory: 0, processing: 0>
2023-06-26 18:17:37,210 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43507
2023-06-26 18:17:37,210 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46190
2023-06-26 18:18:04,144 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 18:18:04,145 - distributed.core - INFO - Connection to tcp://10.120.104.11:46194 has been closed.
2023-06-26 18:18:04,145 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41221', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,146 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41221
2023-06-26 18:18:04,146 - distributed.core - INFO - Connection to tcp://10.120.104.11:46084 has been closed.
2023-06-26 18:18:04,146 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41775', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,146 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41775
2023-06-26 18:18:04,146 - distributed.core - INFO - Connection to tcp://10.120.104.11:46102 has been closed.
2023-06-26 18:18:04,146 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46209', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,146 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46209
2023-06-26 18:18:04,147 - distributed.core - INFO - Connection to tcp://10.120.104.11:46056 has been closed.
2023-06-26 18:18:04,147 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46155', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,147 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46155
2023-06-26 18:18:04,147 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 18:18:04,148 - distributed.core - INFO - Connection to tcp://10.120.104.11:46158 has been closed.
2023-06-26 18:18:04,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39335', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,148 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39335
2023-06-26 18:18:04,148 - distributed.core - INFO - Connection to tcp://10.120.104.11:46098 has been closed.
2023-06-26 18:18:04,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46591', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,148 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46591
2023-06-26 18:18:04,148 - distributed.core - INFO - Connection to tcp://10.120.104.11:46128 has been closed.
2023-06-26 18:18:04,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36093', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,148 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36093
2023-06-26 18:18:04,149 - distributed.core - INFO - Connection to tcp://10.120.104.11:46146 has been closed.
2023-06-26 18:18:04,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:37137', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,149 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37137
2023-06-26 18:18:04,149 - distributed.core - INFO - Connection to tcp://10.120.104.11:46138 has been closed.
2023-06-26 18:18:04,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46135', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,149 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46135
2023-06-26 18:18:04,149 - distributed.core - INFO - Connection to tcp://10.120.104.11:46112 has been closed.
2023-06-26 18:18:04,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41345', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,149 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41345
2023-06-26 18:18:04,149 - distributed.core - INFO - Connection to tcp://10.120.104.11:46092 has been closed.
2023-06-26 18:18:04,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36759', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,149 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36759
2023-06-26 18:18:04,150 - distributed.core - INFO - Connection to tcp://10.120.104.11:46048 has been closed.
2023-06-26 18:18:04,150 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39311', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,150 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39311
2023-06-26 18:18:04,150 - distributed.core - INFO - Connection to tcp://10.120.104.11:46068 has been closed.
2023-06-26 18:18:04,150 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41597', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,150 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41597
2023-06-26 18:18:04,151 - distributed.core - INFO - Connection to tcp://10.120.104.11:46184 has been closed.
2023-06-26 18:18:04,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41121', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,151 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41121
2023-06-26 18:18:04,151 - distributed.core - INFO - Connection to tcp://10.120.104.11:46172 has been closed.
2023-06-26 18:18:04,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36151', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,151 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36151
2023-06-26 18:18:04,151 - distributed.core - INFO - Connection to tcp://10.120.104.11:46190 has been closed.
2023-06-26 18:18:04,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43507', status: running, memory: 0, processing: 0>
2023-06-26 18:18:04,151 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43507
2023-06-26 18:18:04,151 - distributed.scheduler - INFO - Lost all workers
2023-06-26 18:18:04,151 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46128>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:18:04,152 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46172>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46172>: Stream is closed
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46092>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46146>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46048>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46048>: Stream is closed
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46158>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46184>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46184>: Stream is closed
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46112>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46068>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46068>: Stream is closed
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46190>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46190>: Stream is closed
2023-06-26 18:18:04,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46138>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:18:04,154 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46098>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:18:04,154 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 18:18:04,156 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 18:18:04,157 - distributed.scheduler - INFO - End scheduler
