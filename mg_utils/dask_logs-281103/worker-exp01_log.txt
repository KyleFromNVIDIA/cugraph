RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 16:50:50,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35883'
2023-06-26 16:50:50,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45813'
2023-06-26 16:50:50,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42809'
2023-06-26 16:50:50,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46621'
2023-06-26 16:50:50,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43099'
2023-06-26 16:50:50,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37517'
2023-06-26 16:50:50,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41411'
2023-06-26 16:50:50,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33485'
2023-06-26 16:50:50,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36425'
2023-06-26 16:50:50,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36821'
2023-06-26 16:50:50,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33807'
2023-06-26 16:50:50,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34447'
2023-06-26 16:50:50,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37955'
2023-06-26 16:50:50,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:44405'
2023-06-26 16:50:50,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46199'
2023-06-26 16:50:50,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:40343'
2023-06-26 16:50:52,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,589 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,589 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,601 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,602 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,649 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,649 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,653 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,660 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,726 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,751 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,760 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:50:52,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:50:52,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,771 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,778 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,826 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,830 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,852 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,904 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,925 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,927 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,929 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,929 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:52,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:50:58,484 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43275
2023-06-26 16:50:58,485 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43275
2023-06-26 16:50:58,485 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35983
2023-06-26 16:50:58,485 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:58,485 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:58,485 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:58,485 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:58,485 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4wmez1kn
2023-06-26 16:50:58,485 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c2456b3c-8fed-4787-af7d-feb30c89cb6f
2023-06-26 16:50:58,486 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5eae8c60-33e4-4a1c-b89a-194209b135f0
2023-06-26 16:50:58,699 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39753
2023-06-26 16:50:58,699 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39753
2023-06-26 16:50:58,699 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36343
2023-06-26 16:50:58,700 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:58,700 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:58,700 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:58,700 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:58,700 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a96jik81
2023-06-26 16:50:58,700 - distributed.worker - INFO - Starting Worker plugin PreImport-19a967f1-5bdf-4534-bed9-00eccec18734
2023-06-26 16:50:58,700 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4692b258-ad95-4056-a0c9-61d4799866c0
2023-06-26 16:50:58,724 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35651
2023-06-26 16:50:58,724 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35651
2023-06-26 16:50:58,724 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45115
2023-06-26 16:50:58,724 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:58,724 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:58,724 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:58,724 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:58,725 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7z06s7aq
2023-06-26 16:50:58,725 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79ed6185-1b66-4451-a436-d18504d90474
2023-06-26 16:50:58,728 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36111
2023-06-26 16:50:58,729 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36111
2023-06-26 16:50:58,729 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35203
2023-06-26 16:50:58,729 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:58,729 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:58,729 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:58,729 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:58,729 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7mhq9nfd
2023-06-26 16:50:58,729 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c05052e-69b6-48b4-a027-2e9a6069701c
2023-06-26 16:50:58,784 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39061
2023-06-26 16:50:58,784 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39061
2023-06-26 16:50:58,785 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35177
2023-06-26 16:50:58,785 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:58,785 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:58,785 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:58,785 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:58,785 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gky6p4j7
2023-06-26 16:50:58,785 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-686563e0-0d48-45de-ac40-4f4a162ac46a
2023-06-26 16:50:58,786 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d399f445-0e6a-4a8d-bf27-dabc8f6464f5
2023-06-26 16:50:58,789 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33305
2023-06-26 16:50:58,789 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33305
2023-06-26 16:50:58,790 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41559
2023-06-26 16:50:58,790 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:58,790 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:58,790 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:58,790 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:58,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hv0n9wxl
2023-06-26 16:50:58,790 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0091ec68-d6b7-4187-8c3e-9e5bd286a8da
2023-06-26 16:50:58,790 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd8ba3c8-fcab-455f-8e27-8e586c7915a6
2023-06-26 16:50:58,993 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32807
2023-06-26 16:50:58,993 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32807
2023-06-26 16:50:58,993 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33319
2023-06-26 16:50:58,993 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:58,993 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:58,993 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:58,993 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:58,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-no9stt1a
2023-06-26 16:50:58,994 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ab32456-bbbe-4897-af5a-9d250ab9025e
2023-06-26 16:50:59,632 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42685
2023-06-26 16:50:59,632 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42685
2023-06-26 16:50:59,632 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42929
2023-06-26 16:50:59,632 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,632 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,632 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,632 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bsp5x8di
2023-06-26 16:50:59,633 - distributed.worker - INFO - Starting Worker plugin PreImport-07336294-baf4-4a99-9f5b-87f253a4dfea
2023-06-26 16:50:59,633 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f3cad8b4-619f-4346-81ad-281f84231757
2023-06-26 16:50:59,636 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41219
2023-06-26 16:50:59,637 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41219
2023-06-26 16:50:59,637 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35743
2023-06-26 16:50:59,637 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,637 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,637 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,637 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,637 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sj5lvpm5
2023-06-26 16:50:59,638 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d71c643e-346a-4dce-a4e8-188fdbfef1c6
2023-06-26 16:50:59,642 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44181
2023-06-26 16:50:59,643 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44181
2023-06-26 16:50:59,643 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46213
2023-06-26 16:50:59,643 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,643 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,643 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,643 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,643 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m3_1wgy1
2023-06-26 16:50:59,644 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2cbd5137-ec2d-4fc5-8af4-1e4a14ce527e
2023-06-26 16:50:59,649 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:32779
2023-06-26 16:50:59,649 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:32779
2023-06-26 16:50:59,650 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46707
2023-06-26 16:50:59,650 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,650 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,650 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,650 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h1pwjrk7
2023-06-26 16:50:59,650 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33133
2023-06-26 16:50:59,650 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33133
2023-06-26 16:50:59,650 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38895
2023-06-26 16:50:59,650 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,651 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,651 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,651 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,651 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zw912a8p
2023-06-26 16:50:59,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a46f33f-4977-444f-9ddc-3d0e635cec58
2023-06-26 16:50:59,651 - distributed.worker - INFO - Starting Worker plugin PreImport-352b1200-7f1d-4f16-a39c-ab2eb2ae5103
2023-06-26 16:50:59,652 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46bbb328-3f7b-484b-9f42-1f33820f7fb8
2023-06-26 16:50:59,660 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43551
2023-06-26 16:50:59,660 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43551
2023-06-26 16:50:59,660 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41361
2023-06-26 16:50:59,660 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,660 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,660 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,660 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3z8poten
2023-06-26 16:50:59,660 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43271
2023-06-26 16:50:59,660 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43271
2023-06-26 16:50:59,661 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35161
2023-06-26 16:50:59,661 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,661 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,661 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a34d3dfb-56fc-462b-b191-4c0bd745ecbd
2023-06-26 16:50:59,661 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,661 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-68o5j9ee
2023-06-26 16:50:59,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-03b483a4-3910-4398-969a-f17baf60db74
2023-06-26 16:50:59,667 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44299
2023-06-26 16:50:59,667 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44299
2023-06-26 16:50:59,667 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41713
2023-06-26 16:50:59,667 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,667 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,667 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,667 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,667 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qnr0ijea
2023-06-26 16:50:59,667 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40833
2023-06-26 16:50:59,667 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40833
2023-06-26 16:50:59,667 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36881
2023-06-26 16:50:59,667 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:50:59,667 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:50:59,667 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:50:59,668 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:50:59,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e759aaf4-9476-45b5-be98-a68aad2e797c
2023-06-26 16:50:59,668 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a3ra7jei
2023-06-26 16:50:59,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09a786a1-9dff-4f41-abdc-1266c211b942
2023-06-26 16:51:01,105 - distributed.worker - INFO - Starting Worker plugin PreImport-f1558aa2-9bba-4af5-9e58-f95d5255d440
2023-06-26 16:51:01,107 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:01,139 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:01,139 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:01,144 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:02,303 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b0d2c7e-d95f-4bcc-865e-73b3b7a1d6b8
2023-06-26 16:51:02,304 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,335 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:02,335 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,337 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:02,446 - distributed.worker - INFO - Starting Worker plugin PreImport-7e02dd36-638f-46c3-9fac-447681575f16
2023-06-26 16:51:02,447 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,472 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:02,472 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,474 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:02,590 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a32bc9d-fbbb-413b-8a51-15af79f4789d
2023-06-26 16:51:02,590 - distributed.worker - INFO - Starting Worker plugin PreImport-e0c10a82-b8f0-40f8-a58c-705c465e3103
2023-06-26 16:51:02,591 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,604 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:02,604 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,609 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:02,635 - distributed.worker - INFO - Starting Worker plugin PreImport-4fb8a1ee-519a-432f-9c4a-d210fd024d85
2023-06-26 16:51:02,636 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55887b4e-4b49-48fe-9759-8eb88f324f49
2023-06-26 16:51:02,644 - distributed.worker - INFO - Starting Worker plugin PreImport-cd824725-2a57-4f85-8648-20bb1f7f4201
2023-06-26 16:51:02,644 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,655 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:02,656 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,657 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:02,665 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:02,665 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,669 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:02,900 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7236beae-47b4-471b-a0dc-4e7722f49da6
2023-06-26 16:51:02,901 - distributed.worker - INFO - Starting Worker plugin PreImport-70eb1f1d-dc8e-41a6-a58d-155acdd7fcfa
2023-06-26 16:51:02,902 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,925 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:02,925 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:02,927 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,045 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f190b9a9-438c-456f-b65d-76a83d3ea7f3
2023-06-26 16:51:03,045 - distributed.worker - INFO - Starting Worker plugin PreImport-5a41e3f1-eb88-4597-9237-57ca67c3735c
2023-06-26 16:51:03,045 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,062 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,063 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,155 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5588246e-52f6-43c0-aae7-9bdabfd58bd0
2023-06-26 16:51:03,157 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,168 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5f190eb-0f8c-4ca4-aaed-fd2653dffbb9
2023-06-26 16:51:03,169 - distributed.worker - INFO - Starting Worker plugin PreImport-0302a5e9-72c6-43cc-a27f-05bbe43428cb
2023-06-26 16:51:03,170 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0255acf7-5b36-4972-84ee-266841353e40
2023-06-26 16:51:03,177 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,177 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,179 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,179 - distributed.worker - INFO - Starting Worker plugin PreImport-2dd0b08b-ab36-484d-8eb1-7455e8b94a46
2023-06-26 16:51:03,182 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,185 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b6ebc7d-9aec-431f-8d66-8949a5a9f3b1
2023-06-26 16:51:03,185 - distributed.worker - INFO - Starting Worker plugin PreImport-2bffd1d3-fd92-4860-8519-63203ee3282a
2023-06-26 16:51:03,187 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,196 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-802de515-b657-4f9d-a072-8df72ef2494c
2023-06-26 16:51:03,196 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8bdda38f-97e6-4736-bd33-ca8f78cf9dd4
2023-06-26 16:51:03,198 - distributed.worker - INFO - Starting Worker plugin PreImport-2239a527-0f8d-478a-aa2b-d80ebc938905
2023-06-26 16:51:03,198 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,199 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,199 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,200 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,203 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,206 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,206 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,209 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,212 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,212 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,213 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,215 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0a5c0f7-0acf-4c71-9605-cd3ba666d4d4
2023-06-26 16:51:03,216 - distributed.worker - INFO - Starting Worker plugin PreImport-6e218e32-b74e-41d8-872f-583f4c7f3309
2023-06-26 16:51:03,216 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,216 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,216 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,219 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,219 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,219 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,221 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b20975c-e5da-4aa6-b10a-ad823206285e
2023-06-26 16:51:03,221 - distributed.worker - INFO - Starting Worker plugin PreImport-2b65fe72-3f6a-4708-93e0-610890576f59
2023-06-26 16:51:03,221 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,222 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,228 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,229 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,230 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:03,237 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:51:03,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:51:03,240 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:51:05,651 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,651 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,651 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,652 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,653 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,653 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,653 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,653 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,654 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,654 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,654 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,654 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,654 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,655 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,655 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,656 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,669 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,670 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,670 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,670 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,670 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,670 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,670 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:05,670 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:51:06,356 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,356 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,356 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,356 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,356 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,356 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:06,357 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:51:09,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:09,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:09,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:20,933 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,165 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,432 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,451 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,534 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,564 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,588 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,609 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,634 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,638 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,660 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,689 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,695 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,755 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:21,782 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:22,004 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:51:28,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:51:28,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:07,633 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,633 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,636 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,638 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,638 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,638 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,639 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,639 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,639 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,639 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,639 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,639 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,641 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,641 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,641 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:07,642 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 16:52:10,779 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:10,786 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 16:52:11,118 - distributed.worker - WARNING - Compute Failed
Key:       ('_filter_batches-915eaf0907313310507fa29ce63b475e', 1)
Function:  subgraph_callable-3f752934-c304-4ac2-b24a-630999a2
args:      ({'number': 1, 'division': None},            _START_  _BATCH_
0         26729893     3022
1         26729962     3022
2         26730024     3022
3         26730060     3022
4         26730161     3022
...            ...      ...
1546779  719885605     6043
1546780  719885607     6043
1546781  719885626     6043
1546782  719885661     6043
1546783  719885696     6043

[1546784 rows x 2 columns],            _START_  _BATCH_
0         26729893     3022
1         26729962     3022
2         26730024     3022
3         26730060     3022
4         26730161     3022
...            ...      ...
1546779  719885605     6043
1546780  719885607     6043
1546781  719885626     6043
1546782  719885661     6043
1546783  719885696     6043

[1546784 rows x 2 columns], '_BATCH_', 1169, 'rename-e86d3e18d20c7cb28128373a62cb3dd7')
kwargs:    {}
Exception: 'TypeError("_filter_batches() got multiple values for argument \'partition_info\'")'

2023-06-26 16:52:11,118 - distributed.worker - WARNING - Compute Failed
Key:       ('_filter_batches-915eaf0907313310507fa29ce63b475e', 0)
Function:  subgraph_callable-3f752934-c304-4ac2-b24a-630999a2
args:      ({'number': 0, 'division': None},            _START_  _BATCH_
0              602        0
1              684        0
2             1384        0
3             1525        0
4             2127        0
...            ...      ...
1546779  693089189     3021
1546780  693089300     3021
1546781  693089304     3021
1546782  693089465     3021
1546783  693089507     3021

[1546784 rows x 2 columns],            _START_  _BATCH_
0              602        0
1              684        0
2             1384        0
3             1525        0
4             2127        0
...            ...      ...
1546779  693089189     3021
1546780  693089300     3021
1546781  693089304     3021
1546782  693089465     3021
1546783  693089507     3021

[1546784 rows x 2 columns], '_BATCH_', 1169, 'rename-e86d3e18d20c7cb28128373a62cb3dd7')
kwargs:    {}
Exception: 'TypeError("_filter_batches() got multiple values for argument \'partition_info\'")'

2023-06-26 16:52:11,119 - distributed.worker - WARNING - Compute Failed
Key:       ('_filter_batches-915eaf0907313310507fa29ce63b475e', 3)
Function:  subgraph_callable-3f752934-c304-4ac2-b24a-630999a2
args:      ({'number': 3, 'division': None},            _START_  _BATCH_
0        102625137     9066
1        102625140     9066
2        102625165     9066
3        102625257     9066
4        102625281     9066
...            ...      ...
1546775  777419458    12087
1546776  777419463    12087
1546777  777419519    12087
1546778  777419663    12087
1546779  777419689    12087

[1546780 rows x 2 columns],            _START_  _BATCH_
0        102625137     9066
1        102625140     9066
2        102625165     9066
3        102625257     9066
4        102625281     9066
...            ...      ...
1546775  777419458    12087
1546776  777419463    12087
1546777  777419519    12087
1546778  777419663    12087
1546779  777419689    12087

[1546780 rows x 2 columns], '_BATCH_', 1169, 'rename-e86d3e18d20c7cb28128373a62cb3dd7')
kwargs:    {}
Exception: 'TypeError("_filter_batches() got multiple values for argument \'partition_info\'")'

2023-06-26 16:52:11,119 - distributed.worker - WARNING - Compute Failed
Key:       ('_filter_batches-915eaf0907313310507fa29ce63b475e', 2)
Function:  subgraph_callable-3f752934-c304-4ac2-b24a-630999a2
args:      ({'number': 2, 'division': None},            _START_  _BATCH_
0         53526087     6044
1         53526142     6044
2         53526143     6044
3         53526290     6044
4         53526327     6044
...            ...      ...
1546775  768984725     9065
1546776  768984744     9065
1546777  768984794     9065
1546778  768984809     9065
1546779  768984842     9065

[1546780 rows x 2 columns],            _START_  _BATCH_
0         53526087     6044
1         53526142     6044
2         53526143     6044
3         53526290     6044
4         53526327     6044
...            ...      ...
1546775  768984725     9065
1546776  768984744     9065
1546777  768984794     9065
1546778  768984809     9065
1546779  768984842     9065

[1546780 rows x 2 columns], '_BATCH_', 1169, 'rename-e86d3e18d20c7cb28128373a62cb3dd7')
kwargs:    {}
Exception: 'TypeError("_filter_batches() got multiple values for argument \'partition_info\'")'

2023-06-26 16:52:11,157 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:11,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:52:15,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,647 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,649 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32779. Reason: scheduler-restart
2023-06-26 16:52:15,650 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,650 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,650 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,650 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:32807. Reason: scheduler-restart
2023-06-26 16:52:15,651 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33133. Reason: scheduler-restart
2023-06-26 16:52:15,651 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,651 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,651 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33305. Reason: scheduler-restart
2023-06-26 16:52:15,652 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,652 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,652 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35651. Reason: scheduler-restart
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,653 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,653 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36111. Reason: scheduler-restart
2023-06-26 16:52:15,653 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,653 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,653 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,653 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39061. Reason: scheduler-restart
2023-06-26 16:52:15,653 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,653 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,653 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,654 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,654 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39753. Reason: scheduler-restart
2023-06-26 16:52:15,654 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,654 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,654 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,654 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,654 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40833. Reason: scheduler-restart
2023-06-26 16:52:15,654 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,655 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,656 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,656 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,656 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,656 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,657 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,657 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,658 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,658 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42685. Reason: scheduler-restart
2023-06-26 16:52:15,658 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 16:52:15,659 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,661 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43271. Reason: scheduler-restart
2023-06-26 16:52:15,661 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43275. Reason: scheduler-restart
2023-06-26 16:52:15,661 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,661 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,661 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,661 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,661 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,661 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,661 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,662 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41219. Reason: scheduler-restart
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,662 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,662 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,663 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42685
2023-06-26 16:52:15,665 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42685
2023-06-26 16:52:15,665 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,666 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,670 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43551. Reason: scheduler-restart
2023-06-26 16:52:15,670 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42685
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43271
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40833
2023-06-26 16:52:15,671 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41219
2023-06-26 16:52:15,672 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,672 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,673 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,673 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44299. Reason: scheduler-restart
2023-06-26 16:52:15,673 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,676 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44181. Reason: scheduler-restart
2023-06-26 16:52:15,676 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,676 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,676 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,676 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,676 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,676 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,676 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,676 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,677 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,677 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,677 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,677 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,677 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,677 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,677 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42685
2023-06-26 16:52:15,677 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43271
2023-06-26 16:52:15,677 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,690 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,691 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42685
2023-06-26 16:52:15,691 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43271
2023-06-26 16:52:15,691 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40833
2023-06-26 16:52:15,691 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41219
2023-06-26 16:52:15,691 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,694 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40833
2023-06-26 16:52:15,694 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41219
2023-06-26 16:52:15,695 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43551
2023-06-26 16:52:15,695 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,695 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43275
2023-06-26 16:52:15,695 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,696 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42685
2023-06-26 16:52:15,696 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43271
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7fa5eca8d100, tag: 0x1d9a80c243f1d4ed>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7fa5eca8d100, tag: 0x1d9a80c243f1d4ed>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7fa5eca8d380, tag: 0x30d3b55b3447df2f>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7fa5eca8d380, tag: 0x30d3b55b3447df2f>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7fa5eca8d1c0, tag: 0x90cc7732f7e1f7f9>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7fa5eca8d1c0, tag: 0x90cc7732f7e1f7f9>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7fa5eca8d580, tag: 0xd285d4df5ad4f79c>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7fa5eca8d580, tag: 0xd285d4df5ad4f79c>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7fa5eca8d500, tag: 0x59911c3385cef7da>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7fa5eca8d500, tag: 0x59911c3385cef7da>: 
2023-06-26 16:52:15,698 - distributed.nanny - INFO - Worker closed
2023-06-26 16:52:15,700 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40833
2023-06-26 16:52:15,701 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41219
2023-06-26 16:52:15,709 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43551
2023-06-26 16:52:15,710 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43275
2023-06-26 16:52:15,710 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 16:52:15,711 - distributed.nanny - INFO - Worker closed
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-6769' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
2023-06-26 16:52:18,555 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:19,093 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:19,664 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:20,265 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:21,138 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:21,139 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:21,141 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:21,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:21,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:21,327 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:21,444 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:22,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:22,205 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:22,380 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:22,726 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:22,726 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:22,897 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:23,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:23,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:23,503 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:23,507 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:23,508 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:23,512 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:23,513 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:23,515 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:23,517 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:23,519 - distributed.nanny - WARNING - Restarting worker
2023-06-26 16:52:23,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:23,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:23,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:23,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:23,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:23,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:23,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:23,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:23,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:23,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:23,715 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:23,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:23,777 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:25,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:25,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:25,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:25,287 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:25,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:25,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:25,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:25,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:25,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:25,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:25,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:25,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:25,301 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:25,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 16:52:25,340 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 16:52:25,443 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,471 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,479 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,480 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 16:52:25,920 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46053
2023-06-26 16:52:25,920 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46053
2023-06-26 16:52:25,920 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40347
2023-06-26 16:52:25,920 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:25,920 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:25,920 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:25,920 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:25,920 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tni31nl3
2023-06-26 16:52:25,921 - distributed.worker - INFO - Starting Worker plugin PreImport-478ee3f0-b559-4da6-bd91-0dca7ff4608f
2023-06-26 16:52:25,921 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1584cc3f-c7ca-4cf8-9a68-c57d5663f59a
2023-06-26 16:52:26,237 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33883
2023-06-26 16:52:26,237 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33883
2023-06-26 16:52:26,237 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44795
2023-06-26 16:52:26,238 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:26,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:26,238 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:26,238 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:26,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v9f3sggo
2023-06-26 16:52:26,238 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48580ce6-5f27-49ca-b03d-3beca273c6c3
2023-06-26 16:52:26,268 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40303
2023-06-26 16:52:26,269 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40303
2023-06-26 16:52:26,269 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46369
2023-06-26 16:52:26,269 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:26,269 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:26,269 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:26,269 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:26,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0a5be3om
2023-06-26 16:52:26,269 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-281fefa0-6abf-4ed0-b2fc-014f07fec7ee
2023-06-26 16:52:26,269 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1bde5b8-1f2e-4c46-adf0-bce6499d8a6b
2023-06-26 16:52:27,361 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40541
2023-06-26 16:52:27,361 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40541
2023-06-26 16:52:27,361 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45083
2023-06-26 16:52:27,361 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:27,361 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:27,361 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:27,362 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:27,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x1wi9far
2023-06-26 16:52:27,362 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c79f8ff4-b685-4e28-a575-260f16766506
2023-06-26 16:52:27,383 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44277
2023-06-26 16:52:27,383 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44277
2023-06-26 16:52:27,383 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32829
2023-06-26 16:52:27,383 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:27,383 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:27,383 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:27,384 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:27,384 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vcmbponk
2023-06-26 16:52:27,384 - distributed.worker - INFO - Starting Worker plugin PreImport-45e070e3-dbc3-4aa3-a0bc-1c89c756c53d
2023-06-26 16:52:27,384 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ce23f56-9559-4f33-a3d4-1dece73a014c
2023-06-26 16:52:27,648 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42057
2023-06-26 16:52:27,648 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42057
2023-06-26 16:52:27,648 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40263
2023-06-26 16:52:27,648 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:27,648 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:27,648 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:27,648 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:27,648 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rwm7y29e
2023-06-26 16:52:27,649 - distributed.worker - INFO - Starting Worker plugin PreImport-8928dd00-1f32-49cb-899d-fd286b4c1716
2023-06-26 16:52:27,649 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b346ba8-bef8-4535-8e85-45c7153cbb68
2023-06-26 16:52:27,705 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42053
2023-06-26 16:52:27,705 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42053
2023-06-26 16:52:27,705 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43101
2023-06-26 16:52:27,705 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:27,705 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:27,705 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:27,705 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:27,706 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4u4e3stn
2023-06-26 16:52:27,706 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca71c6af-dfa1-4615-b75a-cbac459e8ed7
2023-06-26 16:52:27,709 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44765
2023-06-26 16:52:27,709 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44765
2023-06-26 16:52:27,709 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32809
2023-06-26 16:52:27,709 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:27,709 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:27,709 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:27,709 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:27,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-54qv18t3
2023-06-26 16:52:27,710 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5224836d-c085-4c4c-8bd1-4236a98a5b41
2023-06-26 16:52:27,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d862499-2893-4f5c-858e-73373f8ef1e0
2023-06-26 16:52:28,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ded9a71d-1c47-4222-af92-ea360a2c2824
2023-06-26 16:52:28,561 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:28,573 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:28,574 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:28,577 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:28,837 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b4f53c3-c519-45cb-9c25-34f1492719f1
2023-06-26 16:52:28,838 - distributed.worker - INFO - Starting Worker plugin PreImport-d2491cb1-d673-45c7-8733-7c0ea1cda761
2023-06-26 16:52:28,840 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:28,857 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:28,857 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:28,864 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:28,885 - distributed.worker - INFO - Starting Worker plugin PreImport-534329d2-37a9-451d-8cd4-7c1a5f76da64
2023-06-26 16:52:28,887 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:28,900 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:28,900 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:28,902 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:31,175 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c2fb18b7-ba19-49a6-a251-8e421642320d
2023-06-26 16:52:31,176 - distributed.worker - INFO - Starting Worker plugin PreImport-721d6d97-ad98-4525-b4f5-0fdfd4a4fe0b
2023-06-26 16:52:31,177 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,200 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:31,200 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,203 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:31,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c71dbb1-2c72-48c5-b261-4ed7c8225636
2023-06-26 16:52:31,627 - distributed.worker - INFO - Starting Worker plugin PreImport-f32f04cd-917f-4dbf-8475-712ff8c2fe73
2023-06-26 16:52:31,629 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,640 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64079479-0d35-4260-8efa-a3aff255be25
2023-06-26 16:52:31,641 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,645 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:31,645 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,648 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:31,649 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8227b8a8-8444-4d82-a3aa-3b8f49c6c6a1
2023-06-26 16:52:31,650 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,652 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:31,652 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,653 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:31,653 - distributed.worker - INFO - Starting Worker plugin PreImport-31778701-7ba8-4c10-981d-208eb12435fd
2023-06-26 16:52:31,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,663 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:31,663 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,665 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:31,665 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:31,665 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:31,668 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:32,344 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39215
2023-06-26 16:52:32,344 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39215
2023-06-26 16:52:32,344 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33921
2023-06-26 16:52:32,344 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:32,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:32,344 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:32,344 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:32,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t3vm8h3s
2023-06-26 16:52:32,345 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7997629-d943-426e-94ef-90468b0f760b
2023-06-26 16:52:32,354 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38473
2023-06-26 16:52:32,355 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38473
2023-06-26 16:52:32,355 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33433
2023-06-26 16:52:32,355 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:32,355 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:32,355 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:32,355 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:32,355 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bgk7hr5w
2023-06-26 16:52:32,355 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12148175-1825-43ca-b59f-423dec5bb60d
2023-06-26 16:52:32,406 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40111
2023-06-26 16:52:32,407 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40111
2023-06-26 16:52:32,407 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33839
2023-06-26 16:52:32,407 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:32,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:32,407 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:32,407 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:32,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ir8lbiue
2023-06-26 16:52:32,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2da577bb-e594-4a31-9595-460556b585af
2023-06-26 16:52:32,854 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39357
2023-06-26 16:52:32,855 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39357
2023-06-26 16:52:32,855 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43221
2023-06-26 16:52:32,855 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:32,855 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:32,855 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:32,855 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:32,855 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gg6s9xm8
2023-06-26 16:52:32,857 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3efcf3e1-d3c4-4c14-9234-979d530bd84a
2023-06-26 16:52:32,879 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40293
2023-06-26 16:52:32,880 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40293
2023-06-26 16:52:32,880 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42769
2023-06-26 16:52:32,880 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:32,880 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:32,880 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:32,880 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:32,880 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vxga4qbu
2023-06-26 16:52:32,881 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f81317c-b07a-4dc7-ad9a-eba4583a7d4a
2023-06-26 16:52:32,912 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44019
2023-06-26 16:52:32,913 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44019
2023-06-26 16:52:32,913 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35977
2023-06-26 16:52:32,913 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:32,913 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:32,913 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:32,913 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:32,913 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s2bbhscq
2023-06-26 16:52:32,914 - distributed.worker - INFO - Starting Worker plugin PreImport-1ce0796d-ec55-427f-bde9-3ce686d9102d
2023-06-26 16:52:32,914 - distributed.worker - INFO - Starting Worker plugin RMMSetup-865b2c2c-93f6-46b9-980e-4b9eae167e1f
2023-06-26 16:52:33,031 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41653
2023-06-26 16:52:33,031 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41653
2023-06-26 16:52:33,031 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46619
2023-06-26 16:52:33,031 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:33,031 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:33,031 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:33,031 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:33,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f17z9vg7
2023-06-26 16:52:33,032 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e4a82ea-ab84-447e-aa6a-fd9237ec6730
2023-06-26 16:52:33,032 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc67230b-3a43-4f46-b45a-f02d2902bbcd
2023-06-26 16:52:33,050 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46803
2023-06-26 16:52:33,050 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46803
2023-06-26 16:52:33,050 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40185
2023-06-26 16:52:33,050 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 16:52:33,050 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:33,050 - distributed.worker - INFO -               Threads:                          1
2023-06-26 16:52:33,050 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 16:52:33,051 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z449ajeu
2023-06-26 16:52:33,051 - distributed.worker - INFO - Starting Worker plugin PreImport-3680816c-b831-4538-ae61-671cbf295d73
2023-06-26 16:52:33,051 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ab7d1f98-39fc-4e83-adb8-520a8d392e56
2023-06-26 16:52:34,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb661809-fad9-4992-b581-7f92dfb5692a
2023-06-26 16:52:34,770 - distributed.worker - INFO - Starting Worker plugin PreImport-c9473819-ab9d-45cc-929c-e2c821163536
2023-06-26 16:52:34,771 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,782 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a056d3a5-44c6-4a71-b923-c099cb5d0d77
2023-06-26 16:52:34,783 - distributed.worker - INFO - Starting Worker plugin PreImport-810b8545-d95b-43a7-88de-7f397a7d577b
2023-06-26 16:52:34,784 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,790 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:34,790 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,793 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:34,804 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:34,804 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,806 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:34,808 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-245b6b1f-c9fd-4d20-83c4-ea6ec066bda6
2023-06-26 16:52:34,809 - distributed.worker - INFO - Starting Worker plugin PreImport-cb1b7ebf-6d76-426a-bce0-590637001293
2023-06-26 16:52:34,809 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,822 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:34,823 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,824 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:34,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8de08d4-183a-480e-884d-f1c383d2c6c0
2023-06-26 16:52:34,843 - distributed.worker - INFO - Starting Worker plugin PreImport-2c34f9bd-0cb5-4b59-b22c-c395a5bcd6af
2023-06-26 16:52:34,845 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,846 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c9511e1-d0bd-492f-a0b9-b86c79066d20
2023-06-26 16:52:34,847 - distributed.worker - INFO - Starting Worker plugin PreImport-6f951420-e2f6-4546-8d35-5cc0c92ff3e9
2023-06-26 16:52:34,848 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,861 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:34,861 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,863 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:34,864 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:34,864 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,866 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:34,910 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-808a97d2-0d32-4d0a-b3c9-0cf5363beb08
2023-06-26 16:52:34,911 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,926 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:34,926 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,928 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:34,947 - distributed.worker - INFO - Starting Worker plugin PreImport-47e81bfe-6633-420a-b6ea-c31e91f31493
2023-06-26 16:52:34,949 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,951 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7c4b87a-96d9-46e4-bf04-33f4d46b30b7
2023-06-26 16:52:34,952 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,963 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:34,963 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,965 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:34,966 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 16:52:34,966 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 16:52:34,968 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 16:52:44,258 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,424 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,500 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,512 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,776 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,778 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,779 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,830 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,845 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,847 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,850 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,943 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:44,950 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:44,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:45,017 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:45,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:45,061 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:45,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:45,064 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 16:52:45,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:45,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,076 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,077 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,087 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,088 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,088 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,088 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,088 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,099 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,100 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,100 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,100 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,100 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,100 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:45,100 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 16:52:48,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:53,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:53,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:54,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:54,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:54,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:54,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:54,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:54,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:54,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:58,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:58,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:58,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:58,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:53:02,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:53:02,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:53:02,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:02,837 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 16:53:14,684 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,684 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,684 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:14,685 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44765. Reason: worker-close
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42053. Reason: worker-close
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40541. Reason: worker-close
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46803. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40111. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40303. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41653. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38473. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42057. Reason: worker-close
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33883. Reason: worker-close
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44277. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39215. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40293. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,458 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44019. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,459 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46053. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,459 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39357. Reason: worker-handle-scheduler-connection-broken
2023-06-26 16:53:19,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35883'. Reason: nanny-close
2023-06-26 16:53:19,460 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,460 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45813'. Reason: nanny-close
2023-06-26 16:53:19,459 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:39968 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:53:19,459 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:39938 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:53:19,461 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,460 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:39930 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:53:19,461 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42809'. Reason: nanny-close
2023-06-26 16:53:19,462 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,460 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:39964 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:53:19,462 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46621'. Reason: nanny-close
2023-06-26 16:53:19,462 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,460 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:39888 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:53:19,463 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43099'. Reason: nanny-close
2023-06-26 16:53:19,463 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,463 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37517'. Reason: nanny-close
2023-06-26 16:53:19,464 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,464 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41411'. Reason: nanny-close
2023-06-26 16:53:19,464 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,464 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33485'. Reason: nanny-close
2023-06-26 16:53:19,465 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,465 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36425'. Reason: nanny-close
2023-06-26 16:53:19,465 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,465 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36821'. Reason: nanny-close
2023-06-26 16:53:19,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,466 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33807'. Reason: nanny-close
2023-06-26 16:53:19,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,466 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34447'. Reason: nanny-close
2023-06-26 16:53:19,467 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,467 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37955'. Reason: nanny-close
2023-06-26 16:53:19,467 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,467 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:44405'. Reason: nanny-close
2023-06-26 16:53:19,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46199'. Reason: nanny-close
2023-06-26 16:53:19,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:40343'. Reason: nanny-close
2023-06-26 16:53:19,469 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 16:53:19,475 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45813 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47852 remote=tcp://10.120.104.11:45813>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45813 after 100 s
2023-06-26 16:53:19,478 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42809 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35634 remote=tcp://10.120.104.11:42809>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42809 after 100 s
2023-06-26 16:53:19,478 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35883 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:46972 remote=tcp://10.120.104.11:35883>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35883 after 100 s
2023-06-26 16:53:19,479 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46199 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:40012 remote=tcp://10.120.104.11:46199>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46199 after 100 s
2023-06-26 16:53:19,479 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:40343 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45152 remote=tcp://10.120.104.11:40343>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:40343 after 100 s
2023-06-26 16:53:19,481 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:44405 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45794 remote=tcp://10.120.104.11:44405>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:44405 after 100 s
2023-06-26 16:53:19,481 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33807 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41770 remote=tcp://10.120.104.11:33807>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33807 after 100 s
2023-06-26 16:53:19,483 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37517 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47220 remote=tcp://10.120.104.11:37517>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37517 after 100 s
2023-06-26 16:53:19,483 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34447 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:32836 remote=tcp://10.120.104.11:34447>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34447 after 100 s
2023-06-26 16:53:19,483 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43099 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:41040 remote=tcp://10.120.104.11:43099>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43099 after 100 s
2023-06-26 16:53:19,484 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41411 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52480 remote=tcp://10.120.104.11:41411>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41411 after 100 s
2023-06-26 16:53:19,485 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36425 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:39272 remote=tcp://10.120.104.11:36425>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36425 after 100 s
2023-06-26 16:53:19,486 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36821 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:34362 remote=tcp://10.120.104.11:36821>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36821 after 100 s
2023-06-26 16:53:19,490 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33485 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:53780 remote=tcp://10.120.104.11:33485>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33485 after 100 s
2023-06-26 16:53:19,494 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37955 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47584 remote=tcp://10.120.104.11:37955>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37955 after 100 s
2023-06-26 16:53:22,670 - distributed.nanny - WARNING - Worker process still alive after 3.1999955749511724 seconds, killing
2023-06-26 16:53:22,670 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:53:22,670 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:53:22,671 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,671 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 16:53:22,672 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,672 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:53:22,673 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 16:53:22,673 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,674 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,675 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,675 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,675 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,676 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,676 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:22,678 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 16:53:23,461 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,463 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,463 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,463 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,464 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,464 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,464 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,466 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,466 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,466 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,466 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,467 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,467 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,468 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,469 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,469 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284118 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284115 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284112 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284109 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284106 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284103 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284100 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284097 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284051 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284046 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284043 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284040 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284021 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284009 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284001 parent=281246 started daemon>
2023-06-26 16:53:23,471 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=283988 parent=281246 started daemon>
2023-06-26 16:53:25,641 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 284118 exit status was already read will report exitcode 255
2023-06-26 16:53:29,500 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 284021 exit status was already read will report exitcode 255
2023-06-26 16:53:30,189 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 284106 exit status was already read will report exitcode 255
