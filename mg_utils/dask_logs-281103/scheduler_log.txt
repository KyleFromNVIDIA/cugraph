RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/mg_utils/dask-scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-06-26 16:50:43,794 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 16:50:44,279 - distributed.scheduler - INFO - State start
2023-06-26 16:50:44,280 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-i91himwz', purging
2023-06-26 16:50:44,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-02attci8', purging
2023-06-26 16:50:44,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-f272onlt', purging
2023-06-26 16:50:44,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cmevasz9', purging
2023-06-26 16:50:44,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-sk5sgo7_', purging
2023-06-26 16:50:44,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ejz94txr', purging
2023-06-26 16:50:44,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-d9hbxbw8', purging
2023-06-26 16:50:44,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-uzh787ga', purging
2023-06-26 16:50:44,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ug0jg51o', purging
2023-06-26 16:50:44,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-of4us6rp', purging
2023-06-26 16:50:44,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vzp0pyeg', purging
2023-06-26 16:50:44,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-elspdpcq', purging
2023-06-26 16:50:44,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2q3cs769', purging
2023-06-26 16:50:44,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7eh9sr5w', purging
2023-06-26 16:50:44,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ht1_zxvt', purging
2023-06-26 16:50:44,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-zpdfydih', purging
2023-06-26 16:50:44,295 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-26 16:50:44,296 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-06-26 16:50:44,296 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-06-26 16:51:01,136 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43275', status: init, memory: 0, processing: 0>
2023-06-26 16:51:01,138 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43275
2023-06-26 16:51:01,139 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40990
2023-06-26 16:51:02,334 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39753', status: init, memory: 0, processing: 0>
2023-06-26 16:51:02,334 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39753
2023-06-26 16:51:02,334 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41002
2023-06-26 16:51:02,472 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39061', status: init, memory: 0, processing: 0>
2023-06-26 16:51:02,472 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39061
2023-06-26 16:51:02,472 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41012
2023-06-26 16:51:02,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:36111', status: init, memory: 0, processing: 0>
2023-06-26 16:51:02,604 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:36111
2023-06-26 16:51:02,604 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41022
2023-06-26 16:51:02,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33305', status: init, memory: 0, processing: 0>
2023-06-26 16:51:02,655 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33305
2023-06-26 16:51:02,655 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41026
2023-06-26 16:51:02,664 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:35651', status: init, memory: 0, processing: 0>
2023-06-26 16:51:02,665 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:35651
2023-06-26 16:51:02,665 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41028
2023-06-26 16:51:02,924 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:32807', status: init, memory: 0, processing: 0>
2023-06-26 16:51:02,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:32807
2023-06-26 16:51:02,925 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41030
2023-06-26 16:51:03,061 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44299', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,061 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44299
2023-06-26 16:51:03,061 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41038
2023-06-26 16:51:03,176 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42685', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,177 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42685
2023-06-26 16:51:03,177 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41044
2023-06-26 16:51:03,199 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43551', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,199 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43551
2023-06-26 16:51:03,199 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41052
2023-06-26 16:51:03,205 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40833', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,206 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40833
2023-06-26 16:51:03,206 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41068
2023-06-26 16:51:03,211 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:32779', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,212 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:32779
2023-06-26 16:51:03,212 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41074
2023-06-26 16:51:03,215 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33133', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,215 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33133
2023-06-26 16:51:03,215 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41072
2023-06-26 16:51:03,218 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44181', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,219 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44181
2023-06-26 16:51:03,219 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41088
2023-06-26 16:51:03,228 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43271', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,228 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43271
2023-06-26 16:51:03,228 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41092
2023-06-26 16:51:03,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41219', status: init, memory: 0, processing: 0>
2023-06-26 16:51:03,237 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41219
2023-06-26 16:51:03,237 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41094
2023-06-26 16:51:05,629 - distributed.scheduler - INFO - Receive client connection: Client-a4408439-1441-11ee-8aa6-5cff35c1a711
2023-06-26 16:51:05,630 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:41256
2023-06-26 16:51:06,342 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 16:51:57,575 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 16:52:15,629 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 16:52:15,631 - distributed.scheduler - INFO - Restarting workers and releasing all keys.
2023-06-26 16:52:15,651 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41074; closing.
2023-06-26 16:52:15,651 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:32779', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,651 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32779
2023-06-26 16:52:15,652 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41030; closing.
2023-06-26 16:52:15,653 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:32807', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,653 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32807
2023-06-26 16:52:15,653 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41072; closing.
2023-06-26 16:52:15,654 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33133', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,654 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33133
2023-06-26 16:52:15,654 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41026; closing.
2023-06-26 16:52:15,654 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33305', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,654 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33305
2023-06-26 16:52:15,655 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41022; closing.
2023-06-26 16:52:15,655 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:36111', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,655 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:36111
2023-06-26 16:52:15,655 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41012; closing.
2023-06-26 16:52:15,655 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41028; closing.
2023-06-26 16:52:15,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39061', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,656 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39061
2023-06-26 16:52:15,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:35651', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,656 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35651
2023-06-26 16:52:15,656 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41002; closing.
2023-06-26 16:52:15,657 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39753', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,657 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39753
2023-06-26 16:52:15,662 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41044; closing.
2023-06-26 16:52:15,662 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42685', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,662 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42685
2023-06-26 16:52:15,663 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41092; closing.
2023-06-26 16:52:15,663 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43271', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,663 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43271
2023-06-26 16:52:15,663 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41068; closing.
2023-06-26 16:52:15,664 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40833', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,664 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40833
2023-06-26 16:52:15,664 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41094; closing.
2023-06-26 16:52:15,665 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41219', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,665 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41219
2023-06-26 16:52:15,672 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41052; closing.
2023-06-26 16:52:15,672 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43551', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,672 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43551
2023-06-26 16:52:15,690 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:40990; closing.
2023-06-26 16:52:15,690 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43275', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,690 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43275
2023-06-26 16:52:15,694 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41038; closing.
2023-06-26 16:52:15,694 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44299', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,694 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44299
2023-06-26 16:52:15,700 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41088; closing.
2023-06-26 16:52:15,701 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44181', status: closing, memory: 0, processing: 0>
2023-06-26 16:52:15,701 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44181
2023-06-26 16:52:15,701 - distributed.scheduler - INFO - Lost all workers
2023-06-26 16:52:28,573 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46053', status: init, memory: 0, processing: 0>
2023-06-26 16:52:28,573 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46053
2023-06-26 16:52:28,573 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58248
2023-06-26 16:52:28,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33883', status: init, memory: 0, processing: 0>
2023-06-26 16:52:28,857 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33883
2023-06-26 16:52:28,857 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39888
2023-06-26 16:52:28,900 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40303', status: init, memory: 0, processing: 0>
2023-06-26 16:52:28,900 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40303
2023-06-26 16:52:28,900 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39890
2023-06-26 16:52:31,199 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40541', status: init, memory: 0, processing: 0>
2023-06-26 16:52:31,200 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40541
2023-06-26 16:52:31,200 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39930
2023-06-26 16:52:31,643 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42053', status: init, memory: 0, processing: 0>
2023-06-26 16:52:31,644 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42053
2023-06-26 16:52:31,644 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39938
2023-06-26 16:52:31,651 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44277', status: init, memory: 0, processing: 0>
2023-06-26 16:52:31,652 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44277
2023-06-26 16:52:31,652 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39948
2023-06-26 16:52:31,663 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44765', status: init, memory: 0, processing: 0>
2023-06-26 16:52:31,663 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44765
2023-06-26 16:52:31,663 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39968
2023-06-26 16:52:31,664 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:42057', status: init, memory: 0, processing: 0>
2023-06-26 16:52:31,664 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:42057
2023-06-26 16:52:31,665 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:39964
2023-06-26 16:52:34,789 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40293', status: init, memory: 0, processing: 0>
2023-06-26 16:52:34,790 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40293
2023-06-26 16:52:34,790 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40034
2023-06-26 16:52:34,803 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:38473', status: init, memory: 0, processing: 0>
2023-06-26 16:52:34,803 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:38473
2023-06-26 16:52:34,803 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40036
2023-06-26 16:52:34,822 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:40111', status: init, memory: 0, processing: 0>
2023-06-26 16:52:34,822 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:40111
2023-06-26 16:52:34,822 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40050
2023-06-26 16:52:34,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39215', status: init, memory: 0, processing: 0>
2023-06-26 16:52:34,861 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39215
2023-06-26 16:52:34,861 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40066
2023-06-26 16:52:34,863 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:39357', status: init, memory: 0, processing: 0>
2023-06-26 16:52:34,864 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:39357
2023-06-26 16:52:34,864 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40060
2023-06-26 16:52:34,925 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44019', status: init, memory: 0, processing: 0>
2023-06-26 16:52:34,926 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44019
2023-06-26 16:52:34,926 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40068
2023-06-26 16:52:34,962 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46803', status: init, memory: 0, processing: 0>
2023-06-26 16:52:34,963 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46803
2023-06-26 16:52:34,963 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40092
2023-06-26 16:52:34,965 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:41653', status: init, memory: 0, processing: 0>
2023-06-26 16:52:34,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:41653
2023-06-26 16:52:34,966 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:40078
2023-06-26 16:52:35,153 - distributed.scheduler - INFO - Restarting finished.
2023-06-26 16:52:45,092 - distributed.worker - INFO - Run out-of-band function '_func_set_scheduler_as_nccl_root'
2023-06-26 16:53:15,118 - distributed.worker - INFO - Run out-of-band function '_func_destroy_scheduler_session'
2023-06-26 16:53:15,120 - distributed.scheduler - INFO - Remove client Client-a4408439-1441-11ee-8aa6-5cff35c1a711
2023-06-26 16:53:15,120 - distributed.core - INFO - Received 'close-stream' from tcp://10.120.104.11:41256; closing.
2023-06-26 16:53:15,120 - distributed.scheduler - INFO - Remove client Client-a4408439-1441-11ee-8aa6-5cff35c1a711
2023-06-26 16:53:15,121 - distributed.scheduler - INFO - Close client connection: Client-a4408439-1441-11ee-8aa6-5cff35c1a711
2023-06-26 16:53:19,458 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-26 16:53:19,459 - distributed.core - INFO - Connection to tcp://10.120.104.11:39890 has been closed.
2023-06-26 16:53:19,459 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40303', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,459 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40303
2023-06-26 16:53:19,460 - distributed.core - INFO - Connection to tcp://10.120.104.11:40092 has been closed.
2023-06-26 16:53:19,460 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46803', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,460 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46803
2023-06-26 16:53:19,460 - distributed.core - INFO - Connection to tcp://10.120.104.11:40078 has been closed.
2023-06-26 16:53:19,460 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:41653', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,461 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41653
2023-06-26 16:53:19,461 - distributed.core - INFO - Connection to tcp://10.120.104.11:40050 has been closed.
2023-06-26 16:53:19,461 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40111', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,462 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40111
2023-06-26 16:53:19,462 - distributed.core - INFO - Connection to tcp://10.120.104.11:40068 has been closed.
2023-06-26 16:53:19,462 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44019', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,462 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44019
2023-06-26 16:53:19,462 - distributed.core - INFO - Connection to tcp://10.120.104.11:40036 has been closed.
2023-06-26 16:53:19,462 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:38473', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,462 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38473
2023-06-26 16:53:19,462 - distributed.core - INFO - Connection to tcp://10.120.104.11:40034 has been closed.
2023-06-26 16:53:19,462 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40293', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,462 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40293
2023-06-26 16:53:19,462 - distributed.core - INFO - Connection to tcp://10.120.104.11:40066 has been closed.
2023-06-26 16:53:19,463 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39215', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,463 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39215
2023-06-26 16:53:19,463 - distributed.core - INFO - Connection to tcp://10.120.104.11:39948 has been closed.
2023-06-26 16:53:19,463 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44277', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,463 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44277
2023-06-26 16:53:19,463 - distributed.scheduler - INFO - Scheduler closing...
2023-06-26 16:53:19,464 - distributed.core - INFO - Connection to tcp://10.120.104.11:58248 has been closed.
2023-06-26 16:53:19,464 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46053', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,464 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46053
2023-06-26 16:53:19,464 - distributed.core - INFO - Connection to tcp://10.120.104.11:40060 has been closed.
2023-06-26 16:53:19,464 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:39357', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,464 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39357
2023-06-26 16:53:19,464 - distributed.core - INFO - Connection to tcp://10.120.104.11:39968 has been closed.
2023-06-26 16:53:19,464 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44765', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,464 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44765
2023-06-26 16:53:19,465 - distributed.core - INFO - Connection to tcp://10.120.104.11:39938 has been closed.
2023-06-26 16:53:19,465 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42053', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,465 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42053
2023-06-26 16:53:19,465 - distributed.core - INFO - Connection to tcp://10.120.104.11:39930 has been closed.
2023-06-26 16:53:19,465 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:40541', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,465 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40541
2023-06-26 16:53:19,466 - distributed.core - INFO - Connection to tcp://10.120.104.11:39964 has been closed.
2023-06-26 16:53:19,466 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:42057', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42057
2023-06-26 16:53:19,466 - distributed.core - INFO - Connection to tcp://10.120.104.11:39888 has been closed.
2023-06-26 16:53:19,466 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33883', status: running, memory: 0, processing: 0>
2023-06-26 16:53:19,466 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33883
2023-06-26 16:53:19,466 - distributed.scheduler - INFO - Lost all workers
2023-06-26 16:53:19,466 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39888>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39888>: Stream is closed
2023-06-26 16:53:19,467 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:40060>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:53:19,467 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39930>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39930>: Stream is closed
2023-06-26 16:53:19,467 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39938>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39938>: Stream is closed
2023-06-26 16:53:19,467 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39964>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39964>: Stream is closed
2023-06-26 16:53:19,468 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:39968>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:53:19,468 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58248>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 16:53:19,468 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-26 16:53:19,471 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-06-26 16:53:19,471 - distributed.scheduler - INFO - End scheduler
