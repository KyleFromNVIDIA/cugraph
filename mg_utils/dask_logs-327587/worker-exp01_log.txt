RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --rmm-async
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-26 17:39:54,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43199'
2023-06-26 17:39:54,088 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37999'
2023-06-26 17:39:54,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:38401'
2023-06-26 17:39:54,093 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:43247'
2023-06-26 17:39:54,096 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37895'
2023-06-26 17:39:54,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:35047'
2023-06-26 17:39:54,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:33245'
2023-06-26 17:39:54,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36585'
2023-06-26 17:39:54,104 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41569'
2023-06-26 17:39:54,106 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37633'
2023-06-26 17:39:54,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:41705'
2023-06-26 17:39:54,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36713'
2023-06-26 17:39:54,113 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:34561'
2023-06-26 17:39:54,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:42703'
2023-06-26 17:39:54,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:46027'
2023-06-26 17:39:54,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:45401'
2023-06-26 17:39:55,586 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,586 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,744 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:55,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,781 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,781 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,785 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,785 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,785 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,785 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,838 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,853 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,853 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,855 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,855 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,863 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:39:55,863 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:39:55,893 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:55,895 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:55,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:55,923 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:55,960 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:55,961 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:55,963 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:55,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:56,018 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:56,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:56,035 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:56,036 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:56,040 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:56,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:39:56,047 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:40:00,344 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41311
2023-06-26 17:40:00,344 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41311
2023-06-26 17:40:00,344 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35239
2023-06-26 17:40:00,344 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:00,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:00,345 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:00,345 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:00,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rcpmxrub
2023-06-26 17:40:00,345 - distributed.worker - INFO - Starting Worker plugin PreImport-d6df03a2-7a76-4f08-a537-25e3c1b19dca
2023-06-26 17:40:00,346 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2875592c-3372-4d6e-a0cf-9f63ff2a9830
2023-06-26 17:40:01,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ff5867a-8310-437e-8577-1d73d1c39a05
2023-06-26 17:40:01,131 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:01,151 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:01,151 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:01,161 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:01,389 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46755
2023-06-26 17:40:01,389 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46755
2023-06-26 17:40:01,389 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44915
2023-06-26 17:40:01,389 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:01,389 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:01,389 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:01,389 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:01,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lnxk0_p4
2023-06-26 17:40:01,390 - distributed.worker - INFO - Starting Worker plugin PreImport-d03875b9-290e-4eed-9a24-f90d9c08253a
2023-06-26 17:40:01,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26be1fb2-ab2f-464d-abc4-dbbdd1a9022c
2023-06-26 17:40:01,390 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42241
2023-06-26 17:40:01,390 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42241
2023-06-26 17:40:01,390 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36501
2023-06-26 17:40:01,390 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:01,390 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:01,390 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:01,390 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:01,390 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t266wz78
2023-06-26 17:40:01,391 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6619e1e8-9701-4e31-b812-d86ce535b5fa
2023-06-26 17:40:01,391 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58564285-df1b-4564-9d1e-d87d7823d0b9
2023-06-26 17:40:01,920 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34335
2023-06-26 17:40:01,920 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34335
2023-06-26 17:40:01,920 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37403
2023-06-26 17:40:01,920 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:01,920 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:01,920 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:01,920 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:01,920 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fkxn0kvh
2023-06-26 17:40:01,921 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4efbddee-7502-4cc0-9e63-a796e982f35f
2023-06-26 17:40:01,922 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1f59f51-5910-4808-9f08-fdc0ff37637d
2023-06-26 17:40:01,939 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39707
2023-06-26 17:40:01,939 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39707
2023-06-26 17:40:01,940 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45775
2023-06-26 17:40:01,940 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:01,940 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:01,940 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:01,940 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:01,940 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bxrfzlyt
2023-06-26 17:40:01,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-89018015-52bb-4e80-bbd6-cc0b415878d4
2023-06-26 17:40:02,025 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41223
2023-06-26 17:40:02,025 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41223
2023-06-26 17:40:02,025 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41413
2023-06-26 17:40:02,025 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,025 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,025 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,026 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lb_wzup0
2023-06-26 17:40:02,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd28c1c7-23b7-4bb7-a65d-3f299fecd2d0
2023-06-26 17:40:02,064 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37359
2023-06-26 17:40:02,065 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37359
2023-06-26 17:40:02,065 - distributed.worker - INFO -          dashboard at:        10.120.104.11:35883
2023-06-26 17:40:02,065 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,065 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,065 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,065 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,065 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wl9zxf3t
2023-06-26 17:40:02,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-678d4b16-5e3e-4bad-a08d-fdfb8083ad79
2023-06-26 17:40:02,069 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40029
2023-06-26 17:40:02,069 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40029
2023-06-26 17:40:02,069 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45011
2023-06-26 17:40:02,069 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,069 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,069 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,069 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,069 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3p0pmm7a
2023-06-26 17:40:02,070 - distributed.worker - INFO - Starting Worker plugin PreImport-6bd232ad-be2c-4443-a187-c95c2104b7c6
2023-06-26 17:40:02,070 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b013a27-7b1c-477c-8bd7-e9281b0098e6
2023-06-26 17:40:02,109 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39527
2023-06-26 17:40:02,109 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39527
2023-06-26 17:40:02,109 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40127
2023-06-26 17:40:02,109 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,109 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,110 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,110 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,110 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n9qdpi7e
2023-06-26 17:40:02,110 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01484a69-5291-4fcd-99d5-e7c1ed5ec4d5
2023-06-26 17:40:02,124 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:37485
2023-06-26 17:40:02,124 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:37485
2023-06-26 17:40:02,124 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42247
2023-06-26 17:40:02,124 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,124 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,124 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,124 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,124 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z4310vde
2023-06-26 17:40:02,125 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:39363
2023-06-26 17:40:02,125 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:39363
2023-06-26 17:40:02,125 - distributed.worker - INFO -          dashboard at:        10.120.104.11:37595
2023-06-26 17:40:02,125 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,125 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,125 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,125 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,125 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o3ifjt6j
2023-06-26 17:40:02,125 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01a6d928-0f07-4227-940a-b71c7e73049b
2023-06-26 17:40:02,126 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39aca25c-850b-4f91-b0fe-4c59c3fb44b4
2023-06-26 17:40:02,126 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5606502-f338-4f0b-b9e4-76930ca5cee4
2023-06-26 17:40:02,153 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38969
2023-06-26 17:40:02,154 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38969
2023-06-26 17:40:02,154 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43857
2023-06-26 17:40:02,154 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,154 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,154 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,154 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,154 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3vw06lle
2023-06-26 17:40:02,154 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c36db89d-066e-40e7-9c8f-f0a75a42cd58
2023-06-26 17:40:02,160 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:41725
2023-06-26 17:40:02,160 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:41725
2023-06-26 17:40:02,160 - distributed.worker - INFO -          dashboard at:        10.120.104.11:40267
2023-06-26 17:40:02,160 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,160 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,160 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,160 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jqby9exf
2023-06-26 17:40:02,161 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99d5c1ef-86d0-49ea-abd2-46f973c2617a
2023-06-26 17:40:02,181 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44405
2023-06-26 17:40:02,181 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44405
2023-06-26 17:40:02,182 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43027
2023-06-26 17:40:02,182 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,182 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,182 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,182 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,182 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_7jn047p
2023-06-26 17:40:02,182 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b24234f9-53c1-4a59-8dcd-67ac6ebcb5ec
2023-06-26 17:40:02,184 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35841
2023-06-26 17:40:02,184 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35841
2023-06-26 17:40:02,184 - distributed.worker - INFO -          dashboard at:        10.120.104.11:36429
2023-06-26 17:40:02,184 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,184 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,184 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,184 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,184 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gqclzw_p
2023-06-26 17:40:02,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49f2d55a-2743-4f9a-8153-69e7a3ae7000
2023-06-26 17:40:02,215 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40729
2023-06-26 17:40:02,215 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40729
2023-06-26 17:40:02,215 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44961
2023-06-26 17:40:02,215 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:40:02,215 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:02,215 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:40:02,215 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:40:02,215 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dba1l_z4
2023-06-26 17:40:02,216 - distributed.worker - INFO - Starting Worker plugin RMMSetup-258c3f97-3dda-400b-9d30-725a666f789b
2023-06-26 17:40:03,852 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:40:03,855 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:40:05,337 - distributed.worker - INFO - Starting Worker plugin PreImport-b0ce88f1-611a-4bf1-b386-42d4c0c110e8
2023-06-26 17:40:05,338 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,363 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:40:05,377 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:05,377 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,382 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:05,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb286216-2f4b-4ed5-a055-0e9280fa2d5a
2023-06-26 17:40:05,635 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,656 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:05,656 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,663 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:05,814 - distributed.worker - INFO - Starting Worker plugin PreImport-9a6bdfc5-79a2-43d1-b979-7a304b3f085f
2023-06-26 17:40:05,815 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,842 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:05,842 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,848 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:05,866 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-366e4e21-e947-4078-8378-949b3bedaa79
2023-06-26 17:40:05,866 - distributed.worker - INFO - Starting Worker plugin PreImport-6edefad4-c9b3-4861-a94b-98091341bf7a
2023-06-26 17:40:05,867 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a31f335f-7195-4bf2-8336-3a7faa7ce0fe
2023-06-26 17:40:05,879 - distributed.worker - INFO - Starting Worker plugin PreImport-c622578e-9881-4fbe-b16f-8f8cb614119c
2023-06-26 17:40:05,880 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,889 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:05,889 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,892 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:05,897 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:05,897 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,899 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:05,928 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80281406-f582-41a3-a2cf-455a8b989503
2023-06-26 17:40:05,928 - distributed.worker - INFO - Starting Worker plugin PreImport-147c9b23-16e5-424f-9814-2585db2ba705
2023-06-26 17:40:05,929 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,935 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6e3c7fe-086a-43a0-9308-15663f76ac13
2023-06-26 17:40:05,936 - distributed.worker - INFO - Starting Worker plugin PreImport-1728834f-1bc3-4c68-921d-b912b3970bdc
2023-06-26 17:40:05,938 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,949 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:05,949 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,950 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:05,959 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f2176bf-68bd-485e-be31-c044c6d3c886
2023-06-26 17:40:05,960 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,963 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:05,963 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,965 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:05,975 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:05,975 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,976 - distributed.worker - INFO - Starting Worker plugin PreImport-8a79d65b-a097-4f74-9dd6-612044b98060
2023-06-26 17:40:05,977 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:05,978 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,978 - distributed.worker - WARNING - Run Failed
Function: _func_init_all
args:     (b'D-\x8a]\xc1jE-\xbd\xc1\x08\xaa|\x83\x9e\xac', b'\x84\x8ai\xd7\xf2\x17\xfd\x18\x02\x00\xb4\xd7\n!\xe4F\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00`\xa4BNt\x7f\x00\x00\xf0i\x1c\xdet\x7f\x00\x00\xb0\xa9BNt\x7f\x00\x00\xb8\xa9BNt\x7f\x00\x00 fh8\x07V\x00\x00\xb0F\x96`t\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xad.\xcc6\x07V\x00\x00 \xd2\x1e`t\x7f\x00\x00\x00\x81\xc8\x8bM\x99\xde', True, {'tcp://10.120.104.11:41311': {'rank': 14, 'port': 35058}}, False, 0)
kwargs:   {'dask_worker': <Worker 'tcp://10.120.104.11:41311', status: running, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>}
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 3281, in run
    result = await function(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/raft_dask/common/comms.py", line 446, in _func_init_all
    _func_init_nccl(sessionId, uniqueId, dask_worker=dask_worker)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/raft_dask/common/comms.py", line 511, in _func_init_nccl
    n.init(nWorkers, uniqueId, wid)
  File "nccl.pyx", line 151, in raft_dask.common.nccl.nccl.init
RuntimeError: NCCL_ERROR: b'invalid argument (run with NCCL_DEBUG=WARN for details)'
2023-06-26 17:40:05,996 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae7aad2e-2db1-47ba-8773-03a89cda6663
2023-06-26 17:40:05,997 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b15f6606-fb20-4904-89f3-23ccae494fc6
2023-06-26 17:40:05,997 - distributed.worker - INFO - Starting Worker plugin PreImport-fd8f76f8-9420-40d4-8e0b-df15ec869b92
2023-06-26 17:40:05,997 - distributed.worker - INFO - Starting Worker plugin PreImport-1a0f653a-3e29-4e67-af55-8b8e78737a76
2023-06-26 17:40:05,998 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:05,998 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,003 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:06,003 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,004 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:06,012 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-907bb721-b615-4059-a58b-ca2dd871847b
2023-06-26 17:40:06,013 - distributed.worker - INFO - Starting Worker plugin PreImport-397d2fb8-044a-46d4-9dcf-59e3e86af258
2023-06-26 17:40:06,013 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:06,013 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,014 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:06,014 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,014 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:06,015 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,015 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:06,016 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0949afd-b838-426c-824d-6188b5b17011
2023-06-26 17:40:06,016 - distributed.worker - INFO - Starting Worker plugin PreImport-1692d958-7d41-4f7b-873c-c391dc33587d
2023-06-26 17:40:06,017 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,019 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87ace856-5d0e-4ff0-8791-7289ed9f868e
2023-06-26 17:40:06,020 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8913c578-7278-48b8-82c9-d4bd905ef37f
2023-06-26 17:40:06,020 - distributed.worker - INFO - Starting Worker plugin PreImport-a211a40f-1df5-4998-b73f-df665721f138
2023-06-26 17:40:06,021 - distributed.worker - INFO - Starting Worker plugin PreImport-b8fbab84-07bb-4d24-b062-58afbef2a34c
2023-06-26 17:40:06,022 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,023 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,029 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:06,029 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,031 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:06,037 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:06,037 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,040 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:06,045 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:06,045 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,048 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:40:06,049 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:40:06,049 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:40:06,052 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:41:36,883 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,889 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,889 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,889 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,889 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,890 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,890 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,890 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,890 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,890 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,891 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,893 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,893 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,893 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,894 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,894 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,903 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,904 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,904 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,904 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,904 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,904 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,904 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,906 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:41:36,915 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,915 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,915 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:36,916 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:41:39,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:51,307 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:51,707 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:51,823 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:51,914 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:51,936 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:51,950 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:51,982 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:51,999 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:52,305 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:52,329 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:52,339 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:52,388 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:52,413 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:52,442 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:52,519 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:52,619 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:41:59,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:41:59,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:37,816 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,820 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,825 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,826 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,826 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,826 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,826 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,826 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,827 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,827 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,827 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,831 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,831 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,831 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,831 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:37,832 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-26 17:42:41,032 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,040 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,041 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,041 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,041 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,042 - distributed.worker - INFO - Run out-of-band function 'set_statistics_adaptor'
2023-06-26 17:42:41,407 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-691c972d-8c8a-4524-9a82-8bf2fe8ac8dd
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,414 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-4a2b4852-9331-4fa7-87f4-bdd9d48255bb
Function:  _filter_batches
args:      ([           _START_  _BATCH_
0              602        0
1              684        0
2             1384        0
3             1525        0
4             2127        0
...            ...      ...
1546779  693089189     3021
1546780  693089300     3021
1546781  693089304     3021
1546782  693089465     3021
1546783  693089507     3021

[1546784 rows x 2 columns]], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,432 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-b972ba64-242b-4480-a374-507f45d580ac
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,432 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-719aa864-ca0d-42f1-b60a-dadf3b0d40d4
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,432 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-50bb2960-30ec-43af-96f2-9ad6161794fa
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,432 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-8c9e4704-d214-428c-9668-c2121c6aa177
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,432 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-2bebda8d-6055-4204-bfc3-7f3af694f89e
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,432 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-fb34fc5b-90cc-4d50-9bfc-a1384f6d1145
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,432 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-f1633ba8-3f35-4575-99dd-7d158be5eeb2
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,432 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-bb6d94b0-caa3-4f38-b6b0-03e8d52e4b7f
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,433 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-ad2b60ee-e919-4471-90d1-88e0890a9ce9
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,433 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-a6145041-f9e0-4b19-9369-46d39f7c9154
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,433 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-9c4586b3-090d-45a4-bcdb-f9550acac1b1
Function:  _filter_batches
args:      ([Empty DataFrame
Columns: [_START_, _BATCH_]
Index: []], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,440 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-417d6eff-f9a3-4e17-9cab-88bf1807cf94
Function:  _filter_batches
args:      ([           _START_  _BATCH_
0         26729893     3022
1         26729962     3022
2         26730024     3022
3         26730060     3022
4         26730161     3022
...            ...      ...
1546779  719885605     6043
1546780  719885607     6043
1546781  719885626     6043
1546782  719885661     6043
1546783  719885696     6043

[1546784 rows x 2 columns]], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,440 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-e2e6091d-f16a-4d29-bc16-53c5c3153f07
Function:  _filter_batches
args:      ([           _START_  _BATCH_
0        102625137     9066
1        102625140     9066
2        102625165     9066
3        102625257     9066
4        102625281     9066
...            ...      ...
1546775  777419458    12087
1546776  777419463    12087
1546777  777419519    12087
1546778  777419663    12087
1546779  777419689    12087

[1546780 rows x 2 columns]], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,443 - distributed.worker - WARNING - Compute Failed
Key:       _filter_batches-cf07d6f0-1b01-47a8-bec8-d75e1ba63d24
Function:  _filter_batches
args:      ([           _START_  _BATCH_
0         53526087     6044
1         53526142     6044
2         53526143     6044
3         53526290     6044
4         53526327     6044
...            ...      ...
1546775  768984725     9065
1546776  768984744     9065
1546777  768984794     9065
1546778  768984809     9065
1546779  768984842     9065

[1546780 rows x 2 columns]], '_BATCH_', 1169)
kwargs:    {}
Exception: "TypeError('list indices must be integers or slices, not str')"

2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,447 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,448 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,448 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,451 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,451 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:41,451 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:42:45,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:42:45,890 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,892 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,892 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34335. Reason: scheduler-restart
2023-06-26 17:42:45,893 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,893 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35841. Reason: scheduler-restart
2023-06-26 17:42:45,893 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,893 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,894 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37359. Reason: scheduler-restart
2023-06-26 17:42:45,894 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,894 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,894 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:37485. Reason: scheduler-restart
2023-06-26 17:42:45,894 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38969. Reason: scheduler-restart
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,895 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:34335
2023-06-26 17:42:45,895 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39363. Reason: scheduler-restart
2023-06-26 17:42:45,895 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,895 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,895 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,896 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39527. Reason: scheduler-restart
2023-06-26 17:42:45,896 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,896 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,896 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,896 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:39707. Reason: scheduler-restart
2023-06-26 17:42:45,896 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,896 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,897 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,897 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40029. Reason: scheduler-restart
2023-06-26 17:42:45,897 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,897 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,897 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,897 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40729. Reason: scheduler-restart
2023-06-26 17:42:45,897 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,898 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,898 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,898 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,898 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,898 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,898 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,899 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,899 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,900 - distributed.nanny - INFO - Nanny asking worker to close. Reason: scheduler-restart
2023-06-26 17:42:45,901 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41311. Reason: scheduler-restart
2023-06-26 17:42:45,901 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,903 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,904 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41223. Reason: scheduler-restart
2023-06-26 17:42:45,904 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,904 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:41725. Reason: scheduler-restart
2023-06-26 17:42:45,904 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,904 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35841
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37359
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37485
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38969
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39363
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39527
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35841
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39707
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40029
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37359
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37485
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35841
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38969
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39363
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37359
2023-06-26 17:42:45,905 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42241. Reason: scheduler-restart
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39527
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37485
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39707
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38969
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40029
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39363
2023-06-26 17:42:45,905 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39527
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39707
2023-06-26 17:42:45,905 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40029
2023-06-26 17:42:45,906 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,906 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,907 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,915 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,917 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46755. Reason: scheduler-restart
2023-06-26 17:42:45,918 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44405. Reason: scheduler-restart
2023-06-26 17:42:45,918 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35841
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37359
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37485
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38969
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39363
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39527
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39707
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40029
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35841
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37359
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37485
2023-06-26 17:42:45,919 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38969
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39363
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39527
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40729
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39707
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40029
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41311
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41223
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41725
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40729
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41311
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41223
2023-06-26 17:42:45,920 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41725
2023-06-26 17:42:45,921 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,923 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42241
2023-06-26 17:42:45,923 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,925 - distributed.nanny - INFO - Worker closed
2023-06-26 17:42:45,927 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:35841
2023-06-26 17:42:45,927 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37359
2023-06-26 17:42:45,927 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:37485
2023-06-26 17:42:45,927 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:38969
2023-06-26 17:42:45,927 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39363
2023-06-26 17:42:45,927 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39527
2023-06-26 17:42:45,928 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:39707
2023-06-26 17:42:45,928 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40029
2023-06-26 17:42:45,928 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:40729
2023-06-26 17:42:45,928 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41311
2023-06-26 17:42:45,928 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41223
2023-06-26 17:42:45,928 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:41725
2023-06-26 17:42:45,929 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:42241
2023-06-26 17:42:45,929 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46755
2023-06-26 17:42:45,929 - distributed.core - INFO - Connection to tcp://10.120.104.11:8786 has been closed.
2023-06-26 17:42:45,931 - distributed.nanny - INFO - Worker closed
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f0c606c8200, tag: 0xd60043f2418b873c>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f0c606c8200, tag: 0xd60043f2418b873c>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f0c606c8300, tag: 0xd6954ebbd2cf717c>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f0c606c8300, tag: 0xd6954ebbd2cf717c>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f0c606c80c0, tag: 0x4c4da16bfcf8d16a>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f0c606c80c0, tag: 0x4c4da16bfcf8d16a>: 
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f0c606c8680, tag: 0x1e8b920f5d926b92>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f0c606c8680, tag: 0x1e8b920f5d926b92>: 
2023-06-26 17:42:45,948 - distributed.nanny - INFO - Worker closed
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-21624' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-21646' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
Task was destroyed but it is pending!
task: <Task cancelling name='Task-21761' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/continuous_ucx_progress.py:88>>
2023-06-26 17:42:47,823 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:49,873 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:50,012 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:50,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:50,194 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:50,460 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:51,287 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:51,288 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:51,291 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:51,855 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:51,856 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,863 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:53,863 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:53,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:53,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:53,899 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,904 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,905 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,907 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,909 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,911 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,914 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,916 - distributed.nanny - WARNING - Restarting worker
2023-06-26 17:42:53,927 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:53,927 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:53,930 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:53,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:53,930 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:53,931 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:53,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:53,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:53,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:53,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:54,050 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:54,091 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:54,101 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:54,112 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:54,112 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:54,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:54,169 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:55,517 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:36489
2023-06-26 17:42:55,517 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:36489
2023-06-26 17:42:55,517 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43431
2023-06-26 17:42:55,517 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:42:55,517 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:55,517 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:42:55,517 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:42:55,517 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2qqwdxzc
2023-06-26 17:42:55,517 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0c8541b-7c7e-4383-9a35-90fbc43f2ea3
2023-06-26 17:42:55,518 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39ddf27d-f67e-4b52-8e4c-9c8260586d17
2023-06-26 17:42:55,786 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:55,786 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:55,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:55,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:55,875 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:55,875 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:55,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:55,882 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:55,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:55,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:55,953 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:55,953 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:55,954 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:55,954 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:55,963 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:55,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-26 17:42:55,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-26 17:42:55,994 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:56,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:56,060 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:56,117 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:56,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:56,140 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:56,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-26 17:42:57,320 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45963
2023-06-26 17:42:57,320 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45963
2023-06-26 17:42:57,320 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42649
2023-06-26 17:42:57,320 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:42:57,321 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:57,321 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:42:57,321 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:42:57,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wn1hzet1
2023-06-26 17:42:57,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39e9aa14-9a10-410f-add7-50bb66945a5d
2023-06-26 17:42:57,777 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35677
2023-06-26 17:42:57,777 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35677
2023-06-26 17:42:57,777 - distributed.worker - INFO -          dashboard at:        10.120.104.11:39417
2023-06-26 17:42:57,777 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:42:57,777 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:57,777 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:42:57,777 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:42:57,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a4usbvy1
2023-06-26 17:42:57,778 - distributed.worker - INFO - Starting Worker plugin PreImport-032b98d4-f63d-41cd-a303-44a92d4cdd66
2023-06-26 17:42:57,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cb95ed5-341a-45ee-bede-872bdb403291
2023-06-26 17:42:57,811 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:35299
2023-06-26 17:42:57,811 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:35299
2023-06-26 17:42:57,811 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33357
2023-06-26 17:42:57,811 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:42:57,811 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:57,811 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:42:57,811 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:42:57,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gt6j_8w2
2023-06-26 17:42:57,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cdfa9ed8-c9d6-4e52-a424-dbdd99a47455
2023-06-26 17:42:57,812 - distributed.worker - INFO - Starting Worker plugin PreImport-d949e5b4-27ad-420c-acb9-cfa60998b1c9
2023-06-26 17:42:57,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74e82eac-7676-46eb-8abc-561d1b46b86a
2023-06-26 17:42:57,816 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:40151
2023-06-26 17:42:57,816 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:40151
2023-06-26 17:42:57,817 - distributed.worker - INFO -          dashboard at:        10.120.104.11:32843
2023-06-26 17:42:57,817 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:42:57,817 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:57,817 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:42:57,817 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:42:57,817 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-811y7tfr
2023-06-26 17:42:57,817 - distributed.worker - INFO - Starting Worker plugin PreImport-033ddaff-de68-45ec-96cf-cc18cb5b0201
2023-06-26 17:42:57,817 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27c4e4b8-3d7b-4878-95b2-c4d0cf9b0c35
2023-06-26 17:42:57,831 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34911
2023-06-26 17:42:57,831 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34911
2023-06-26 17:42:57,831 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45749
2023-06-26 17:42:57,832 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:42:57,832 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:57,832 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:42:57,832 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:42:57,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qfpjafc2
2023-06-26 17:42:57,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbe395ca-a554-4784-aa25-bdae81198ad5
2023-06-26 17:42:57,841 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:33657
2023-06-26 17:42:57,842 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:33657
2023-06-26 17:42:57,842 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34897
2023-06-26 17:42:57,842 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:42:57,842 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:57,842 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:42:57,841 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42037
2023-06-26 17:42:57,842 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:42:57,842 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42037
2023-06-26 17:42:57,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z5g6v1yg
2023-06-26 17:42:57,842 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46059
2023-06-26 17:42:57,842 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:42:57,842 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:57,842 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:42:57,842 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:42:57,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7lr_5qam
2023-06-26 17:42:57,843 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de68c82e-f6cf-4df2-bc29-280e752c7a76
2023-06-26 17:42:57,843 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23b56020-3bdb-43f9-b473-7636eb155b2c
2023-06-26 17:42:58,400 - distributed.worker - INFO - Starting Worker plugin PreImport-999d395a-c797-4757-bc95-7dc6b53ebc8b
2023-06-26 17:42:58,401 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:58,413 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:42:58,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:42:58,416 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:00,575 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22d51b83-3c5d-4bb0-a905-d4f02be3ed0d
2023-06-26 17:43:00,576 - distributed.worker - INFO - Starting Worker plugin PreImport-9665df3e-9756-4b2c-b832-2f614784c6a2
2023-06-26 17:43:00,576 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:00,603 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:00,603 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:00,605 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:01,259 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5070b2ec-fc30-4955-b020-5c3c530dafa4
2023-06-26 17:43:01,260 - distributed.worker - INFO - Starting Worker plugin PreImport-e2eb5ee2-8fc2-48c3-8eee-5208e5f6799e
2023-06-26 17:43:01,261 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,294 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:01,294 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,297 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:01,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80a80eef-aed5-4344-9433-efcc9bf3759c
2023-06-26 17:43:01,488 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,501 - distributed.worker - INFO - Starting Worker plugin PreImport-b807c194-0467-4392-a86d-8febfdfba9e5
2023-06-26 17:43:01,501 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4527957e-eb67-4e5f-8bc1-3765137d441b
2023-06-26 17:43:01,503 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:01,503 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,503 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,504 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:01,520 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:01,521 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,522 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,524 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:01,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2340ed1-ff2f-4618-9f92-bc3bb15e10a5
2023-06-26 17:43:01,541 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:01,541 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,542 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,544 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:01,563 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:01,563 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,566 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:01,595 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3594fa2-3270-427c-9a56-7f4e5830da10
2023-06-26 17:43:01,596 - distributed.worker - INFO - Starting Worker plugin PreImport-b953bb36-2500-4e14-bac8-8d547d6c7c69
2023-06-26 17:43:01,597 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,610 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:01,610 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:01,612 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:02,622 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:34159
2023-06-26 17:43:02,622 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:34159
2023-06-26 17:43:02,622 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38457
2023-06-26 17:43:02,622 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:43:02,622 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:02,622 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:43:02,622 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:43:02,622 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o_v41l5i
2023-06-26 17:43:02,623 - distributed.worker - INFO - Starting Worker plugin RMMSetup-06408510-bcf3-4e06-8134-ced7a960567f
2023-06-26 17:43:02,670 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:46237
2023-06-26 17:43:02,670 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:46237
2023-06-26 17:43:02,670 - distributed.worker - INFO -          dashboard at:        10.120.104.11:34905
2023-06-26 17:43:02,670 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:43:02,670 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:02,670 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:43:02,670 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:43:02,670 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_23141po
2023-06-26 17:43:02,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65ff99e3-193d-4e29-aba0-e863ce7a3102
2023-06-26 17:43:03,045 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42641
2023-06-26 17:43:03,045 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42641
2023-06-26 17:43:03,045 - distributed.worker - INFO -          dashboard at:        10.120.104.11:38971
2023-06-26 17:43:03,045 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:43:03,045 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:03,045 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:43:03,045 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:43:03,045 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0yacjoeh
2023-06-26 17:43:03,046 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2851809-8e94-49d2-b199-9324662615b7
2023-06-26 17:43:03,059 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:42497
2023-06-26 17:43:03,060 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:42497
2023-06-26 17:43:03,060 - distributed.worker - INFO -          dashboard at:        10.120.104.11:44273
2023-06-26 17:43:03,060 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:43:03,060 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:03,060 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:43:03,060 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:43:03,060 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ytw2kr2h
2023-06-26 17:43:03,060 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0413bd8-aa1f-40ff-abc0-d13dd7f38458
2023-06-26 17:43:03,061 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6b9c84d-4871-4d45-83a2-c23a66a47498
2023-06-26 17:43:03,082 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:43723
2023-06-26 17:43:03,083 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:43723
2023-06-26 17:43:03,083 - distributed.worker - INFO -          dashboard at:        10.120.104.11:42421
2023-06-26 17:43:03,083 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:43:03,083 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:03,083 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:43:03,083 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:43:03,083 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lcb83_kn
2023-06-26 17:43:03,084 - distributed.worker - INFO - Starting Worker plugin PreImport-63e00d97-6377-48b1-a19a-8fb659bbec22
2023-06-26 17:43:03,084 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13d86c9e-cae8-45a2-8736-bea3b76bf125
2023-06-26 17:43:03,086 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:44057
2023-06-26 17:43:03,086 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:44057
2023-06-26 17:43:03,086 - distributed.worker - INFO -          dashboard at:        10.120.104.11:43011
2023-06-26 17:43:03,086 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:43:03,086 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:03,086 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:43:03,086 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:43:03,086 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_feuzbag
2023-06-26 17:43:03,087 - distributed.worker - INFO - Starting Worker plugin PreImport-b1385fad-b3ee-4ff9-89d7-4285c7830a90
2023-06-26 17:43:03,087 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91ba68fd-f59c-4be1-bcad-496c29f40a6f
2023-06-26 17:43:03,092 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38569
2023-06-26 17:43:03,092 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38569
2023-06-26 17:43:03,092 - distributed.worker - INFO -          dashboard at:        10.120.104.11:41947
2023-06-26 17:43:03,092 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:43:03,092 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:03,092 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:43:03,093 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:43:03,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6yqvlcz8
2023-06-26 17:43:03,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9eccfbc0-a0c1-4e80-9179-62a7046956e1
2023-06-26 17:43:03,096 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38655
2023-06-26 17:43:03,097 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38655
2023-06-26 17:43:03,097 - distributed.worker - INFO -          dashboard at:        10.120.104.11:33405
2023-06-26 17:43:03,097 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-06-26 17:43:03,097 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:03,097 - distributed.worker - INFO -               Threads:                          1
2023-06-26 17:43:03,097 - distributed.worker - INFO -                Memory:                  94.41 GiB
2023-06-26 17:43:03,097 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-haff64ld
2023-06-26 17:43:03,098 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c960f6c-968d-4d3d-8b41-49d00cb58f4d
2023-06-26 17:43:04,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75c0d44c-7692-4f80-a44d-d42ded602609
2023-06-26 17:43:04,487 - distributed.worker - INFO - Starting Worker plugin PreImport-d0fb836f-c2f7-44da-b543-91f6b58a9807
2023-06-26 17:43:04,488 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:04,516 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:04,516 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:04,518 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:04,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1d84bc3e-0ef0-47ce-b93c-6e0512641a8a
2023-06-26 17:43:04,634 - distributed.worker - INFO - Starting Worker plugin PreImport-49b57854-a4e3-42f7-b30e-29e4821e0d31
2023-06-26 17:43:04,635 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:04,654 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:04,654 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:04,657 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:05,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab9b4951-6a41-47a4-b0e0-992013218627
2023-06-26 17:43:05,032 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,034 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9f1d9ba-30a5-4f7e-ae38-d453a2f2c69b
2023-06-26 17:43:05,034 - distributed.worker - INFO - Starting Worker plugin PreImport-82e78ba3-abac-48f6-b6bb-cae2a2425a82
2023-06-26 17:43:05,035 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,038 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0cc3a2bc-112f-4411-8208-07708f6a8202
2023-06-26 17:43:05,040 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,044 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:05,044 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,045 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:05,050 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:05,050 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,051 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:05,056 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:05,056 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,058 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:05,058 - distributed.worker - INFO - Starting Worker plugin PreImport-a76369a5-424b-4cca-8c12-87d4769de0f4
2023-06-26 17:43:05,060 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a6c73c1-46ba-4487-a9d5-e1fe6e465ca1
2023-06-26 17:43:05,061 - distributed.worker - INFO - Starting Worker plugin PreImport-57c27a71-e82b-4414-a266-2b668546472f
2023-06-26 17:43:05,061 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5cda163-71fd-4d8c-936e-2c24afe3592a
2023-06-26 17:43:05,068 - distributed.worker - INFO - Starting Worker plugin PreImport-f8de6cef-b3c3-4322-9c6f-e3cafbf36404
2023-06-26 17:43:05,069 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,077 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:05,078 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,079 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:05,082 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:05,082 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,085 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:05,085 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-06-26 17:43:05,085 - distributed.worker - INFO - -------------------------------------------------
2023-06-26 17:43:05,088 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-06-26 17:43:14,388 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,576 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,608 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,662 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,667 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,710 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,735 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,766 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,767 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,774 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,798 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,808 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,837 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:14,939 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:14,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:15,008 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:15,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:15,058 - distributed.worker - INFO - Run out-of-band function 'enable_spilling'
2023-06-26 17:43:15,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:15,070 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,070 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,070 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,070 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,070 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,071 - distributed.worker - INFO - Run out-of-band function '_get_nvml_device_index'
2023-06-26 17:43:15,081 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,081 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,081 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,081 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,081 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,081 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,081 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
[1687801395.082472] [exp01:331454:0]            sock.c:470  UCX  ERROR bind(fd=369 addr=0.0.0.0:38329) failed: Address already in use
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,082 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,094 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,095 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,095 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,095 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:15,095 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-26 17:43:23,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:23,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:24,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:28,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:28,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:28,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:28,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-26 17:43:28,830 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,830 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:28,831 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-26 17:43:40,687 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,687 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,687 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,687 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,687 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,687 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 17:43:40,688 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42497. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45963. Reason: worker-close
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34911. Reason: worker-close
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35677. Reason: worker-close
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42037. Reason: worker-close
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:44057. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:42641. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:35299. Reason: worker-close
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38569. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:43723. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:33657. Reason: worker-close
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:36489. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38655. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:46237. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:34159. Reason: worker-handle-scheduler-connection-broken
2023-06-26 18:03:01,066 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43199'. Reason: nanny-close
2023-06-26 18:03:01,067 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37999'. Reason: nanny-close
2023-06-26 18:03:01,067 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:43018 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:03:01,067 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:43004 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:03:01,069 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,067 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:43058 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:03:01,067 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:42982 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:03:01,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:38401'. Reason: nanny-close
2023-06-26 18:03:01,069 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:43247'. Reason: nanny-close
2023-06-26 18:03:01,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,068 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:43032 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
Process Dask Worker process (from Nanny):
2023-06-26 18:03:01,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:35047'. Reason: nanny-close
2023-06-26 18:03:01,068 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.120.104.11:42996 remote=tcp://10.120.104.11:8786>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-26 18:03:01,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 202, in _run
    target(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 999, in _run
    asyncio.run(run())
2023-06-26 18:03:01,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:33245'. Reason: nanny-close
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 47, in run
    _cancel_all_tasks(loop)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 63, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 685, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 919, in _run
    val = self.callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/system_monitor.py", line 171, in update
    disk_ioc = psutil.disk_io_counters()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/psutil/__init__.py", line 2069, in disk_io_counters
    rawdict = _psplatform.disk_io_counters(**kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/psutil/_pslinux.py", line 1164, in disk_io_counters
    for entry in gen:
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/psutil/_pslinux.py", line 1116, in read_procfs
    lines = f.readlines()
  File "/opt/conda/envs/rapids/lib/python3.10/codecs.py", line 319, in decode
    def decode(self, input, final=False):
KeyboardInterrupt
2023-06-26 18:03:01,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36585'. Reason: nanny-close
2023-06-26 18:03:01,072 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41569'. Reason: nanny-close
2023-06-26 18:03:01,072 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37633'. Reason: nanny-close
2023-06-26 18:03:01,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37895'. Reason: nanny-close
2023-06-26 18:03:01,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:41705'. Reason: nanny-close
2023-06-26 18:03:01,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36713'. Reason: nanny-close
2023-06-26 18:03:01,075 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:34561'. Reason: nanny-close
2023-06-26 18:03:01,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:42703'. Reason: nanny-close
2023-06-26 18:03:01,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:46027'. Reason: nanny-close
2023-06-26 18:03:01,077 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,077 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:45401'. Reason: nanny-close
2023-06-26 18:03:01,079 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-26 18:03:01,088 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37999 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52050 remote=tcp://10.120.104.11:37999>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37999 after 100 s
2023-06-26 18:03:01,091 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:33245 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:35790 remote=tcp://10.120.104.11:33245>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:33245 after 100 s
2023-06-26 18:03:01,091 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:35047 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:49702 remote=tcp://10.120.104.11:35047>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:35047 after 100 s
2023-06-26 18:03:01,091 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:43247 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:47184 remote=tcp://10.120.104.11:43247>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:43247 after 100 s
2023-06-26 18:03:01,092 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36585 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:58990 remote=tcp://10.120.104.11:36585>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36585 after 100 s
2023-06-26 18:03:01,092 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41569 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:36388 remote=tcp://10.120.104.11:41569>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41569 after 100 s
2023-06-26 18:03:01,093 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37633 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:32890 remote=tcp://10.120.104.11:37633>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37633 after 100 s
2023-06-26 18:03:01,096 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:42703 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:45108 remote=tcp://10.120.104.11:42703>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:42703 after 100 s
2023-06-26 18:03:01,098 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:45401 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:33426 remote=tcp://10.120.104.11:45401>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:45401 after 100 s
2023-06-26 18:03:01,098 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:34561 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:50922 remote=tcp://10.120.104.11:34561>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:34561 after 100 s
2023-06-26 18:03:01,099 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:41705 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:59284 remote=tcp://10.120.104.11:41705>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:41705 after 100 s
2023-06-26 18:03:01,099 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:46027 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:49930 remote=tcp://10.120.104.11:46027>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:46027 after 100 s
2023-06-26 18:03:01,104 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:37895 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:52130 remote=tcp://10.120.104.11:37895>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:37895 after 100 s
2023-06-26 18:03:01,105 - distributed.core - ERROR - Timed out during handshake while connecting to tcp://10.120.104.11:36713 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 392, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.120.104.11:49622 remote=tcp://10.120.104.11:36713>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1558, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 397, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.120.104.11:36713 after 100 s
2023-06-26 18:03:01,598 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:40151. Reason: worker-close
2023-06-26 18:03:01,599 - distributed.core - ERROR - Event loop is closed
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 955, in run
    await worker.finished()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1534, in close
    self.status = Status.closing
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1011, in status
    self._send_worker_status_change(stimulus_id)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1022, in _send_worker_status_change
    self.batched_send(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1167, in batched_send
    self.batched_stream.send(msg)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 162, in send
    self.waker.set()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/locks.py", line 222, in set
    fut.set_result(None)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 753, in call_soon
    self._check_closed()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Exception ignored in: <coroutine object WorkerProcess._run.<locals>.run at 0x7fb0eccf47b0>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 939, in run
    async with worker:
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 644, in __aexit__
    await self.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1534, in close
    self.status = Status.closing
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1011, in status
    self._send_worker_status_change(stimulus_id)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1022, in _send_worker_status_change
    self.batched_send(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1167, in batched_send
    self.batched_stream.send(msg)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 162, in send
    self.waker.set()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/locks.py", line 222, in set
    fut.set_result(None)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 753, in call_soon
    self._check_closed()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Task was destroyed but it is pending!
task: <Task pending name='Task-11' coro=<Worker.handle_scheduler() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py:209> wait_for=<Future cancelled> cb=[IOLoop.add_future.<locals>.<lambda>() at /opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py:685, gather.<locals>._done_callback() at /opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py:720]>
2023-06-26 18:03:01,601 - distributed.worker - ERROR - <asyncio.locks.Event object at 0x7fb0ecd2de40 [unset]> is bound to a different event loop
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1304, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1025, in handle_stream
    await comm.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 210, in wrapper
    future = _create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 147, in _create_future
    future = Future()  # type: Future
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 656, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'MainThread'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1510, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 592, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 211, in wait
    fut = self._get_loop().create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/mixins.py", line 30, in _get_loop
    raise RuntimeError(f'{self!r} is bound to a different event loop')
RuntimeError: <asyncio.locks.Event object at 0x7fb0ecd2de40 [unset]> is bound to a different event loop
2023-06-26 18:03:01,602 - distributed.worker - CRITICAL - Error trying close worker in response to broken internal state. Forcibly exiting worker NOW
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1304, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1025, in handle_stream
    await comm.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 210, in wrapper
    future = _create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 147, in _create_future
    future = Future()  # type: Future
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 656, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'MainThread'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 209, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1306, in handle_scheduler
    await self.close(reason="worker-handle-scheduler-connection-broken")
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 795, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1510, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 592, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 211, in wait
    fut = self._get_loop().create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/mixins.py", line 30, in _get_loop
    raise RuntimeError(f'{self!r} is bound to a different event loop')
RuntimeError: <asyncio.locks.Event object at 0x7fb0ecd2de40 [unset]> is bound to a different event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 241, in _force_close
    await wait_for(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 405, in wait_for
    loop = events.get_running_loop()
RuntimeError: no running event loop
2023-06-26 18:03:02,315 - distributed.nanny - INFO - Worker process 331454 exited with status 1
2023-06-26 18:03:04,280 - distributed.nanny - WARNING - Worker process still alive after 3.199995727539063 seconds, killing
2023-06-26 18:03:04,281 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-26 18:03:04,281 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:03:04,282 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:03:04,283 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:03:04,283 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:03:04,283 - distributed.nanny - WARNING - Worker process still alive after 3.19999740600586 seconds, killing
2023-06-26 18:03:04,284 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:03:04,284 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:03:04,284 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:03:04,284 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:03:04,285 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2023-06-26 18:03:04,286 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:03:04,287 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-26 18:03:04,287 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-26 18:03:05,082 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,083 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,084 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,084 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,084 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,084 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,084 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,085 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,085 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,085 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,085 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,085 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,085 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,085 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,086 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1919, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-26 18:03:05,087 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331510 parent=327746 started daemon>
2023-06-26 18:03:05,087 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331507 parent=327746 started daemon>
2023-06-26 18:03:05,087 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331504 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331501 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331498 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331495 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331492 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331489 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331451 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331437 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331434 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331431 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331421 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331408 parent=327746 started daemon>
2023-06-26 18:03:05,088 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=331375 parent=327746 started daemon>
2023-06-26 18:03:07,710 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 331434 exit status was already read will report exitcode 255
