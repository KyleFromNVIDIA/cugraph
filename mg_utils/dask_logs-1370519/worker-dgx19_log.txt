RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=12G
             --local-directory=/tmp/
             --scheduler-file=/root/work/cugraph/mg_utils/dask-scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-06-22 20:16:23,391 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:33387'
2023-06-22 20:16:23,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:36231'
2023-06-22 20:16:23,396 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:40255'
2023-06-22 20:16:23,399 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:36909'
2023-06-22 20:16:23,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:45231'
2023-06-22 20:16:23,404 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:46417'
2023-06-22 20:16:23,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:33721'
2023-06-22 20:16:23,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.33.227.169:41453'
2023-06-22 20:16:24,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pz9_wobh', purging
2023-06-22 20:16:24,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5m9cfgde', purging
2023-06-22 20:16:24,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ajjg3nlf', purging
2023-06-22 20:16:24,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qobdj6iw', purging
2023-06-22 20:16:24,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x88fambr', purging
2023-06-22 20:16:24,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-9z1wm9kv', purging
2023-06-22 20:16:24,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-tgatfer_', purging
2023-06-22 20:16:24,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3oezgwse', purging
2023-06-22 20:16:24,855 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 20:16:24,855 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 20:16:24,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 20:16:24,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 20:16:24,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 20:16:24,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 20:16:24,912 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 20:16:24,912 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 20:16:24,914 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 20:16:24,914 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 20:16:24,920 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 20:16:24,920 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 20:16:24,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 20:16:24,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 20:16:24,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-22 20:16:24,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-22 20:16:25,269 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 20:16:25,338 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 20:16:25,347 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 20:16:25,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 20:16:25,351 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 20:16:25,361 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 20:16:25,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 20:16:25,379 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-22 20:16:26,920 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:45915
2023-06-22 20:16:26,921 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:45915
2023-06-22 20:16:26,921 - distributed.worker - INFO -          dashboard at:        10.33.227.169:43127
2023-06-22 20:16:26,921 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 20:16:26,921 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:26,921 - distributed.worker - INFO -               Threads:                          1
2023-06-22 20:16:26,921 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 20:16:26,921 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8nc73ktx
2023-06-22 20:16:26,922 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a324a3c5-ebcc-48b8-aaf5-987422fe547c
2023-06-22 20:16:26,923 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec800b1d-ac7a-4df1-a07d-f96454ad224d
2023-06-22 20:16:27,129 - distributed.worker - INFO - Starting Worker plugin PreImport-4f3191fe-ac0a-492f-8574-06927a48ed01
2023-06-22 20:16:27,130 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,413 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 20:16:27,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,415 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 20:16:27,425 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:16:27,496 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:16:27,833 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:41219
2023-06-22 20:16:27,833 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:41219
2023-06-22 20:16:27,833 - distributed.worker - INFO -          dashboard at:        10.33.227.169:44359
2023-06-22 20:16:27,833 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 20:16:27,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,833 - distributed.worker - INFO -               Threads:                          1
2023-06-22 20:16:27,834 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 20:16:27,834 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8w3ze61b
2023-06-22 20:16:27,834 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfdc33f1-d401-42bf-a8d0-e101880ead10
2023-06-22 20:16:27,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-06d466e4-6fe6-4afa-bee1-bb444600fd4c
2023-06-22 20:16:27,839 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:45507
2023-06-22 20:16:27,840 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:45507
2023-06-22 20:16:27,840 - distributed.worker - INFO -          dashboard at:        10.33.227.169:34507
2023-06-22 20:16:27,840 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 20:16:27,840 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,840 - distributed.worker - INFO -               Threads:                          1
2023-06-22 20:16:27,840 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 20:16:27,840 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s0_xhlzi
2023-06-22 20:16:27,840 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:35749
2023-06-22 20:16:27,841 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:35749
2023-06-22 20:16:27,841 - distributed.worker - INFO -          dashboard at:        10.33.227.169:45503
2023-06-22 20:16:27,841 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 20:16:27,841 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,841 - distributed.worker - INFO -               Threads:                          1
2023-06-22 20:16:27,841 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20d15312-38a0-4e51-bf0d-6381abf0e24f
2023-06-22 20:16:27,841 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 20:16:27,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-laiq10w5
2023-06-22 20:16:27,841 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7eb9c38c-858c-4f06-99fc-74130bf7d040
2023-06-22 20:16:27,841 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:39033
2023-06-22 20:16:27,841 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:39033
2023-06-22 20:16:27,841 - distributed.worker - INFO -          dashboard at:        10.33.227.169:41189
2023-06-22 20:16:27,841 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 20:16:27,841 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,841 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e9ddd67-dc78-4c9b-a3e2-562c76bec997
2023-06-22 20:16:27,841 - distributed.worker - INFO -               Threads:                          1
2023-06-22 20:16:27,842 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 20:16:27,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xdj6h1__
2023-06-22 20:16:27,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-debca79e-09ad-437e-bb01-9a95b6b8b516
2023-06-22 20:16:27,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9fc5e7d4-1134-41d4-90fa-389292537913
2023-06-22 20:16:27,843 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:45337
2023-06-22 20:16:27,843 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:45337
2023-06-22 20:16:27,843 - distributed.worker - INFO -          dashboard at:        10.33.227.169:38547
2023-06-22 20:16:27,843 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 20:16:27,843 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,843 - distributed.worker - INFO -               Threads:                          1
2023-06-22 20:16:27,843 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 20:16:27,843 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hz6k7s75
2023-06-22 20:16:27,844 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0beaa2c2-8eea-4a65-af11-8e04de013c5b
2023-06-22 20:16:27,844 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee7b0bfc-1af1-40bc-949b-da4e484c6157
2023-06-22 20:16:27,844 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:38383
2023-06-22 20:16:27,845 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:38383
2023-06-22 20:16:27,845 - distributed.worker - INFO -          dashboard at:        10.33.227.169:40357
2023-06-22 20:16:27,845 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 20:16:27,845 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,845 - distributed.worker - INFO -               Threads:                          1
2023-06-22 20:16:27,845 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 20:16:27,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ehbonv_r
2023-06-22 20:16:27,845 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b9b19eb-25b1-498b-af90-40e92177c52a
2023-06-22 20:16:27,846 - distributed.worker - INFO - Starting Worker plugin RMMSetup-087e2b70-99c5-4473-b394-bf2798842929
2023-06-22 20:16:27,846 - distributed.worker - INFO -       Start worker at:  tcp://10.33.227.169:45763
2023-06-22 20:16:27,846 - distributed.worker - INFO -          Listening to:  tcp://10.33.227.169:45763
2023-06-22 20:16:27,846 - distributed.worker - INFO -          dashboard at:        10.33.227.169:45453
2023-06-22 20:16:27,846 - distributed.worker - INFO - Waiting to connect to:   tcp://10.33.227.169:8786
2023-06-22 20:16:27,846 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:27,846 - distributed.worker - INFO -               Threads:                          1
2023-06-22 20:16:27,846 - distributed.worker - INFO -                Memory:                  62.97 GiB
2023-06-22 20:16:27,846 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ir8io8v1
2023-06-22 20:16:27,847 - distributed.worker - INFO - Starting Worker plugin PreImport-fe369718-0eba-4798-910c-6ac4df9ba8c4
2023-06-22 20:16:27,847 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f2af44c-1f6f-43bb-9148-22c6cd504aee
2023-06-22 20:16:28,043 - distributed.worker - INFO - Starting Worker plugin PreImport-15080a19-3827-40bb-b1e2-8e6a7b74944d
2023-06-22 20:16:28,044 - distributed.worker - INFO - Starting Worker plugin PreImport-01107d22-da4c-46e4-a576-39efe26438c1
2023-06-22 20:16:28,044 - distributed.worker - INFO - Starting Worker plugin PreImport-bbc6fa24-c3f7-4ce1-86ad-7308ba185c60
2023-06-22 20:16:28,044 - distributed.worker - INFO - Starting Worker plugin PreImport-b4bdee62-50bb-4f81-b182-014b15962cd7
2023-06-22 20:16:28,044 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0c395b7-8182-4bc6-b82a-e2d17aad4e0b
2023-06-22 20:16:28,044 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ead5806-924b-436e-9219-64825e00ca1b
2023-06-22 20:16:28,044 - distributed.worker - INFO - Starting Worker plugin PreImport-d50247ab-0ef4-4dd4-bb97-d398be09e32e
2023-06-22 20:16:28,044 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,045 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,045 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,045 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,045 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,047 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,047 - distributed.worker - INFO - Starting Worker plugin PreImport-eaac2ed8-7ff5-4d83-91ba-6bd550aed2e3
2023-06-22 20:16:28,049 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,056 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 20:16:28,056 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,057 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 20:16:28,057 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,058 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 20:16:28,059 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 20:16:28,059 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 20:16:28,059 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,060 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 20:16:28,060 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,061 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 20:16:28,062 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 20:16:28,062 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 20:16:28,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,064 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 20:16:28,065 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 20:16:28,065 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,068 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 20:16:28,072 - distributed.worker - INFO -         Registered to:   tcp://10.33.227.169:8786
2023-06-22 20:16:28,072 - distributed.worker - INFO - -------------------------------------------------
2023-06-22 20:16:28,074 - distributed.core - INFO - Starting established connection to tcp://10.33.227.169:8786
2023-06-22 20:16:35,827 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:16:40,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:20,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:20,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:20,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:20,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:20,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:20,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:20,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:25,130 - distributed.worker - WARNING - Compute Failed
Key:       _make_plc_graph-c48e73ba-bc5c-4c51-b2ef-2035439eae8c
Function:  _make_plc_graph
args:      (b'j\x8c\x8d\xdc\x1c\xe6H\xa0\xb3P\xe9\xb6H>Bq', [                 src       dst
100980368  108886467   9374596
100980369   36743203   9374596
100980370   64292340   9374596
100980371   12716162   9374596
100980372   25478427   9374596
...              ...       ...
201960731  108565589  14463884
201960732   18176435  14463885
201960733   85995975  14463886
201960734   80797653  14463886
201960735   71487912  14463886

[100980368 rows x 2 columns],                   src       dst
908823310   110071713  41524242
908823311   107666675  41524242
908823312    37319310  41524242
908823313    75884585  41524242
908823314   109533739  41524242
...               ...       ...
1009803671   86422339  45294951
1009803672    5245188  45294951
1009803673   12868350  45294951
1009803674   62090208  45294951
1009803675   71584833  45294951

[100980366 rows x 2 columns]], <pylibcugraph.graph_properties.GraphProperties object at 0x7f95b32c7970>, 'src', 'dst', False, 1615685872)
kwargs:    {}
Exception: "KeyError('handle')"

2023-06-22 20:17:25,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:25,201 - distributed.worker - WARNING - Compute Failed
Key:       _make_plc_graph-89c0d209-c95e-4d1a-9496-ae9b09936cb9
Function:  _make_plc_graph
args:      (b'j\x8c\x8d\xdc\x1c\xe6H\xa0\xb3P\xe9\xb6H>Bq', [                 src       dst
504901840   51470085  26188365
504901841   37653138  26188366
504901842   42063137  26188366
504901843   39505067  26188366
504901844  103308789  26188366
...              ...       ...
605882203   64460755  30020721
605882204    9119721  30020721
605882205   34918611  30020721
605882206   44424599  30020721
605882207   66097247  30020721

[100980368 rows x 2 columns],                  src       dst
1312744774  47593647  53862860
1312744775   1565600  53862860
1312744776  19104845  53862860
1312744777  47940212  53862860
1312744778  81968936  53862860
...              ...       ...
1413725135  32877480  63417585
1413725136  61400920  63417585
1413725137  33222904  63417585
1413725138  56376924  63417585
1413725139  17419357  63417585

[100980366 rows x 2 columns]], <pylibcugraph.graph_properties.GraphProperties object at 0x7fc2cb1875f0>, 'src', 'dst', False, 1615685872)
kwargs:    {}
Exception: "KeyError('handle')"

2023-06-22 20:17:25,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:25,242 - distributed.worker - WARNING - Compute Failed
Key:       _make_plc_graph-fcb2ae23-c7fc-4986-96cc-37530185435b
Function:  _make_plc_graph
args:      (b'j\x8c\x8d\xdc\x1c\xe6H\xa0\xb3P\xe9\xb6H>Bq', [                 src       dst
403921472   47526306  22354727
403921473  109648177  22354727
403921474   53193649  22354727
403921475   42103302  22354727
403921476   41869650  22354727
...              ...       ...
504901835   75293874  26188364
504901836  109744464  26188364
504901837   77029877  26188364
504901838   95048287  26188364
504901839   33409010  26188365

[100980368 rows x 2 columns],                  src       dst
1211764408  32365269  51011948
1211764409  39473016  51011948
1211764410  39474176  51011948
1211764411  46422924  51011948
1211764412  47980597  51011948
...              ...       ...
1312744769  52534914  53862859
1312744770  51745514  53862859
1312744771  85598484  53862859
1312744772  84659970  53862859
1312744773  43084017  53862860

[100980366 rows x 2 columns]], <pylibcugraph.graph_properties.GraphProperties object at 0x7fefc31a2350>, 'src', 'dst', False, 1615685872)
kwargs:    {}
Exception: "KeyError('handle')"

2023-06-22 20:17:25,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:25,361 - distributed.worker - WARNING - Compute Failed
Key:       _make_plc_graph-7c134d6a-edd0-47b1-9045-3b753f8183c3
Function:  _make_plc_graph
args:      (b'j\x8c\x8d\xdc\x1c\xe6H\xa0\xb3P\xe9\xb6H>Bq', [                 src      dst
0          102309412        0
1           13241850        1
2           39897563        2
3           60103142        3
4           68395754        3
...              ...      ...
100980363   75829826  9374596
100980364     429328  9374596
100980365   35613908  9374596
100980366   35464461  9374596
100980367   71945242  9374596

[100980368 rows x 2 columns],                  src       dst
807842944   32219358  37683243
807842945   35961835  37683243
807842946   55495501  37683243
807842947   56814777  37683243
807842948   65908076  37683243
...              ...       ...
908823305  107831273  41524242
908823306   77040776  41524242
908823307   80241661  41524242
908823308   73962844  41524242
908823309   80482868  41524242

[100980366 rows x 2 columns]], <pylibcugraph.graph_properties.GraphProperties object at 0x7fec5328f110>, 'src', 'dst', False, 1615685872)
kwargs:    {}
Exception: "KeyError('handle')"

2023-06-22 20:17:25,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:25,398 - distributed.worker - WARNING - Compute Failed
Key:       _make_plc_graph-d6b553c5-bfde-4094-bdd5-f8d6545b8289
Function:  _make_plc_graph
args:      (b'j\x8c\x8d\xdc\x1c\xe6H\xa0\xb3P\xe9\xb6H>Bq', [                 src       dst
302941104   77848297  18512152
302941105   24205773  18512152
302941106   47363452  18512152
302941107   84395114  18512152
302941108   49644807  18512152
...              ...       ...
403921467   35289272  22354727
403921468   36785195  22354727
403921469   47482313  22354727
403921470  107509527  22354727
403921471   84794540  22354727

[100980368 rows x 2 columns],                   src       dst
1110784042   21449051  48156951
1110784043   71985668  48156951
1110784044   50741568  48156951
1110784045   12283750  48156951
1110784046   46601175  48156951
...               ...       ...
1211764403   42284438  51011948
1211764404   39691690  51011948
1211764405   54704344  51011948
1211764406   81692298  51011948
1211764407  107619563  51011948

[100980366 rows x 2 columns]], <pylibcugraph.graph_properties.GraphProperties object at 0x7fc537ada290>, 'src', 'dst', False, 1615685872)
kwargs:    {}
Exception: "KeyError('handle')"

2023-06-22 20:17:25,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:25,598 - distributed.worker - WARNING - Compute Failed
Key:       _make_plc_graph-ecf39153-593b-47b9-947b-bed6b2a9ab63
Function:  _make_plc_graph
args:      (b'j\x8c\x8d\xdc\x1c\xe6H\xa0\xb3P\xe9\xb6H>Bq', [                src       dst
605882208   2538272  30020721
605882209  46988837  30020721
605882210  80954061  30020721
605882211  27887218  30020721
605882212  34037896  30020721
...             ...       ...
706862571  42486387  33852297
706862572  42277734  33852297
706862573  50530749  33852297
706862574  23797161  33852297
706862575  24280298  33852297

[100980368 rows x 2 columns],                   src       dst
1413725140   29203446  63417585
1413725141   38532371  63417585
1413725142   40981475  63417585
1413725143     693780  63417585
1413725144   26076761  63417585
...               ...       ...
1514705501   85730000  78145282
1514705502   88197634  78145282
1514705503  102617990  78145282
1514705504  107613406  78145282
1514705505   85012520  78145282

[100980366 rows x 2 columns]], <pylibcugraph.graph_properties.GraphProperties object at 0x7f444320f250>, 'src', 'dst', False, 1615685872)
kwargs:    {}
Exception: "KeyError('handle')"

2023-06-22 20:17:25,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:25,713 - distributed.worker - WARNING - Compute Failed
Key:       _make_plc_graph-1af8d229-1a5e-42dc-babc-10c5c293b5db
Function:  _make_plc_graph
args:      (b'j\x8c\x8d\xdc\x1c\xe6H\xa0\xb3P\xe9\xb6H>Bq', [                 src       dst
201960736  102054773  14463886
201960737   80968297  14463886
201960738  103619813  14463886
201960739  110394291  14463886
201960740    8484077  14463886
...              ...       ...
302941099   34884513  18512151
302941100   43951958  18512151
302941101   54577075  18512151
302941102   50266015  18512151
302941103  101569986  18512151

[100980368 rows x 2 columns],                   src       dst
1009803676  102689454  45294951
1009803677   84075817  45294951
1009803678   23749378  45294951
1009803679   65931430  45294951
1009803680   71694450  45294951
...               ...       ...
1110784037   51484669  48156951
1110784038   53376798  48156951
1110784039   81369762  48156951
1110784040   49320332  48156951
1110784041    6835208  48156951

[100980366 rows x 2 columns]], <pylibcugraph.graph_properties.GraphProperties object at 0x7f4e33c5f8f0>, 'src', 'dst', False, 1615685872)
kwargs:    {}
Exception: "KeyError('handle')"

2023-06-22 20:17:25,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-06-22 20:17:25,723 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:17:25,723 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:17:25,729 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:17:25,729 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:17:25,729 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:17:25,729 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:17:25,729 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:17:25,731 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:17:29,874 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:17:29,908 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-44a43722-0b6d-49dd-8a2d-fa00aed4aef4
Function:  _call_plc_uniform_neighbor_sample
args:      (b'j\x8c\x8d\xdc\x1c\xe6H\xa0\xb3P\xe9\xb6H>Bq', <pylibcugraph.graphs.MGGraph object at 0x7f82f0467c10>,       _START_  _BATCH_
1248     1248        1
1249     1249        1
608       608        0
609       609        0
610       610        0
...       ...      ...
9209     9209        9
9210     9210        9
9211     9211        9
9212     9212        9
9213     9213        9

[10000 rows x 2 columns], True, 8, dd.Scalar<series-..., dtype=int32>, dd.Scalar<series-..., dtype=int32>, array([10, 25], dtype=int32), False)
kwargs:    {'weight_t': 'float32', 'with_edge_properties': True, 'random_state': 7737282193452807786, 'return_offsets': False}
Exception: 'AttributeError("\'Scalar\' object has no attribute \'_parent_meta\'")'

2023-06-22 20:18:11,829 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:11,829 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:11,829 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:11,830 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:11,830 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:11,830 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:11,830 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:11,830 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:11,836 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:18:11,836 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:18:11,836 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:18:11,837 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:18:11,837 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:18:11,837 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:18:11,837 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:18:11,837 - distributed.worker - INFO - Run out-of-band function '_func_init_all'
2023-06-22 20:18:13,599 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:18:13,600 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:18:13,600 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:18:13,600 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:18:13,600 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:18:13,600 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:18:13,600 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:18:13,600 - distributed.worker - INFO - Run out-of-band function '_subcomm_init'
2023-06-22 20:18:34,084 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:18:34,084 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:18:34,090 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:18:34,090 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:18:34,090 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:18:34,090 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:18:34,090 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:18:34,091 - distributed.worker - INFO - Run out-of-band function 'collect'
2023-06-22 20:18:34,894 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:34,894 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:34,899 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:34,899 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:34,899 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:34,899 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:34,900 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:34,900 - distributed.worker - INFO - Run out-of-band function '_func_ucp_listener_port'
2023-06-22 20:18:35,576 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-79461fa8-f52e-44c0-9951-3573bd528cff
Function:  _call_plc_uniform_neighbor_sample
args:      (b'a\xd8z\x10\x17BBj\xa3\xe7\x83\xf9\x83\xc9\x8d\xf6', <pylibcugraph.graphs.MGGraph object at 0x7fc537c94510>, < could not convert arg to str >, True, 8, 0, 9, array([10, 25], dtype=int32), False)
kwargs:    {'weight_t': 'float32', 'with_edge_properties': True, 'random_state': 2195202774958869338, 'return_offsets': False}
Exception: "RuntimeError('non-success value returned from cugraph_uniform_neighbor_sample_with_edge_properties: CUGRAPH_UNKNOWN_ERROR for_each: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-06-22 20:18:35,576 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-c5fdae8b-6117-45e1-90c7-3f15386e4b4f
Function:  _call_plc_uniform_neighbor_sample
args:      (b'a\xd8z\x10\x17BBj\xa3\xe7\x83\xf9\x83\xc9\x8d\xf6', <pylibcugraph.graphs.MGGraph object at 0x7f94d5d86eb0>, < could not convert arg to str >, True, 8, 0, 9, array([10, 25], dtype=int32), False)
kwargs:    {'weight_t': 'float32', 'with_edge_properties': True, 'random_state': 4978993430813935407, 'return_offsets': False}
Exception: "RuntimeError('non-success value returned from cugraph_uniform_neighbor_sample_with_edge_properties: CUGRAPH_UNKNOWN_ERROR for_each: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-06-22 20:18:35,577 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-e60a7cf1-03fd-4480-a873-84dba2c1124e
Function:  _call_plc_uniform_neighbor_sample
args:      (b'a\xd8z\x10\x17BBj\xa3\xe7\x83\xf9\x83\xc9\x8d\xf6', <pylibcugraph.graphs.MGGraph object at 0x7fc2cb2240f0>, < could not convert arg to str >, True, 8, 0, 9, array([10, 25], dtype=int32), False)
kwargs:    {'weight_t': 'float32', 'with_edge_properties': True, 'random_state': 3809769502088171397, 'return_offsets': False}
Exception: "RuntimeError('non-success value returned from cugraph_uniform_neighbor_sample_with_edge_properties: CUGRAPH_UNKNOWN_ERROR transform: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-06-22 20:18:35,579 - distributed.worker - WARNING - Compute Failed
Key:       _call_plc_uniform_neighbor_sample-72cb208b-5765-4422-b8fc-def7bac62914
Function:  _call_plc_uniform_neighbor_sample
args:      (b'a\xd8z\x10\x17BBj\xa3\xe7\x83\xf9\x83\xc9\x8d\xf6', <pylibcugraph.graphs.MGGraph object at 0x7f444335a690>, < could not convert arg to str >, True, 8, 0, 9, array([10, 25], dtype=int32), False)
kwargs:    {'weight_t': 'float32', 'with_edge_properties': True, 'random_state': -1083304526884524969, 'return_offsets': False}
Exception: "RuntimeError('non-success value returned from cugraph_uniform_neighbor_sample_with_edge_properties: CUGRAPH_UNKNOWN_ERROR for_each: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-06-22 20:19:38,730 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:45763. Reason: worker-handle-scheduler-connection-broken
2023-06-22 20:19:38,730 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:41219. Reason: worker-handle-scheduler-connection-broken
2023-06-22 20:19:38,731 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:45507. Reason: worker-handle-scheduler-connection-broken
2023-06-22 20:19:38,731 - distributed.worker - INFO - Stopping worker at tcp://10.33.227.169:38383. Reason: worker-handle-scheduler-connection-broken
2023-06-22 20:19:38,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:36909'. Reason: nanny-close
2023-06-22 20:19:38,732 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 20:19:38,734 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:33387'. Reason: nanny-close
2023-06-22 20:19:38,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 20:19:38,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:36231'. Reason: nanny-close
2023-06-22 20:19:38,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 20:19:38,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:40255'. Reason: nanny-close
2023-06-22 20:19:38,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 20:19:38,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:45231'. Reason: nanny-close
2023-06-22 20:19:38,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 20:19:38,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:46417'. Reason: nanny-close
2023-06-22 20:19:38,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 20:19:38,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:33721'. Reason: nanny-close
2023-06-22 20:19:38,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 20:19:38,739 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.33.227.169:41453'. Reason: nanny-close
2023-06-22 20:19:38,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-22 20:19:38,744 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:36909 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:56158 remote=tcp://10.33.227.169:36909>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:36909 after 100 s
2023-06-22 20:19:38,747 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:33387 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:57752 remote=tcp://10.33.227.169:33387>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:33387 after 100 s
2023-06-22 20:19:38,748 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:40255 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:43650 remote=tcp://10.33.227.169:40255>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:40255 after 100 s
2023-06-22 20:19:38,750 - distributed.worker - ERROR - Timed out during handshake while connecting to tcp://10.33.227.169:41453 after 100 s
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1871, in _run_once
    event_list = self._selector.select(timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/selectors.py", line 469, in select
    fd_event_list = self._selector.poll(timeout, max_ev)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/iostream.py", line 1116, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 373, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.33.227.169:36480 remote=tcp://10.33.227.169:41453>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 754, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1540, in close
    await r.close_gracefully(reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1362, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1606, in connect
    return await connect_attempt
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 1527, in _connect
    comm = await connect(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/core.py", line 378, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://10.33.227.169:41453 after 100 s
2023-06-22 20:19:41,940 - distributed.nanny - WARNING - Worker process still alive after 3.199994201660157 seconds, killing
2023-06-22 20:19:41,940 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 20:19:41,941 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 20:19:41,942 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 20:19:41,942 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 20:19:41,943 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-22 20:19:41,943 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 20:19:41,944 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-22 20:19:42,733 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 20:19:42,736 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 20:19:42,736 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 20:19:42,737 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 20:19:42,737 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 20:19:42,739 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 20:19:42,739 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 20:19:42,739 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 597, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 380, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 853, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1878, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-22 20:19:42,740 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1370751 parent=1370718 started daemon>
2023-06-22 20:19:42,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1370748 parent=1370718 started daemon>
2023-06-22 20:19:42,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1370745 parent=1370718 started daemon>
2023-06-22 20:19:42,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1370742 parent=1370718 started daemon>
2023-06-22 20:19:42,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1370739 parent=1370718 started daemon>
2023-06-22 20:19:42,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1370736 parent=1370718 started daemon>
2023-06-22 20:19:42,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1370733 parent=1370718 started daemon>
2023-06-22 20:19:42,741 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1370730 parent=1370718 started daemon>
2023-06-22 20:19:43,110 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1370733 exit status was already read will report exitcode 255
2023-06-22 20:19:43,346 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1370745 exit status was already read will report exitcode 255
2023-06-22 20:19:43,743 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 1370748 exit status was already read will report exitcode 255
